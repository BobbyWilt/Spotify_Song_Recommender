{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtkO3uVfZRAx"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NeHYp6htZNXz",
    "outputId": "9bd94a27-1ef3-4a4a-cca3-145975582cba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # General Toolboxes\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "import math\n",
    "import zipfile\n",
    "import ast\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import importlib\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "import datetime\n",
    "from numpy2tfrecord import Numpy2TFRecordConverter, build_dataset_from_tfrecord\n",
    "\n",
    "# Visualizations\n",
    "import plotly.express as px\n",
    "\n",
    "# Neural Network\n",
    "import keras\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(tf.config.list_physical_devices())\n",
    "from tensorflow.python.keras import backend\n",
    "\n",
    "# Tensorflow imports\n",
    "import gc\n",
    "from keras.activations import relu, sigmoid, softmax, tanh, selu, elu, gelu, leaky_relu\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation, AlphaDropout, LSTM, RNN, GRU, SimpleRNN, LayerNormalization, InputLayer, TimeDistributed, Bidirectional\n",
    "from keras.layers import ReLU, ELU, LeakyReLU, PReLU, MaxPooling1D, AveragePooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D, Concatenate, Flatten\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.regularizers import l2, l1, L1L2\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import keras_tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Imports for custom tuner\n",
    "from keras_tuner.src.engine import tuner_utils\n",
    "import copy\n",
    "import random\n",
    "try:\n",
    "    import scipy\n",
    "    import scipy.optimize\n",
    "except ImportError:\n",
    "    scipy = None\n",
    "from keras_tuner.engine import hyperparameters as hp_module\n",
    "from keras.src.utils import io_utils\n",
    "from keras_tuner.src.engine import oracle as oracle_module\n",
    "from keras_tuner.src.engine import trial as trial_module\n",
    "from keras_tuner.src.engine import tuner as tuner_module\n",
    "from keras_tuner.src.tuners.bayesian import BayesianOptimizationOracle\n",
    "\n",
    "# Spotify\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "\n",
    "# SkLearn\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight, shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, average_precision_score, mean_squared_error\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge\n",
    "\n",
    "import spacy\n",
    "import joblib\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print('Imports Complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "DATA_DIR = os.environ.get('DATA_DIR')\n",
    "def man_win_lin_encode(str_path):\n",
    "    if ':' in str_path:\n",
    "        out = str_path.replace('\\\\', '/')\n",
    "        out = out.replace(f'{DATA_DIR[:2]}', f'/mnt/{DATA_DIR[0]}'.lower())\n",
    "    else:\n",
    "        out = str_path\n",
    "    return out\n",
    "DATA_DIR = man_win_lin_encode(DATA_DIR)\n",
    "# Check to make sure subfolders are created for environment\n",
    "dir_folders = ['Databases', 'Model_Tuning', 'Saved_Models']\n",
    "for folder in dir_folders:\n",
    "  dirname = os.path.join(DATA_DIR, folder)\n",
    "  if os.path.exists(dirname) == False:\n",
    "      os.mkdir(dirname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widgets\n",
    "config = json.load(open('config.json'))\n",
    "prediction_type = config['prediction_type']\n",
    "if prediction_type=='Regression':\n",
    "  regression=True\n",
    "else:\n",
    "  regression=False\n",
    "style = {'description_width': '50%'}\n",
    "tune_wdg = widgets.Dropdown(\n",
    "    options=[\"general\", \"genre\", 'sct_data', 'sgm_loud', 'sgm_pitch', 'sgm_timbre', \"big_data\", \"final_lr\"],\n",
    "    value=\"sgm_pitch\",\n",
    "    description='Dataset for Model Tuning:',\n",
    "    layout=Layout(width='25%'),\n",
    "    disabled=False,\n",
    "    style = {'description_width': 'initial'}\n",
    ")\n",
    "rating_key_wdg = widgets.Dropdown(\n",
    "    options=[range(len(config['ratings'][prediction_type].keys()))],\n",
    "    value=None,\n",
    "    description='Select the playlist rating index for positive classes (ie: 4 for playlists 4 and 5 to be positive):',\n",
    "    layout=Layout(width='25%'),\n",
    "    disabled=False,\n",
    "    style = {'description_width': 'initial'}\n",
    ")\n",
    "early_stop_patience_wdg = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=5,\n",
    "    max=50,\n",
    "    step=5,\n",
    "    description='Early Stopping # of Epochs:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    layout=Layout(width='40%'),\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    style = {'description_width': 'initial'}\n",
    ")\n",
    "num_random_iter_wdg = widgets.IntSlider(\n",
    "    value=200,\n",
    "    min=25,\n",
    "    max=300,\n",
    "    step=25,\n",
    "    description='Number of random hyperparameter initialization epochs for bayesian optimization:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    layout=Layout(width='50%'),\n",
    "    style = {'description_width': 'initial'}\n",
    ")\n",
    "num_model_iter_wdg = widgets.IntSlider(\n",
    "    value=3000,\n",
    "    min=100,\n",
    "    max=2000,\n",
    "    step=100,\n",
    "    description='Number of model tuning iterations:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d',\n",
    "    layout=Layout(width='40%'),\n",
    "    style = {'description_width': 'initial'}\n",
    ")\n",
    "acc_model_tune_metric_wdg = widgets.Dropdown(\n",
    "    options=[\"pr_auc\", \"accuracy\", \"loss\"],\n",
    "    value=\"pr_auc\",\n",
    "    description='Accuracy Tuning Metric:',\n",
    "    layout=Layout(width='25%'),\n",
    "    disabled=False,\n",
    "    style = {'description_width': 'initial'}\n",
    ")\n",
    "reg_model_tune_metric_wdg = widgets.Dropdown(\n",
    "    options=[\"mean_squared_error\", \"mean_absolute_error\", \"mape\", \"msle\", \"huber\"],\n",
    "    value=\"mean_squared_error\",\n",
    "    description='RegressionTuning Metric:',\n",
    "    layout=Layout(width='25%'),\n",
    "    disabled=False,\n",
    "    style = {'description_width': 'initial'}\n",
    ")\n",
    "made_widgets = [tune_wdg, rating_key_wdg, early_stop_patience_wdg,\n",
    "                num_random_iter_wdg, num_model_iter_wdg, acc_model_tune_metric_wdg,\n",
    "                reg_model_tune_metric_wdg]\n",
    "if prediction_type=='Regression':\n",
    "  made_widgets.remove(rating_key_wdg)\n",
    "  made_widgets.remove(acc_model_tune_metric_wdg)\n",
    "else:\n",
    "  made_widgets.remove(reg_model_tune_metric_wdg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for widget in made_widgets:\n",
    "  display(widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "nsXQP3ge43Vv"
   },
   "outputs": [],
   "source": [
    "tune = tune_wdg.value\n",
    "rating_key = rating_key_wdg.value\n",
    "early_stop_patience = early_stop_patience_wdg.value\n",
    "model_rand_num_iterations = num_random_iter_wdg.value\n",
    "model_tune_num_iterations = num_model_iter_wdg.value\n",
    "acc_model_tune_metric = acc_model_tune_metric_wdg.value\n",
    "reg_model_tune_metric = reg_model_tune_metric_wdg.value\n",
    "BEST_SCORE = None\n",
    "\n",
    "if tune=='genre':\n",
    "  tune_mode = 'genre'\n",
    "elif tune=='general':\n",
    "  tune_mode = 'overall'\n",
    "elif tune=='final_nn':\n",
    "  tune_mode = 'model'\n",
    "elif tune=='final_lr':\n",
    "  tune_mode = 'lr_overall'\n",
    "else:\n",
    "  tune_mode = 'other'\n",
    "save = True #@param {type:\"boolean\"}\n",
    "tuner_seed = \"\" #@param {type:\"string\"}\n",
    "\n",
    "if regression:\n",
    "  TUNE_METRIC_NAME = reg_model_tune_metric\n",
    "else:\n",
    "  TUNE_METRIC_NAME = acc_model_tune_metric\n",
    "if (tune=='final_lr') or( tune=='final_nn'):\n",
    "    model_tune_name='overall'\n",
    "    file_dir = os.path.join(DATA_DIR, 'Databases', \"overall\")\n",
    "    save_dir = os.path.join(DATA_DIR, 'Model_Tuning')\n",
    "    checkpoint_dir = os.path.join(save_dir, 'overall', 'overall')\n",
    "else:\n",
    "    model_tune_name = tune\n",
    "    file_dir = os.path.join(DATA_DIR, 'Databases', tune)\n",
    "    save_dir = os.path.join(DATA_DIR, 'Model_Tuning')\n",
    "    checkpoint_dir = os.path.join(save_dir, model_tune_name, model_tune_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "INPUT_DATA_DIR = file_dir\n",
    "TRAIN_DATA_COEFFICIENT = 0.80\n",
    "VALIDATION_DATA_COEFFICIENT = 0.20\n",
    "# TEST_DATA_COEFFICIENT = 0.10\n",
    "TEST_DATA_COEFFICIENT = 0.0\n",
    "file_list = os.listdir(file_dir)\n",
    "num_files = len(file_list)\n",
    "file_list = [f'{model_tune_name}_dataset_p{i+1}' for i in range(len(os.listdir(file_dir)))]\n",
    "num_train = int(num_files * TRAIN_DATA_COEFFICIENT)\n",
    "num_val = int(num_files * VALIDATION_DATA_COEFFICIENT)\n",
    "num_test = int(num_files * TEST_DATA_COEFFICIENT)\n",
    "GEN_INPUT_SHAPES = {\n",
    "  'genre': [None, 300],\n",
    "  'general': [None, 49],\n",
    "  'sct_data': [None, 300, 10],\n",
    "  'sgm_loud': [None, 3000, 5],\n",
    "  'sgm_pitch': [None, 3000, 13],\n",
    "  'sgm_timbre': [None, 3000, 13],\n",
    "  'big_data': [None, 3000, 39],\n",
    "  'overall': [None, 6]\n",
    "}\n",
    "INPUT_DIM=GEN_INPUT_SHAPES[model_tune_name][1:]\n",
    "\n",
    "TRAIN_FILES = [os.path.join(file_dir, f'{file}.tfrecord') for file in file_list[:num_train]]\n",
    "VALIDATION_FILES = [os.path.join(file_dir, f'{file}.tfrecord') for file in file_list[num_train:num_train+num_val]]\n",
    "# TEST_FILES = [os.path.join(file_dir, f'{file}.tfrecord') for file in file_list[num_train+num_val:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLEFBKXKCGZz"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eWEaUYGOrGQm"
   },
   "outputs": [],
   "source": [
    "def parse_feature_function_song_id(example_proto, tune_shapes=GEN_INPUT_SHAPES):\n",
    "    data_shape=tune_shapes[FEATURE][1:]\n",
    "    tfrecord_format = {\n",
    "            \"x\": tf.io.FixedLenFeature(data_shape, tf.float32),\n",
    "            \"y\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"weight\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"song_id\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    features = tf.io.parse_single_example(example_proto, tfrecord_format)\n",
    "    x=features['x']\n",
    "    y = features['y']\n",
    "    weight = features['weight']\n",
    "    song_id = features['song_id']\n",
    "    return x, y, weight, song_id\n",
    "\n",
    "\n",
    "def parse_feature_function(example_proto, tune=model_tune_name, tune_shapes=GEN_INPUT_SHAPES):\n",
    "    data_shape=tune_shapes[tune][1:]\n",
    "    tfrecord_format = {\n",
    "            \"x\": tf.io.FixedLenFeature(data_shape, tf.float32),\n",
    "            \"y\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"weight\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"song_id\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    features = tf.io.parse_single_example(example_proto, tfrecord_format)\n",
    "    x=features['x']\n",
    "    y = features['y']\n",
    "    weight = features['weight']\n",
    "    return x, y, weight\n",
    "\n",
    "\n",
    "def translate_ids(song_ids, return_id_only=True):\n",
    "    song_id_lookup = json.load(open('song_id_lookup.json'))\n",
    "    if type(song_ids)==str:\n",
    "        unq_ids, ratings = song_id_lookup[song_ids]\n",
    "    else:\n",
    "        unq_ids, ratings = [], []\n",
    "        for song_id in song_ids:\n",
    "            unq_id, rating = song_id_lookup[song_id]\n",
    "            unq_ids.append(unq_id)\n",
    "            ratings.append(rating)\n",
    "    if return_id_only:\n",
    "        return unq_ids\n",
    "    else:\n",
    "        return unq_ids, ratings\n",
    "\n",
    "def reverse_translate_ids(song_ids, return_song_id=True):\n",
    "    song_id_lookup = json.load(open('song_id_lookup.json'))\n",
    "    reverse_dict = {}\n",
    "    for key, value in song_id_lookup.items():\n",
    "        reverse_dict[value[0]] = (key, value[1])\n",
    "    if type(song_ids)==str:\n",
    "        unq_ids, ratings = reverse_dict[song_ids]\n",
    "    else:\n",
    "        unq_ids, ratings = [], []\n",
    "        for song_id in song_ids:\n",
    "            unq_id, rating = reverse_dict[song_id]\n",
    "            unq_ids.append(unq_id)\n",
    "            ratings.append(rating)\n",
    "    if return_song_id:\n",
    "        return unq_ids\n",
    "    else:\n",
    "        return unq_ids, ratings\n",
    "\n",
    "\n",
    "def get_overall_data(data_files):\n",
    "  global FEATURE\n",
    "  FEATURE='overall'\n",
    "  features = [\"genre\", \"general\", \"sct_data\", \"sgm_loud\", \"sgm_pitch\", \"sgm_timbre\"]\n",
    "  for i, file in enumerate(data_files):\n",
    "      dataset = tf.data.TFRecordDataset([file])\n",
    "      dataset = dataset.map(parse_feature_function_song_id)\n",
    "      dataset = dataset.batch(500)\n",
    "      for raw_record in dataset:\n",
    "          x, y, weight, song_id = raw_record\n",
    "      x, y, weight, song_id = x.numpy(), y.numpy().reshape(-1,1), weight.numpy().reshape(-1,1), song_id.numpy().reshape(-1,1)\n",
    "      song_id = np.array(reverse_translate_ids(song_id.reshape(-1))).reshape(-1,1)\n",
    "      data = np.hstack((x,y,weight))\n",
    "      df_pred = pd.DataFrame(data=data, columns=features + ['rating', 'weight'])\n",
    "      df_pred['song_id'] = song_id\n",
    "      if i==0:\n",
    "          out=df_pred\n",
    "      else:\n",
    "          out = pd.concat([out, df_pred], axis=0)\n",
    "  return out\n",
    "\n",
    "\n",
    "def parse_feature_function_lr(example_proto, tune_shapes=GEN_INPUT_SHAPES):\n",
    "    data_shape=tune_shapes[FEATURE][1:]\n",
    "    tfrecord_format = {\n",
    "            \"x\": tf.io.FixedLenFeature(data_shape, tf.float32),\n",
    "            \"y\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"weight\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"song_id\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    features = tf.io.parse_single_example(example_proto, tfrecord_format)\n",
    "    x=features['x']\n",
    "    y = features['y']\n",
    "    weight = features['weight']\n",
    "    return x, y, weight\n",
    "\n",
    "def load_dataset(filenames, batch_size):\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "    ignore_order = tf.data.Options()\n",
    "    ignore_order.experimental_deterministic = False\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.with_options(ignore_order)\n",
    "    dataset = dataset.map(parse_feature_function, num_parallel_calls=AUTOTUNE)\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def overall_model_grid_search(train_data, val_data, test_data, lr_model_train_data,\n",
    "                  param_grid, cv=5, metric=TUNE_METRIC_NAME,\n",
    "                  overfit_penalty=5, prediction_type=prediction_type):\n",
    "\n",
    "  huber = tf.keras.losses.Huber()\n",
    "  mse = keras.metrics.MeanSquaredError()\n",
    "  X, y = lr_model_train_data[:, :-3], lr_model_train_data[:, -3:-1]\n",
    "  test_x, test_y = test_data[:, :-3], test_data[:, -3:-1]\n",
    "  X_shuf, y_shuf = shuffle(X, y, random_state=42)\n",
    "  # Split Data\n",
    "  kf = KFold(n_splits=cv)\n",
    "  data = {}\n",
    "  param_opts = list(param_grid.keys())\n",
    "  score_nams = ['mean_score', 'mean_train_score', 'mean_ovfit_score']\n",
    "  for key in param_opts + score_nams: data[key] = []\n",
    "\n",
    "  # Loop through param_grid\n",
    "  split_data = []\n",
    "  for train_index , test_index in kf.split(X_shuf):\n",
    "    X_train, X_val = X_shuf[train_index], X_shuf[test_index]\n",
    "    y_train , y_val = y_shuf[train_index], y_shuf[test_index]\n",
    "    # Split ratings and weights\n",
    "    y_train, train_weight = split_y(y_train)\n",
    "    y_val, val_weight = split_y(y_val)\n",
    "    split_data.append((X_train, X_val, y_train, train_weight, y_val, val_weight))\n",
    "  # return split_data\n",
    "  if prediction_type=='Regression':\n",
    "    # Regression\n",
    "    for alpha in param_grid['alpha']:\n",
    "        for solver in param_grid['solver']:\n",
    "          try:\n",
    "            model = Ridge(alpha=alpha, solver=solver, random_state=42)\n",
    "            kf_train_score, kf_val_score, kf_overfit_score = [], [], []\n",
    "            for cv in split_data:\n",
    "              X_train, X_val, y_train, train_weight, y_val, val_weight = cv\n",
    "              # Fit model\n",
    "              model.fit(X_train, y_train, sample_weight=train_weight)\n",
    "              y_train_pred = model.predict(X_train)\n",
    "              y_val_pred = model.predict(X_val)\n",
    "              if metric=='mean_squared_error':\n",
    "                train_score = mse(y_train.reshape(-1,1), y_train_pred.reshape(-1,1), sample_weight=train_weight.reshape(-1,1)).numpy()\n",
    "                val_score = mse(y_val.reshape(-1,1), y_val_pred.reshape(-1,1), sample_weight=val_weight.reshape(-1,1)).numpy()\n",
    "              else:\n",
    "                train_score = huber(np.array(y_train).reshape(-1,1), np.array(y_train_pred).reshape(-1,1), sample_weight=np.array(train_weight).reshape(-1,1)).numpy()\n",
    "                val_score = huber(np.array(y_val).reshape(-1,1), np.array(y_val_pred).reshape(-1,1), sample_weight=np.array(val_weight).reshape(-1,1)).numpy()\n",
    "\n",
    "              overfit_score = val_score + (overfit_penalty * (val_score - train_score))\n",
    "              kf_train_score.append(train_score)\n",
    "              kf_val_score.append(val_score)\n",
    "              kf_overfit_score.append(overfit_score)\n",
    "            # get mean scores\n",
    "            data['mean_score'].append(np.mean(kf_val_score))\n",
    "            data['mean_train_score'].append(np.mean(kf_train_score))\n",
    "            data['mean_ovfit_score'].append(np.mean(kf_overfit_score))\n",
    "            data['alpha'].append(alpha)\n",
    "            data['solver'].append(solver)\n",
    "          except:\n",
    "            pass\n",
    "  else:\n",
    "    # Classification\n",
    "    for penalty in param_grid['penalty']:\n",
    "      for l1_ratio in param_grid['l1_ratio']:\n",
    "        for C in param_grid['C']:\n",
    "          for solver in param_grid['solver']:\n",
    "            try:\n",
    "              model = LogisticRegression(penalty=penalty, l1_ratio=l1_ratio, C=C, solver=solver)\n",
    "              kf_train_score, kf_val_score, kf_overfit_score = [], [], []\n",
    "              print('created model')\n",
    "              for cv in split_data:\n",
    "                X_train, X_val, y_train, train_weight, y_val, val_weight = cv\n",
    "                # Fit model\n",
    "                model.fit(X_train, y_train, sample_weight=train_weight)\n",
    "                if metric=='map':\n",
    "                  y_train_pred = model.predict_proba(X_train)[:,1]\n",
    "                  y_val_pred = model.predict_proba(X_val)[:,1]\n",
    "                  train_score = average_precision_score(y_train, y_train_pred, sample_weight=train_weight)\n",
    "                  val_score = average_precision_score(y_val, y_val_pred, sample_weight=val_weight)\n",
    "                elif metric=='mean_squared_error':\n",
    "                  y_train_pred = model.predict(X_train)\n",
    "                  y_val_pred = model.predict(X_val)\n",
    "                  train_score = mse(y_train, y_train_pred, sample_weight=train_weight)\n",
    "                  val_score = mse(y_val, y_val_pred, sample_weight=val_weight)\n",
    "                else:\n",
    "                  y_train_pred = model.predict(X_train)\n",
    "                  y_val_pred = model.predict(X_val)\n",
    "                  train_score = accuracy_score(y_train, y_train_pred, sample_weight=train_weight)\n",
    "                  val_score = accuracy_score(y_val, y_val_pred, sample_weight=val_weight)\n",
    "                overfit_score = val_score + (overfit_penalty * (val_score - train_score))\n",
    "                kf_train_score.append(train_score)\n",
    "                kf_val_score.append(val_score)\n",
    "                kf_overfit_score.append(overfit_score)\n",
    "              # get mean scores\n",
    "              data['mean_score'].append(np.mean(kf_val_score))\n",
    "              data['mean_train_score'].append(np.mean(kf_train_score))\n",
    "              data['mean_ovfit_score'].append(np.mean(kf_overfit_score))\n",
    "              if penalty==None:\n",
    "                data['penalty'].append('None')\n",
    "              else:\n",
    "                data['penalty'].append(penalty)\n",
    "              data['l1_ratio'].append(l1_ratio)\n",
    "              data['C'].append(C)\n",
    "              data['solver'].append(solver)\n",
    "            except:\n",
    "              pass\n",
    "  data = pd.DataFrame(data)\n",
    "  data.sort_values('mean_ovfit_score', ascending=True, inplace=True)\n",
    "  display(data)\n",
    "\n",
    "  # refit model on all train data with best model parameters\n",
    "  if prediction_type=='Regression':\n",
    "    fig = px.scatter(data, x='alpha', y='mean_ovfit_score', color='solver',\n",
    "                title='Computed Score based on Alpha and solver')\n",
    "    fig.show()\n",
    "    alpha, solver = data['alpha'].iloc[0], data['solver'].iloc[0]\n",
    "    # alpha, solver = data['alpha'].iloc[1], data['solver'].iloc[1]\n",
    "    print(alpha, solver)\n",
    "    model = Ridge(alpha=alpha, solver=solver, random_state=42)\n",
    "  else:\n",
    "    fig = px.scatter(data, x='C', y='mean_ovfit_score', symbol='penalty', color='solver',\n",
    "                title='Computed Score based on C, penalty and solver')\n",
    "    fig.show()\n",
    "    C, solver, penalty, l1_ratio = data['C'].iloc[0], data['solver'].iloc[0], data['penalty'].iloc[0], data['l1_ratio'].iloc[0]\n",
    "    model = LogisticRegression(penalty=penalty, l1_ratio=l1_ratio, C=C, solver=solver)\n",
    "  # Fit model and make predictions\n",
    "  model.fit(X, y[:,0], sample_weight=y[:,1])\n",
    "  y_train_pred = model.predict(train_data[:,:-3])\n",
    "  y_val_pred = model.predict(val_data[:,:-3])\n",
    "  y_test_pred = model.predict(test_data[:,:-3])\n",
    "  if metric=='map':\n",
    "    y_train_pred = model.predict_proba(train_data[:,:-2])\n",
    "    y_val_pred = model.predict_proba(val_data[:,:-2])\n",
    "    y_test_pred = model.predict_proba(test_data[:,:-2])\n",
    "    train_score = average_precision_score(train_data[:,-2], y_train_pred, sample_weight=train_data[:,-2])\n",
    "    val_score = average_precision_score(val_data[:,-2], y_val_pred, sample_weight=val_data[:,-2])\n",
    "    test_score = average_precision_score(test_data[:,-2], y_test_pred, sample_weight=test_data[:,-2])\n",
    "  elif metric=='mean_squared_error':\n",
    "    train_score = mse(train_data[:,-3].reshape(-1,1), y_train_pred.reshape(-1,1), sample_weight=train_data[:,-2].reshape(-1,1))\n",
    "    val_score = mse(val_data[:,-3].reshape(-1,1), y_val_pred, sample_weight=val_data[:,-2].reshape(-1,1))\n",
    "    test_score = mse(test_data[:,-3].reshape(-1,1), y_test_pred.reshape(-1,1), sample_weight=test_data[:,-2].reshape(-1,1))\n",
    "  elif metric=='huber':\n",
    "    train_score = huber(np.array(train_data[:,-2]).reshape(-1,1), np.array(y_train_pred).reshape(-1,1), sample_weight=np.array(train_data[:,-2]).reshape(-1,1)).numpy()\n",
    "    val_score = huber(np.array(val_data[:,-2]).reshape(-1,1), np.array(y_val_pred).reshape(-1,1), sample_weight=np.array(val_data[:,-2]).reshape(-1,1)).numpy()\n",
    "    test_score = huber(np.array(test_data[:,-2]).reshape(-1,1), np.array(y_test_pred).reshape(-1,1), sample_weight=np.array(test_data[:,-2]).reshape(-1,1)).numpy()\n",
    "  else:\n",
    "    train_score = accuracy_score(train_data[:,-2], y_train_pred, sample_weight=train_data[:,-2])\n",
    "    val_score = accuracy_score(val_data[:,-2], y_val_pred, sample_weight=val_data[:,-2])\n",
    "    test_score = accuracy_score(test_data[:,-2], y_test_pred, sample_weight=test_data[:,-2])\n",
    "  scores = pd.DataFrame({'Data':['Train', 'Val', 'Test'], metric:[train_score, val_score, test_score]})\n",
    "  display(scores)\n",
    "  all_data = np.concatenate((train_data, val_data, test_data), axis=0)\n",
    "  y_pred = model.predict(all_data[:,:-3])\n",
    "  all_data_df = pd.DataFrame({'Rating': all_data[:,-3], 'Pred_Rating': y_pred})\n",
    "  all_data_df['Rating'] = all_data_df['Rating'].astype(str)\n",
    "  fig = px.scatter(all_data_df, x=all_data_df.index, y='Pred_Rating', color='Rating',\n",
    "                  title='Predicted ratings from overall model vs Actual Rating in Color')\n",
    "  fig.update_layout(height=1000)\n",
    "  # fig.show()\n",
    "  return model, data, all_data_df\n",
    "\n",
    "\n",
    "def generate_batches(files, batch_size, tune=tune):\n",
    "    counter = 0\n",
    "    while True:\n",
    "        fname = files[counter]\n",
    "\n",
    "        frame = np.load(os.path.join(INPUT_DATA_DIR, f'{tune}_dataset_p{counter+1}.npy'), allow_pickle=True)\n",
    "        counter = (counter + 1) % len(files)\n",
    "\n",
    "        # here is your preprocessing\n",
    "        if tune=='sgm_loud' or tune=='sgm_timbre' or tune=='sgm_pitch' or tune=='sct_data':\n",
    "          input = np.array([np.array(row) for row in frame[:,0]])\n",
    "          output = frame[:, -2].astype(np.float64).reshape(frame[:,-2].shape[0],1)\n",
    "          weight_output = frame[:, -1].astype(np.float64).reshape(frame[:,-1].shape[0],1)\n",
    "        elif tune=='general':\n",
    "          input=np.delete(frame, (3,4), 1)\n",
    "          output=frame[:, 3].reshape(frame[:,3].shape[0],1)\n",
    "          weight_output=frame[:, 4].reshape(frame[:,4].shape[0],1)\n",
    "        elif tune=='genre':\n",
    "          input=frame[:,:-2]\n",
    "          output=frame[:, -2].reshape(frame[:,-2].shape[0],1)\n",
    "          weight_output=frame[:, -1].reshape(frame[:,-1].shape[0],1)\n",
    "\n",
    "        for local_index in range(0, input.shape[0], batch_size):\n",
    "          input_local = input[local_index:(local_index + batch_size)]\n",
    "          output_local = output[local_index:(local_index + batch_size)]\n",
    "          weight_output_local = weight_output[local_index:(local_index + batch_size)]\n",
    "\n",
    "          yield input_local, output_local, weight_output_local\n",
    "\n",
    "\n",
    "def process_feat_list(sample):\n",
    "  '''Converst string list to list'''\n",
    "  return json.loads(sample)\n",
    "\n",
    "\n",
    "def split_y(y_var, rating_key=rating_key):\n",
    "  y_out, y_weight = [], []\n",
    "  for row in y_var:\n",
    "    y_out.append(row[0])\n",
    "    y_weight.append(row[1])\n",
    "  return np.array(y_out), np.array(y_weight)\n",
    "\n",
    "\n",
    "def process_component(data, max_length, col_id_skip=None, shift_param=0.00001):\n",
    "  '''Input is a list and returns normalized dataframe'''\n",
    "  out = []\n",
    "  cols_skip = []\n",
    "  counter = 0\n",
    "  for row in data:  # Loop through song\n",
    "    row_data = []\n",
    "    for item in row: # loop through bar\n",
    "      for i, subitem in enumerate(item): # loop through elements in bar\n",
    "        if col_id_skip:\n",
    "          if i in col_id_skip:\n",
    "            cols_skip.append([f'comp_{counter}'])\n",
    "        counter += 1\n",
    "        row_data.append(subitem)\n",
    "    # Check if row needs padding or trimming\n",
    "    if len(row_data) < max_length:\n",
    "      zero_pads = [0] * (max_length - len(row_data))\n",
    "      row_data.extend(zero_pads)\n",
    "    elif len(row_data) >= max_length:\n",
    "      row_data = row_data[:max_length]\n",
    "    out.append(row_data)\n",
    "\n",
    "  column_names = [f'comp_{i}' for i in range(max_length)]\n",
    "  df = pd.DataFrame(data=out, columns=column_names)\n",
    "  # Apply boxcox normalization to each column\n",
    "  for col in df.columns:\n",
    "    if cols_skip:\n",
    "      if col in cols_skip:\n",
    "        continue\n",
    "    try:\n",
    "      norm_comp = normalize(stats.boxcox(df[col]+shift_param)[0])\n",
    "      df[col] = norm_comp\n",
    "    except:\n",
    "      print('data values are negative, applying normalization first')\n",
    "      print(col)\n",
    "      continue\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def normalize(column, negative=False):\n",
    "    if negative:\n",
    "      for i, song in enumerate(column):\n",
    "        for j, segment in enumerate(song):\n",
    "          for k, item in enumerate(segment):\n",
    "            if item < 0:\n",
    "              column[i][j][k] = -1 * item\n",
    "            else:\n",
    "              column[i][j][k] = (item+0.00001)  * 2\n",
    "      return column\n",
    "    upper = column.max()\n",
    "    lower = column.min()\n",
    "    y = (column - lower)/(upper-lower)\n",
    "    return y.tolist()\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "def chunks(lst, n):\n",
    "      \"\"\"Yield successive n-sized chunks from lst\n",
    "      Inputs:\n",
    "        lst (list): list of items to be split\n",
    "        n (int): number of splits to make\n",
    "      Output:\n",
    "        lst (list): returns list of lists that are broken up into chunks of size n\n",
    "        \"\"\"\n",
    "      for i in range(0, len(lst), n):\n",
    "          yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def normalize_data(data, use_max=None, use_min=None, sav_max=False):\n",
    "  ''' Normalizes a list of data to be values from 0-1.  Uses a max value if\n",
    "      provided\n",
    "  Input:\n",
    "    data (list): list of data points\n",
    "    use_max (float): if specified, uses provided max value\n",
    "    sav_max (bool): if True, save outputs data max.\n",
    "  Output:-\n",
    "    norm_data (list): normalized from 0 - 1\n",
    "    norm_data (float): calculated maximum data point\n",
    "    '''\n",
    "  if use_max:\n",
    "    data_max = use_max\n",
    "    data_min = use_min\n",
    "  else:\n",
    "    data_max = np.max(data)\n",
    "    data_min = np.min(data)\n",
    "  norm_data = (data - data_min) / (data_max - data_min)\n",
    "  if sav_max:\n",
    "    return norm_data, data_max, data_min\n",
    "  else:\n",
    "    return norm_data\n",
    "\n",
    "\n",
    "\n",
    "def rand_sel(source_list, length=None):\n",
    "  '''Creates a random list of values with a random length from a sample list.\n",
    "     List length can be specified or randomly chosen.  For spotify's recommend\n",
    "     function, the max seed length is 5.  Putting length above 5 will break the\n",
    "     code.\n",
    "  Inputs:\n",
    "    source_list (list): list of items to be randomly queried from\n",
    "    length (int): number of items to randomly select from source_list\n",
    "  Outputs:\n",
    "    output (list): randomly selected n number of item/s from source_list based\n",
    "    on specified length'''\n",
    "  if length==None:\n",
    "    length = np.random.randint(len(source_list))\n",
    "  output = list(np.random.choice(source_list, (1,length))[0])\n",
    "  return output\n",
    "\n",
    "\n",
    "def get_date_float(input_str):\n",
    "  '''Convert release date to float with year_number.month percent of year.\n",
    "     If string format not easily interpretable, then return some value for year and month.\n",
    "  Input:\n",
    "    input_str (sting): datetime string\n",
    "  Output:\n",
    "    year (int): year specified in input_str\n",
    "    month (Int): month specified in input_str\n",
    "    '''\n",
    "  if len(input_str) > 5:\n",
    "    try:\n",
    "      time = datetime.strptime(input_str, '%Y-%m-%d')\n",
    "      year = time.timetuple()[0]\n",
    "      month = time.timetuple()[1]\n",
    "    except:\n",
    "      year=2021\n",
    "      month=1\n",
    "  else:\n",
    "    try:\n",
    "      time = datetime.strptime(input_str, '%Y')\n",
    "      year = time.timetuple()[0]\n",
    "      month = 1\n",
    "    except:\n",
    "      year = 2021\n",
    "      month = 1\n",
    "  return year, month\n",
    "\n",
    "\n",
    "def man_ord_encode(item):\n",
    "  '''Manual encode album type column with values between 0 and 1\n",
    "  Input:\n",
    "    item (string): album string name\n",
    "  Output:\n",
    "    out (float): value based on album string name\n",
    "    '''\n",
    "  if item.lower() == 'single':\n",
    "    out = 0.33\n",
    "  elif item.lower() == 'album':\n",
    "    out = 0.66\n",
    "  elif item.lower() == 'compilation':\n",
    "    out=0.99\n",
    "  else:\n",
    "    out=0\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def rec_batch_size(n_rec):\n",
    "  '''Generates a list of API call legnths based on n_rec which is the number\n",
    "     of desired recommendations.  Spotify API has a recommendation of 100 per\n",
    "     recommendation API call.\n",
    "  Input:\n",
    "    n_rec (int): number of desired recommendations\n",
    "  Output:\n",
    "    out (list): list of recommendation limit numbers whereby the max is 100 for\n",
    "    each list\n",
    "  Example:\n",
    "    n_rec = 230, output = [[100], [100], [23]]\n",
    "    '''\n",
    "  if n_rec <= 100:\n",
    "    out = [n_rec]\n",
    "  else:\n",
    "    num = math.ceil(n_rec / 100)\n",
    "    out = ([100] * (num - 1))\n",
    "    last = [n_rec - (num - 1) * 100]\n",
    "    out.extend(last)\n",
    "  return out\n",
    "\n",
    "\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        print(lr0 * 0.1**(epoch / s))\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "\n",
    "class MyHyperModel(kt.HyperModel):\n",
    "  '''Complicated model tuning class'''\n",
    "  def build(self, hp):\n",
    "\n",
    "    # Tuning params\n",
    "    optimizer = hp.Choice('optimizer', values=['adam', 'nadam', 'adamax', 'rmsprop',])\n",
    "                                              #  'nesterov'])\n",
    "    # n_layers = hp.Int('n_layers', min_value=13, max_value=16, step=1)\n",
    "    n_layers=1\n",
    "    n_units=1\n",
    "    reg_type = hp.Choice('reg_type', values=['l1', 'l2', 'none'])\n",
    "    max_norm = hp.Int('max_norm', min_value=1, max_value=10000, step=10)\n",
    "    activation = hp.Choice('activation', values=['relu', 'selu', 'elu'])\n",
    "    p_drop = hp.Float('p_drop', min_value=0.0, max_value=0.9, step=0.01)\n",
    "    batch_norm = hp.Choice('batch_norm', values=[0, 1])\n",
    "    reg_max = hp.Float('reg_max', min_value=0.0001, max_value=0.4, step=0.0001)\n",
    "\n",
    "    if optimizer=='nadam':\n",
    "      with hp.conditional_scope(\"optimizer\", [\"nadam\"]):\n",
    "        constant_lr = hp.Choice(\"constant_learning_rate\", values=[1E-2, 1E-3, 1e-4, 3e-4, 4e-4, 5e-4, 1e-5])\n",
    "        opt = tf.keras.optimizers.Nadam(learning_rate=constant_lr)\n",
    "    else:\n",
    "      with hp.conditional_scope(\"optimizer\", [\"adam\", \"adamax\", 'rmsprop', 'nesterov']):\n",
    "        lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "          initial_learning_rate=hp.Choice(\"initial_learning_rate\", values=[1E-2, 1E-3, 1e-4, 3e-4, 4e-4, 5e-4]),\n",
    "          decay_steps=hp.Int('decay_steps', min_value=100, max_value=5000, step=10),\n",
    "          decay_rate=hp.Float('decay_rate', min_value=0.3, max_value=1, step=0.01)\n",
    "          )\n",
    "      # Set optimizer learning rate\n",
    "      if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "      elif optimizer == 'adamax':\n",
    "        opt = tf.keras.optimizers.Adamax(learning_rate=lr)\n",
    "      elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "      elif optimizer == 'nesterov':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "\n",
    "    # Adjust batch normalization\n",
    "    if batch_norm==1:\n",
    "      use_bias=False\n",
    "    else:\n",
    "      use_bias=True\n",
    "\n",
    "    # Adjust activation initializer if selu\n",
    "    if activation=='selu':\n",
    "      kernel_initializer = 'lecun_normal'\n",
    "    else:\n",
    "      kernel_initializer = 'he_normal'\n",
    "\n",
    "    # Set Weight regularizer\n",
    "    if reg_type == 'l1':\n",
    "      kernel_regularizer = l1(reg_max)\n",
    "    elif reg_type == 'l2':\n",
    "      kernel_regularizer = l2(reg_max)\n",
    "    else:\n",
    "      kernel_regularizer=None\n",
    "\n",
    "    # Set weight value constraint\n",
    "    kernel_constraint = MaxNorm(max_value=max_norm)\n",
    "\n",
    "    # Main model\n",
    "    input_main = tf.keras.layers.Input(shape=INPUT_DIM)\n",
    "\n",
    "    if batch_norm==1:\n",
    "        x = BatchNormalization()(input_main)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "      if i==0:\n",
    "        if batch_norm==1:\n",
    "          x = Dense(hp.Int('n_layer_1', min_value=10, max_value=5000, step=5),\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_constraint=kernel_constraint,\n",
    "                    kernel_initializer=kernel_initializer,\n",
    "                    use_bias=use_bias)(x)\n",
    "        else:\n",
    "          x = Dense(hp.Int('n_layer_1', min_value=10, max_value=5000, step=5),\n",
    "                    kernel_regularizer=kernel_regularizer,\n",
    "                    kernel_constraint=kernel_constraint,\n",
    "                    kernel_initializer=kernel_initializer,\n",
    "                    use_bias=use_bias)(input_main)\n",
    "      else:\n",
    "        x = Dense(n_units,\n",
    "                  kernel_regularizer=kernel_regularizer,\n",
    "                  kernel_constraint=kernel_constraint,\n",
    "                  kernel_initializer=kernel_initializer,\n",
    "                  use_bias=use_bias)(x)\n",
    "      if batch_norm==1:\n",
    "        x = BatchNormalization()(x)\n",
    "      x = Activation(activation)(x)\n",
    "      if activation == 'selu':\n",
    "        x = AlphaDropout(p_drop)(x)\n",
    "      else:\n",
    "        x = Dropout(p_drop)(x)\n",
    "\n",
    "    pr_auc = tf.keras.metrics.AUC(num_thresholds=1000, curve=\"PR\", name='pr_auc', from_logits=True)\n",
    "    output = Dense(1, activation='tanh')(x)\n",
    "\n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs=[input_main], outputs=[output])\n",
    "    model.compile(loss='squared_hinge', optimizer=opt,\n",
    "                  # metrics=['accuracy', pr_auc],\n",
    "                  weighted_metrics=['accuracy', pr_auc])\n",
    "    return model\n",
    "\n",
    "  def fit(self, hp, model, *args, **kwargs):\n",
    "      return model.fit(\n",
    "          *args,\n",
    "          batch_size=hp.Int(\"batch_size\", min_value=10, max_value=180, step=2),\n",
    "          **kwargs,)\n",
    "\n",
    "\n",
    "def vis_preds(data, data_vec, y_data, models, fig_title, pred_songs=False):\n",
    "  if pred_songs:\n",
    "    if 'rating' in df_test.columns: df_test.drop(columns='rating', inplace=True)\n",
    "    if len(models)>1:\n",
    "      out_pred = get_avg_predict(models, data, data_vec)\n",
    "    else:\n",
    "      model=models[0]\n",
    "      out_pred = model.predict(x=(data, data_vec))\n",
    "    fig = px.histogram(out_pred, nbins=200, title=fig_title)\n",
    "  else:\n",
    "    if len(models)>1:\n",
    "      train_pred = get_avg_predict(models, data, data_vec)\n",
    "    else:\n",
    "      model=models[0]\n",
    "      train_pred = model.predict(x=(data, data_vec))\n",
    "    cols = list(df.columns)\n",
    "    cols.remove('rating')\n",
    "    cols.remove('genres')\n",
    "    ac_test = pd.DataFrame(data=data, columns=cols)\n",
    "    ac_test['rating'] = y_data\n",
    "    ac_test['pred_rating'] = train_pred\n",
    "    ac_test.sort_values('rating', ascending=True, inplace=True)\n",
    "    fig = px.histogram(ac_test, x='pred_rating', color='rating', opacity=0.75,\n",
    "                       barmode='overlay', nbins=200, title=fig_title)\n",
    "  return fig.show()\n",
    "\n",
    "\n",
    "def get_avg_predict(models, data, data_vec):\n",
    "  '''Return average predicted rating from models'''\n",
    "  for i, model in enumerate(models):\n",
    "      if i==0:\n",
    "        out_pred = model.predict(x=(data, data_vec))\n",
    "      else:\n",
    "        out_pred += model.predict(x=(data, data_vec))\n",
    "  out_pred = out_pred / len(models)\n",
    "  return out_pred\n",
    "\n",
    "\n",
    "def calc_pred_thresh(models, percen_split=99):\n",
    "  if 'rating' in df_test.columns: df_test.drop(columns='rating', inplace=True)\n",
    "  if len(models)>1:\n",
    "    out_pred = get_avg_predict(models, recs, df_vec_test)\n",
    "  else:\n",
    "    model = models[0]\n",
    "    out_pred = model.predict(x=(recs, df_vec_test))\n",
    "  threshold = np.percentile(out_pred, percen_split)\n",
    "  return threshold\n",
    "\n",
    "\n",
    "\n",
    "def save_model(model):\n",
    "  '''Saves and downloads model to desktop'''\n",
    "  model.save(f'saved_model/saved_model')\n",
    "  !zip -r /content/saved_model.zip  /content/saved_model\n",
    "  from google.colab import files\n",
    "  files.download(\"/content/saved_model.zip\")\n",
    "  print('Model saved and Downloaded')\n",
    "\n",
    "def load_model(file_path):\n",
    "  '''Load tensorflow model from Gdrive'''\n",
    "  zip_ref = zipfile.ZipFile(file_path, 'r')\n",
    "  zip_ref.extractall()\n",
    "  zip_ref.close()\n",
    "  # Load Model\n",
    "  model = tf.keras.models.load_model('content/saved_model/saved_model')\n",
    "  return model\n",
    "\n",
    "\n",
    "def data_splitter_for_model(data, col_idx=5):\n",
    "  '''Splits dataframe into two numpy arrays with song genre vectorization split into the second array'''\n",
    "  # Check if dataframe input\n",
    "  if type(data)!=np.ndarray:\n",
    "    data = data.to_numpy()\n",
    "  # Grab vectorization column\n",
    "  data_vec = np.array([row[col_idx] for row in data]).astype('float32')\n",
    "  # Remove vectorization column from data\n",
    "  data = np.delete(data, col_idx, 1)\n",
    "  # Convert arrays to float32 type\n",
    "  data = np.array(data).astype('float32')\n",
    "  data_vec = np.array(data_vec).astype('float32')\n",
    "  return data, data_vec\n",
    "\n",
    "\n",
    "def process_str_list(sample):\n",
    "  '''Converst string list to list'''\n",
    "  sample = sample.replace(\"\\n\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "  sample_list = list(sample.split(\" \"))\n",
    "  sample_list=[x for x in sample_list if len(x)>0]\n",
    "  sample_list = [float(x) for x in sample_list]\n",
    "  return sample_list\n",
    "\n",
    "\n",
    "def encode_data(data):\n",
    "  '''Applies two models to encode input data'''\n",
    "  if 'rating' in list(data.columns):\n",
    "    data = data.drop(columns='rating')\n",
    "  _, data_vec = data_splitter_for_model(data)\n",
    "  data_vec_comp = vec_model.predict(data_vec)\n",
    "  data.drop(columns=['genres'], inplace=True)\n",
    "  data[vec_col_nams] = data_vec_comp\n",
    "  data_processed = enc_model.predict(data)\n",
    "  return data_processed\n",
    "\n",
    "\n",
    "def encode_data_vec(data):\n",
    "  '''Applies vec model to encode input data'''\n",
    "  if 'rating' in list(data.columns):\n",
    "    data = data.drop(columns='rating')\n",
    "  _, data_vec = data_splitter_for_model(data)\n",
    "  data_vec_comp = vec_model.predict(data_vec)\n",
    "  data.drop(columns=['genres'], inplace=True)\n",
    "  data[vec_col_nams] = data_vec_comp\n",
    "  return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq0xeGzKJ0Dx"
   },
   "outputs": [],
   "source": [
    "if tune!='lr_overall':\n",
    "  class CustomTuner(kt.Tuner):\n",
    "    def _build_and_fit_model(self, trial, *args, **kwargs):\n",
    "      hp = trial.hyperparameters\n",
    "      model = self._try_build(hp)\n",
    "      results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
    "      tuner_utils.validate_trial_results(\n",
    "          results, self.oracle.objective, \"HyperModel.fit()\"\n",
    "      )\n",
    "      ### My additions ###\n",
    "      print('End Execution')\n",
    "      del model\n",
    "      ### End my additions ###\n",
    "      return results\n",
    "    def run_trial(self, trial, *args, **kwargs):\n",
    "      config = tf.compat.v1.ConfigProto(log_device_placement=True)\n",
    "      config.gpu_options.allow_growth = True\n",
    "      session = tf.compat.v1.Session(config=config)\n",
    "      backend.set_session(session)\n",
    "      original_callbacks = kwargs.pop(\"callbacks\", [])\n",
    "      kwargs['batch_size'] = trial.hyperparameters.Int('batch_size', 10, 80, step=2)\n",
    "      kwargs['x'] = load_dataset(TRAIN_FILES, trial.hyperparameters.get('batch_size'))\n",
    "      kwargs['validation_data'] = load_dataset(VALIDATION_FILES, trial.hyperparameters.get('batch_size'))\n",
    "      # Run the training process multiple times.\n",
    "      histories = []\n",
    "      for execution in range(self.executions_per_trial):\n",
    "          copied_kwargs = copy.copy(kwargs)\n",
    "          callbacks = self._deepcopy_callbacks(original_callbacks)\n",
    "          self._configure_tensorboard_dir(callbacks, trial, execution)\n",
    "          callbacks.append(tuner_utils.TunerCallback(self, trial))\n",
    "          # Only checkpoint the best epoch across all executions.\n",
    "          copied_kwargs[\"callbacks\"] = callbacks\n",
    "          obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
    "\n",
    "          histories.append(obj_value)\n",
    "\n",
    "      ##### MY ADDITION #####\n",
    "      backend.clear_session()\n",
    "      gc.collect()\n",
    "        # Get best score from all executions and return to the tuner\n",
    "      if regression==False:\n",
    "        best_scores = [max(hist.history[f'val_{TUNE_METRIC_NAME}']) for hist in histories]\n",
    "        return max(best_scores)\n",
    "      elif regression==True:\n",
    "        best_scores = [min(hist.history[f'val_{TUNE_METRIC_NAME}']) for hist in histories]\n",
    "        return min(best_scores)\n",
    "      ##### END MY ADDITION #####\n",
    "\n",
    "\n",
    "  class BayesianOptimization(CustomTuner):\n",
    "      def __init__(\n",
    "          self,\n",
    "          hypermodel=None,\n",
    "          objective=None,\n",
    "          max_trials=10,\n",
    "          num_initial_points=2,\n",
    "          alpha=1e-4,\n",
    "          beta=2.8,\n",
    "          seed=None,\n",
    "          hyperparameters=None,\n",
    "          tune_new_entries=True,\n",
    "          allow_new_entries=True,\n",
    "          max_retries_per_trial=0,\n",
    "          max_consecutive_failed_trials=1000,\n",
    "          **kwargs\n",
    "      ):\n",
    "          oracle = BayesianOptimizationOracle(\n",
    "              objective=objective,\n",
    "              max_trials=max_trials,\n",
    "              num_initial_points=num_initial_points,\n",
    "              alpha=alpha,\n",
    "              beta=beta,\n",
    "              seed=seed,\n",
    "              hyperparameters=hyperparameters,\n",
    "              tune_new_entries=tune_new_entries,\n",
    "              allow_new_entries=allow_new_entries,\n",
    "              max_retries_per_trial=max_retries_per_trial,\n",
    "              max_consecutive_failed_trials=max_consecutive_failed_trials\n",
    "          )\n",
    "          super(\n",
    "              BayesianOptimization, self,\n",
    "          ).__init__(oracle=oracle, hypermodel=hypermodel, **kwargs)\n",
    "          if scipy is None:\n",
    "              raise ImportError(\n",
    "                  \"Please install scipy before using the `BayesianOptimization`.\"\n",
    "              )\n",
    "          self.seed=seed\n",
    "          self.num_initial_points = num_initial_points\n",
    "          self.alpha = alpha\n",
    "          self.beta = beta\n",
    "          self._random_state = np.random.RandomState(self.seed)\n",
    "          self.gpr = self._make_gpr()\n",
    "      def _make_gpr(self):\n",
    "        return sklearn.gaussian_process.GaussianProcessRegressor(\n",
    "            kernel=sklearn.gaussian_process.kernels.Matern(nu=2.5),\n",
    "            n_restarts_optimizer=0,\n",
    "            normalize_y=True,\n",
    "            alpha=self.alpha,\n",
    "            random_state=self.seed,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tune!='lr_overall':\n",
    "  class LazyModule:\n",
    "      def __init__(self, name, pip_name=None):\n",
    "          self.name = name\n",
    "          pip_name = pip_name or name\n",
    "          self.pip_name = pip_name\n",
    "          self.module = None\n",
    "          self._available = None\n",
    "\n",
    "\n",
    "      @property\n",
    "      def available(self):\n",
    "          if self._available is None:\n",
    "              try:\n",
    "                  self.initialize()\n",
    "                  self._available = True\n",
    "              except ImportError:\n",
    "                  self._available = False\n",
    "          return self._available\n",
    "\n",
    "      def initialize(self):\n",
    "          try:\n",
    "              self.module = importlib.import_module(self.name)\n",
    "          except ImportError:\n",
    "              raise ImportError(\n",
    "                  f\"This requires the {self.name} module. \"\n",
    "                  f\"You can install it via `pip install {self.pip_name}`\"\n",
    "              )\n",
    "\n",
    "      def __getattr__(self, name):\n",
    "          if name == \"_api_export_path\":\n",
    "              raise AttributeError\n",
    "          if self.module is None:\n",
    "              self.initialize()\n",
    "          return getattr(self.module, name)\n",
    "\n",
    "\n",
    "  tensorflow = LazyModule(\"tensorflow\")\n",
    "  gfile = LazyModule(\"tensorflow.io.gfile\", pip_name=\"tensorflow\")\n",
    "  tensorflow_io = LazyModule(\"tensorflow_io\")\n",
    "  scipy = LazyModule(\"scipy\")\n",
    "\n",
    "  class CustomStopper(keras.callbacks.EarlyStopping):\n",
    "      def __init__(self, monitor='val_loss',\n",
    "               min_delta=0, patience=0, verbose=0, mode='auto', start_epoch = 10, baseline=3, restore_best_weights=True): # add argument for starting epoch\n",
    "          super(CustomStopper, self).__init__()\n",
    "          self.start_epoch = start_epoch\n",
    "\n",
    "      def on_epoch_end(self, epoch, logs=None):\n",
    "          if epoch > self.start_epoch:\n",
    "              super().on_epoch_end(epoch, logs)\n",
    "\n",
    "class CustomStopperCheckpoints(keras.callbacks.EarlyStopping):\n",
    "  def __init__(self, monitor=f'val_{TUNE_METRIC_NAME}',\n",
    "                 min_delta=0.01, patience=10, verbose=0, mode='auto', start_epoch_1=10,\n",
    "                 baseline_1=13, start_epoch_2=25, baseline_2=11, restore_best_weights=True): # add argument for starting epoch\n",
    "      super(CustomStopperCheckpoints, self).__init__()\n",
    "      self.monitor=monitor\n",
    "      self.patience=patience\n",
    "      self.min_delta=min_delta\n",
    "      self.restore_best_weights=restore_best_weights\n",
    "      self.verbose=verbose\n",
    "      self.mode='auto'\n",
    "      self.start_epoch_1 = start_epoch_1\n",
    "      self.baseline_1 = baseline_1\n",
    "      self.start_epoch_2 = start_epoch_2\n",
    "      self.baseline_2 = baseline_2\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    if self.monitor_op is None:\n",
    "      self._set_monitor_op()\n",
    "    current = self.get_monitor_value(logs)\n",
    "    if current is None or epoch < self.start_epoch_1:\n",
    "      if np.isnan(current) and epoch<=1:\n",
    "        self.stopped_epoch = epoch\n",
    "        self.model.stop_training = True\n",
    "      else:\n",
    "        return\n",
    "    elif epoch==self.start_epoch_1 and self._is_improvement(current, self.baseline_1)==False:\n",
    "      self.stopped_epoch = epoch\n",
    "      self.model.stop_training = True\n",
    "    elif epoch > self.start_epoch_1 and epoch < self.start_epoch_2:\n",
    "      return\n",
    "    elif epoch==self.start_epoch_2 and self._is_improvement(current, self.baseline_2)==False:\n",
    "      self.stopped_epoch = epoch\n",
    "      self.model.stop_training = True\n",
    "    elif epoch > self.start_epoch_2:\n",
    "      if self.restore_best_weights and self.best_weights is None:\n",
    "          self.best_weights = self.model.get_weights()\n",
    "          self.best_epoch = epoch\n",
    "      self.wait += 1\n",
    "      if epoch==self.start_epoch_1 and self._is_improvement(current, self.baseline_1)==False:\n",
    "        self.stopped_epoch = epoch\n",
    "        self.model.stop_training = True\n",
    "      if self._is_improvement(current, self.best):\n",
    "          self.best = current\n",
    "          self.best_epoch = epoch\n",
    "          if self.restore_best_weights:\n",
    "              self.best_weights = self.model.get_weights()\n",
    "          self.wait = 0\n",
    "          return\n",
    "\n",
    "      if self.wait >= self.patience and epoch > 0:\n",
    "          # Patience has been exceeded: stop training\n",
    "          self.stopped_epoch = epoch\n",
    "          self.model.stop_training = True\n",
    "    else:\n",
    "      return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if tune!='lr_overall':\n",
    "  # Custom Model Saving Callback\n",
    "  def is_remote_path(filepath):\n",
    "      if re.match(r\"^(/cns|/cfs|/gcs|/hdfs|.*://).*$\", str(filepath)):\n",
    "          return True\n",
    "      return False\n",
    "\n",
    "  def _raise_if_no_gfile(path):\n",
    "      raise ValueError(\n",
    "          \"Handling remote paths requires installing TensorFlow \"\n",
    "          f\"(in order to use gfile). Received path: {path}\"\n",
    "      )\n",
    "\n",
    "  def path_to_string(path):\n",
    "      if isinstance(path, os.PathLike):\n",
    "          return os.fspath(path)\n",
    "      return path\n",
    "\n",
    "  def exists(path):\n",
    "      if is_remote_path(path):\n",
    "          if gfile.available:\n",
    "              return gfile.exists(path)\n",
    "          else:\n",
    "              _raise_if_no_gfile(path)\n",
    "      return os.path.exists(path)\n",
    "\n",
    "  def makedirs(path):\n",
    "      if is_remote_path(path):\n",
    "          if gfile.available:\n",
    "              return gfile.makedirs(path)\n",
    "          else:\n",
    "              _raise_if_no_gfile(path)\n",
    "      return os.makedirs(path)\n",
    "\n",
    "  class OverallModelCheckpoint(ModelCheckpoint):\n",
    "      def __init__(\n",
    "          self,\n",
    "          filepath,\n",
    "          base_dir=checkpoint_dir,\n",
    "          monitor=f\"val_{TUNE_METRIC_NAME}\",\n",
    "          verbose=0,\n",
    "          save_best_only=False,\n",
    "          save_weights_only=False,\n",
    "          mode=\"auto\",\n",
    "          save_freq=\"epoch\",\n",
    "          initial_value_threshold=None,\n",
    "      ):\n",
    "          global BEST_SCORE\n",
    "          super().__init__(\n",
    "              filepath + '.keras',\n",
    "              monitor=f\"val_{TUNE_METRIC_NAME}\",\n",
    "              verbose=0,\n",
    "              save_best_only=False,\n",
    "              save_weights_only=False,\n",
    "              mode=\"auto\",\n",
    "              save_freq=\"epoch\",\n",
    "              initial_value_threshold=None,\n",
    "          )\n",
    "          self.monitor = monitor\n",
    "          self.verbose = verbose\n",
    "          self.base_dir = checkpoint_dir\n",
    "          self.filepath = path_to_string(self.base_dir + f'_{BEST_SCORE}.keras')\n",
    "          self.save_best_only = save_best_only\n",
    "          self.save_weights_only = save_weights_only\n",
    "          self.save_freq = save_freq\n",
    "          self._batches_seen_since_last_saving = 0\n",
    "          self._last_batch_seen = 0\n",
    "\n",
    "          if mode not in [\"auto\", \"min\", \"max\"]:\n",
    "              warnings.warn(\n",
    "                  f\"ModelCheckpoint mode '{mode}' is unknown, \"\n",
    "                  \"fallback to auto mode.\",\n",
    "                  stacklevel=2,\n",
    "              )\n",
    "              mode = \"auto\"\n",
    "\n",
    "          if mode == \"min\":\n",
    "              self.monitor_op = np.less\n",
    "              if BEST_SCORE is None:\n",
    "                  BEST_SCORE = np.Inf\n",
    "          elif mode == \"max\":\n",
    "              self.monitor_op = np.greater\n",
    "              if BEST_SCORE is None:\n",
    "                  BEST_SCORE = -np.Inf\n",
    "          else:\n",
    "              if \"acc\" in self.monitor or self.monitor.startswith(\"fmeasure\"):\n",
    "                  self.monitor_op = np.greater\n",
    "                  if BEST_SCORE is None:\n",
    "                      BEST_SCORE = -np.Inf\n",
    "              else:\n",
    "                  self.monitor_op = np.less\n",
    "                  if BEST_SCORE is None:\n",
    "                      BEST_SCORE = np.Inf\n",
    "\n",
    "          if self.save_freq != \"epoch\" and not isinstance(self.save_freq, int):\n",
    "              raise ValueError(\n",
    "                  f\"Unrecognized save_freq: {self.save_freq}. \"\n",
    "                  \"Expected save_freq are 'epoch' or integer values\"\n",
    "              )\n",
    "      def _save_model(self, epoch, batch, logs):\n",
    "          \"\"\"Saves the model.\n",
    "\n",
    "          Args:\n",
    "              epoch: the epoch this iteration is in.\n",
    "              batch: the batch this iteration is in. `None` if the `save_freq`\n",
    "                  is set to `\"epoch\"`.\n",
    "              logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n",
    "          \"\"\"\n",
    "          global BEST_SCORE\n",
    "          logs = logs or {}\n",
    "\n",
    "          filepath = self._get_file_path(epoch, batch, logs)\n",
    "          dirname = os.path.dirname(filepath)\n",
    "          if dirname and not exists(dirname):\n",
    "              makedirs(dirname)\n",
    "\n",
    "          try:\n",
    "              if self.save_best_only:\n",
    "                  current = logs.get(self.monitor)\n",
    "                  if current is None:\n",
    "                      warnings.warn(\n",
    "                          f\"Can save best model only with {self.monitor} \"\n",
    "                          \"available, skipping.\",\n",
    "                          stacklevel=2)\n",
    "                  else:\n",
    "                      if self.monitor_op(current, BEST_SCORE):\n",
    "                          filepath = path_to_string(self.base_dir + f'_{BEST_SCORE}.keras')\n",
    "                          self.filepath = filepath\n",
    "                          try:\n",
    "                              os.remove(self.filepath)\n",
    "                          except:\n",
    "                              print(f'failed to remove: {self.filepath}')\n",
    "                          if self.verbose > 0:\n",
    "                              io_utils.print_msg(\n",
    "                                  f\"\\nEpoch {epoch + 1}: {self.monitor} \"\n",
    "                                  \"improved \"\n",
    "                                  f\"from {BEST_SCORE:.5f} to {current:.5f}, \"\n",
    "                                  f\"saving model to {self.filepath}\"\n",
    "                              )\n",
    "                          # Delete previous best and save new best\n",
    "                          BEST_SCORE = current\n",
    "                          filepath = path_to_string(self.base_dir + f'_{BEST_SCORE}.keras')\n",
    "                          self.filepath = filepath\n",
    "                          # output new global variable\n",
    "                          if self.save_weights_only:\n",
    "                              self.model.save_weights(self.filepath, overwrite=True)\n",
    "                          else:\n",
    "                              self.model.save(self.filepath, overwrite=True)\n",
    "                      else:\n",
    "                          if self.verbose >= 2:\n",
    "                              io_utils.print_msg(\n",
    "                                  f\"\\nEpoch {epoch + 1}: \"\n",
    "                                  f\"{self.monitor} did not improve \"\n",
    "                                  f\"from {BEST_SCORE:.5f}\"\n",
    "                              )\n",
    "              else:\n",
    "                  if self.verbose > 0:\n",
    "                      io_utils.print_msg(\n",
    "                          f\"\\nEpoch {epoch + 1}: saving model to {self.filepath}\"\n",
    "                      )\n",
    "                  if self.save_weights_only:\n",
    "                      self.model.save_weights(self.filepath, overwrite=True)\n",
    "                  else:\n",
    "                      self.model.save(self.filepath, overwrite=True)\n",
    "          except IsADirectoryError:  # h5py 3.x\n",
    "              raise IOError(\n",
    "                  \"Please specify a non-directory filepath for \"\n",
    "                  \"ModelCheckpoint. Filepath used is an existing \"\n",
    "                  f\"directory: {self.filepath}\"\n",
    "              )\n",
    "          except IOError as e:  # h5py 2.x\n",
    "              if \"is a directory\" in str(e.args[0]).lower():\n",
    "                  raise IOError(\n",
    "                      \"Please specify a non-directory filepath for \"\n",
    "                      \"ModelCheckpoint. Filepath used is an existing \"\n",
    "                      f\"directory: f{self.filepath}\"\n",
    "                  )\n",
    "              # Re-throw the error for any other causes.\n",
    "              raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fucA0ySgwXZ1"
   },
   "source": [
    "## Time Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tune_mode=='other':\n",
    "  class MyHyperModel(kt.HyperModel):\n",
    "      '''Complicated model tuning class'''\n",
    "      def __init__(self, loss_metric, regression):\n",
    "        self.loss_metric = loss_metric\n",
    "        self.regression = regression\n",
    "\n",
    "      def build(self, hp):\n",
    "        optimizer = hp.Choice('optimizer', values=['adam', 'nadam', 'adamax', 'rmsprop','adafactor', 'lion'])\n",
    "        reg_type = hp.Choice('reg_type', values=['l1', 'l2', 'l1l2', 'None'])\n",
    "        max_norm = hp.Int('max_norm', min_value=1, max_value=10000, step=10)\n",
    "        p_drop = hp.Float('p_drop', min_value=0.0, max_value=0.6, step=0.01)\n",
    "        batch_norm = hp.Choice('batch_norm', values=[0, 1])\n",
    "        activation = hp.Choice('activation', values=['relu', 'selu', 'elu', 'gelu', 'leaky_relu'])\n",
    "        activation_layer = hp.Choice('activation_layer', values=['Relu', 'Elu', 'LeakyReLU'])\n",
    "        pool_layer = hp.Choice('pool_layer', ['local_max', 'local_average'])\n",
    "        pool_size = hp.Int('pool_size', min_value=1, max_value=30, step=1)\n",
    "        num_conv_layers=hp.Choice('num_conv_layers', [1,2,3])\n",
    "        kernel_size=hp.Int('kernel_size', min_value=1, max_value=16, step=1)\n",
    "        stride=hp.Int('stride', min_value=1, max_value=32, step=1)\n",
    "        dense_size=hp.Int('dense_size', min_value=1, max_value=500, step=1)\n",
    "        initial_learning_rate=hp.Choice(\"initial_learning_rate\", values=[1E-2, 1E-3, 1e-4, 3e-4, 4e-4, 5e-4]),\n",
    "        with hp.conditional_scope('optimizer', ['adam','adamax', 'rmsprop','adafactor', 'lion']):\n",
    "          if optimizer!='nadam':\n",
    "            decay_steps=hp.Int('decay_steps', min_value=10, max_value=5000, step=10),\n",
    "            decay_rate=hp.Float('decay_rate', min_value=0.5, max_value=1, step=0.01)\n",
    "            lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                    initial_learning_rate=initial_learning_rate,\n",
    "                    decay_steps=hp.get('decay_steps'),\n",
    "                    decay_rate=hp.get('decay_rate'))\n",
    "        if batch_norm==1:\n",
    "          use_bias=False\n",
    "        else:\n",
    "          use_bias=True\n",
    "        # Set Weight regularizer\n",
    "        if reg_type =='None':\n",
    "            kernel_regularizer=None\n",
    "        else:\n",
    "          with hp.conditional_scope('reg_type', ['l1', 'l2', 'l1l2']):\n",
    "            if reg_type !='None':\n",
    "              reg_max = hp.Float('reg_max', min_value=0.005, max_value=0.4, step=0.001)\n",
    "              if reg_type == 'l1':\n",
    "                kernel_regularizer = l1(reg_max)\n",
    "              elif reg_type == 'l2':\n",
    "                kernel_regularizer = l2(reg_max)\n",
    "              elif reg_type=='l1l2':\n",
    "                kernel_regularizer = L1L2(l1=reg_max, l2=reg_max)\n",
    "      # Set weight value constraint\n",
    "        kernel_constraint = MaxNorm(max_value=max_norm)\n",
    "        # Set optimizer learning rate\n",
    "        if optimizer == 'adam':\n",
    "          opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        elif optimizer == 'adamax':\n",
    "          opt = tf.keras.optimizers.Adamax(learning_rate=lr)\n",
    "        elif optimizer == 'rmsprop':\n",
    "          opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "        elif optimizer == 'nesterov':\n",
    "          opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "        elif optimizer == 'adafactor':\n",
    "            opt = tf.keras.optimizers.Adafactor(learning_rate=lr)\n",
    "        elif optimizer == 'lion':\n",
    "            opt = tf.keras.optimizers.Lion(learning_rate=lr)\n",
    "        elif optimizer == 'nadam':\n",
    "            opt = tf.keras.optimizers.Nadam(learning_rate=hp.get(\"initial_learning_rate\"))\n",
    "\n",
    "        if pool_layer=='local_max':\n",
    "          pool_layer = tf.keras.layers.MaxPooling1D(pool_size, padding='same')\n",
    "        else:\n",
    "          pool_layer = tf.keras.layers.AveragePooling1D(pool_size, padding='same')\n",
    "\n",
    "        ### Model Building\n",
    "        model = tf.keras.models.Sequential()\n",
    "        ## CONV SECTION\n",
    "        model.add(tf.keras.layers.Conv1D(filters=hp.Int('conv_1_filter', min_value=2, max_value=128, step=1),\n",
    "                                         kernel_size=kernel_size, dilation_rate=stride, activation=activation,\n",
    "                                         padding='same', input_shape=INPUT_DIM))\n",
    "        model.add(pool_layer)\n",
    "        with hp.conditional_scope('num_conv_layers', [2,3]):\n",
    "          if num_conv_layers>1:\n",
    "            model.add(tf.keras.layers.Conv1D(filters=hp.Int('conv_2_filter', min_value=2, max_value=128, step=1),\n",
    "                                         kernel_size=kernel_size, dilation_rate=stride, activation=activation,\n",
    "                                         padding='same', input_shape=INPUT_DIM))\n",
    "            if batch_norm==1:\n",
    "              model.add(BatchNormalization())\n",
    "            model.add(Dropout(p_drop))\n",
    "            if activation_layer=='Relu':\n",
    "              model.add(ReLU())\n",
    "            elif activation_layer=='LeakyReLU':\n",
    "              model.add(LeakyReLU())\n",
    "            elif activation_layer=='PReLU':\n",
    "              model.add(PReLU())\n",
    "            else:\n",
    "              model.add(ELU())\n",
    "            model.add(pool_layer)\n",
    "        with hp.conditional_scope('num_conv_layers', [3]):\n",
    "          if num_conv_layers>2:\n",
    "            model.add(tf.keras.layers.Conv1D(filters=hp.Int('conv_3_filter', min_value=2, max_value=128, step=1),\n",
    "                                         kernel_size=kernel_size, dilation_rate=stride, activation=activation,\n",
    "                                         padding='same', input_shape=INPUT_DIM))\n",
    "            if batch_norm==1:\n",
    "              model.add(BatchNormalization())\n",
    "            model.add(Dropout(p_drop))\n",
    "            if activation_layer=='Relu':\n",
    "              model.add(ReLU())\n",
    "            elif activation_layer=='LeakyReLU':\n",
    "              model.add(LeakyReLU())\n",
    "            elif activation_layer=='PReLU':\n",
    "              model.add(PReLU())\n",
    "            else:\n",
    "              model.add(ELU())\n",
    "            model.add(pool_layer)\n",
    "\n",
    "        ## LSTM SECTION\n",
    "        sequences=False\n",
    "        model.add(Bidirectional(LSTM(hp.Int('lstm_1', min_value=2, max_value=300, step=1), return_sequences=sequences)))\n",
    "        model.add(Dropout(p_drop))\n",
    "\n",
    "        ## DENSE SECTION\n",
    "        model.add(Dense(hp.Int('last_hidden', min_value=1, max_value=500, step=1),\n",
    "                                              kernel_regularizer=kernel_regularizer,\n",
    "                                              kernel_constraint=kernel_constraint, use_bias=use_bias))\n",
    "        if batch_norm==1:\n",
    "          model.add(BatchNormalization())\n",
    "        model.add(Activation(activation))\n",
    "        if self.regression==False: # classification\n",
    "          model.add(tf.keras.layers.Dense(1, activation='tanh'))\n",
    "          pr_auc = tf.keras.metrics.AUC(num_thresholds=1000, curve=\"PR\", name='pr_auc', from_logits=True)\n",
    "          model.compile(loss='squared_hinge', optimizer=opt,\n",
    "                    weighted_metrics=['accuracy', pr_auc])\n",
    "        elif self.regression==True: # Regression\n",
    "          model.add(Dense(1))\n",
    "          model.compile(loss=self.loss_metric, optimizer=opt,\n",
    "                    weighted_metrics=[self.loss_metric],\n",
    "                       #### REMOVE THIS AFTER THE FIX\n",
    "                        jit_compile=False\n",
    "                       )\n",
    "\n",
    "        return model\n",
    "\n",
    "      def fit(self, hp, model, *args, **kwargs):\n",
    "        return model.fit(*args,**kwargs,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3WT6m4j5fd9"
   },
   "source": [
    "## Hypermodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tune_mode=='genre') or (tune_mode=='overall') or (tune_mode=='nn_trim') or (tune_mode=='model'):\n",
    "\n",
    "  class MyHyperModel(kt.HyperModel):\n",
    "    '''Complicated model tuning class'''\n",
    "    def __init__(self, loss_metric, regression):\n",
    "      self.loss_metric = loss_metric\n",
    "      self.regression = regression\n",
    "\n",
    "    def build(self, hp):\n",
    "\n",
    "      # Tuning params\n",
    "      optimizer = hp.Choice('optimizer', values=['adam', 'nadam', 'adamax', 'rmsprop', 'lion'])\n",
    "                                                #  'nesterov'])\n",
    "      n_layers = hp.Int('n_layers', min_value=1, max_value=5, step=1)\n",
    "      reg_type = hp.Choice('reg_type', values=['l1', 'l2', 'l1l2', 'None'])\n",
    "      max_norm = hp.Int('max_norm', min_value=1, max_value=10000, step=10)\n",
    "      activation = hp.Choice('activation', values=['relu', 'selu', 'elu', 'gelu', 'leaky_relu'])\n",
    "      p_drop = hp.Float('p_drop', min_value=0.0, max_value=0.9, step=0.01)\n",
    "      batch_norm = hp.Choice('batch_norm', values=[0, 1])\n",
    "\n",
    "      # Pick number of units per layer and store in a list with descending values\n",
    "      layer_nums = []\n",
    "      min_val = 1\n",
    "      max_val = 3000\n",
    "      lay_range = list(range(hp.get('n_layers')))\n",
    "      lay_range = [x+1 for x in lay_range]\n",
    "\n",
    "      layer_1_s = hp.Int('layer_1', min_value=min_val, max_value=max_val, step=1)\n",
    "      layer_nums.append(hp.get('layer_1'))\n",
    "      if hp.get('n_layers') >= 2:\n",
    "        with hp.conditional_scope(\"n_layers\", [2,3,4,5]):\n",
    "          layer_2_s = hp.Int(f'layer_2', min_value=min_val, max_value=2000, step=1)\n",
    "          layer_nums.append(hp.get('layer_2'))\n",
    "      if hp.get('n_layers') >= 3:\n",
    "        with hp.conditional_scope(\"n_layers\", [3,4,5]):\n",
    "          layer_3_s = hp.Int(f'layer_3', min_value=min_val, max_value=2000, step=1)\n",
    "          layer_nums.append(hp.get('layer_3'))\n",
    "      if hp.get('n_layers') >= 4:\n",
    "        with hp.conditional_scope(\"n_layers\", [4,5]):\n",
    "          layer_4_s = hp.Int(f'layer_4', min_value=min_val, max_value=2000, step=1)\n",
    "          layer_nums.append(hp.get('layer_4'))\n",
    "      if hp.get('n_layers') >= 5:\n",
    "        with hp.conditional_scope(\"n_layers\", [5]):\n",
    "          layer_5_s = hp.Int(f'layer_5', min_value=min_val, max_value=2000, step=1)\n",
    "          layer_nums.append(hp.get('layer_5'))\n",
    "\n",
    "      # Learning Rate\n",
    "      initial_learning_rate=hp.Choice(\"initial_learning_rate\", values=[1E-2, 1E-3, 1e-4, 3e-4, 4e-4, 5e-4]),\n",
    "      with hp.conditional_scope('optimizer', ['adam','adamax', 'rmsprop','adafactor', 'lion']):\n",
    "        if optimizer!='nadam':\n",
    "          decay_steps=hp.Int('decay_steps', min_value=10, max_value=5000, step=10),\n",
    "          decay_rate=hp.Float('decay_rate', min_value=0.5, max_value=1, step=0.01)\n",
    "          lr = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                  initial_learning_rate=initial_learning_rate,\n",
    "                  decay_steps=hp.get('decay_steps'),\n",
    "                  decay_rate=hp.get('decay_rate'))\n",
    "      # Set optimizer learning rate\n",
    "      if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "      elif optimizer == 'adamax':\n",
    "        opt = tf.keras.optimizers.Adamax(learning_rate=lr)\n",
    "      elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=lr)\n",
    "      elif optimizer == 'nesterov':\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)\n",
    "      elif optimizer == 'adafactor':\n",
    "          opt = tf.keras.optimizers.Adafactor(learning_rate=lr)\n",
    "      elif optimizer == 'lion':\n",
    "          opt = tf.keras.optimizers.Lion(learning_rate=lr)\n",
    "      elif optimizer == 'nadam':\n",
    "          opt = tf.keras.optimizers.Nadam(learning_rate=hp.get(\"initial_learning_rate\"))\n",
    "\n",
    "      # Adjust batch normalization\n",
    "      if batch_norm==1:\n",
    "        use_bias=False\n",
    "      else:\n",
    "        use_bias=True\n",
    "\n",
    "      # Adjust activation initializer if selu\n",
    "      if activation=='selu':\n",
    "        kernel_initializer = 'lecun_normal'\n",
    "      else:\n",
    "        kernel_initializer = 'he_normal'\n",
    "\n",
    "      # Set Weight regularizer\n",
    "      if reg_type =='None':\n",
    "          kernel_regularizer=None\n",
    "      else:\n",
    "          with hp.conditional_scope('reg_type', ['l1', 'l2', 'l1l2']):\n",
    "              reg_max = hp.Float('reg_max', min_value=0.005, max_value=0.4, step=0.001)\n",
    "              if reg_type == 'l1':\n",
    "                kernel_regularizer = l1(reg_max)\n",
    "              elif reg_type == 'l2':\n",
    "                kernel_regularizer = l2(reg_max)\n",
    "              elif reg_type=='l1l2':\n",
    "                kernel_regularizer = L1L2(l1=reg_max, l2=reg_max)\n",
    "\n",
    "      # Set weight value constraint\n",
    "      kernel_constraint = MaxNorm(max_value=max_norm)\n",
    "\n",
    "      # Create sequential model\n",
    "      model = Sequential()\n",
    "      model.add(tf.keras.Input(shape=INPUT_DIM))\n",
    "\n",
    "      # Build model\n",
    "      for i in range(hp.get('n_layers')):\n",
    "        model.add(Dense(layer_nums[i],\n",
    "                        kernel_regularizer=kernel_regularizer,\n",
    "                        kernel_constraint=kernel_constraint,\n",
    "                        kernel_initializer=kernel_initializer,\n",
    "                        use_bias=use_bias))\n",
    "        if batch_norm==1:\n",
    "            model.add(BatchNormalization())\n",
    "        model.add(Activation(activation))\n",
    "        if i!=hp.get('n_layers')-1:\n",
    "          model.add(Dropout(p_drop))\n",
    "\n",
    "      if self.regression==False: # classification\n",
    "        model.add(Dense(1, activation='tanh'))\n",
    "        model.compile(loss='squared_hinge', optimizer=opt,\n",
    "                  weighted_metrics=['accuracy', pr_auc])\n",
    "      elif self.regression==True: # Regression\n",
    "        model.add(Dense(1))\n",
    "        model.compile(loss=self.loss_metric, optimizer=opt,\n",
    "                  weighted_metrics=[self.loss_metric],\n",
    "                     ####### REMOVE LATER IF NEEDED\n",
    "                      jit_compile=False\n",
    "                      # removing jit compile\n",
    "                     )\n",
    "\n",
    "      return model\n",
    "  def fit(self, hp, model, *args, **kwargs):\n",
    "      return model.fit(*args, **kwargs,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySI-aJNy5hqx"
   },
   "source": [
    "# Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PXtYQUqg7Iip"
   },
   "outputs": [],
   "source": [
    "if tune_mode != 'lr_overall':\n",
    "  BEST_SCORE=None\n",
    "  def run_tuning_encoder(input_dim, regression, num_iter, tune_metric='pr_auc',\n",
    "                         tuner_seed=None, save_dir=save_dir, checkpoint_dir=checkpoint_dir,\n",
    "                        model_tune_name=model_tune_name):\n",
    "    global BEST_SCORE\n",
    "    if model_tune_name=='genre' or model_tune_name=='general':\n",
    "      num_executions=2\n",
    "    else:\n",
    "      num_executions=1\n",
    "      # Early Stopping\n",
    "    if regression==False:\n",
    "      baseline=0.6\n",
    "      if tune_metric == 'accuracy':\n",
    "        metric_nam = 'val_accuracy'\n",
    "        direction='max'\n",
    "      elif tune_metric == 'loss':\n",
    "        metric_nam = 'val_loss'\n",
    "        direction='min'\n",
    "      else:\n",
    "        metric_nam = 'val_pr_auc'\n",
    "        direction='max'\n",
    "    elif regression:\n",
    "      metric_nam=f'val_{tune_metric}'\n",
    "      direction='min'\n",
    "    es = CustomStopperCheckpoints(\n",
    "      monitor=metric_nam,\n",
    "      patience=early_stop_patience,\n",
    "      baseline_1=12.7,  # mse\n",
    "      baseline_2=12.2, # mse\n",
    "      start_epoch_2=25,\n",
    "      mode=direction,\n",
    "      restore_best_weights=True,\n",
    "      min_delta=0.005)\n",
    "    mcp_save = OverallModelCheckpoint(checkpoint_dir, save_best_only=True,\n",
    "                               monitor=metric_nam, mode=direction, verbose=1)\n",
    "\n",
    "    if not tuner_seed:\n",
    "      tuner_seed = str(np.random.choice(range(0,900000)))\n",
    "\n",
    "    tuner = BayesianOptimization(\n",
    "      hypermodel=MyHyperModel(tune_metric, regression),\n",
    "      objective=kt.Objective(metric_nam, direction=direction),\n",
    "      max_trials=model_tune_num_iterations,\n",
    "      num_initial_points=model_rand_num_iterations,\n",
    "      seed=42,\n",
    "      beta=2.8,\n",
    "      alpha=0.0001,\n",
    "      directory=save_dir,\n",
    "      overwrite=False,\n",
    "      project_name=model_tune_name,\n",
    "      executions_per_trial=1,\n",
    "      max_retries_per_trial=0\n",
    "      )\n",
    "    # Get best score if possible\n",
    "    try:\n",
    "      current_best_score = tuner.oracle.get_best_trials(num_trials=1)[0].score\n",
    "      if current_best_score < BEST_SCORE:\n",
    "        BEST_SCORE=current_best_score\n",
    "        print(f'Best score so far: {BEST_SCORE}')\n",
    "    except:\n",
    "      pass\n",
    "    # Tuner Search\n",
    "    tuner.search(\n",
    "      shuffle=True,\n",
    "      verbose=1,\n",
    "      callbacks=[es, mcp_save],\n",
    "      epochs=500,\n",
    "    )\n",
    "    return tuner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SjOGNyCYgDhp"
   },
   "source": [
    "# Tuner Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CLxu9UghmcwY",
    "outputId": "a4fc726d-0232-432b-b494-5749dfdab9f6"
   },
   "outputs": [],
   "source": [
    "if tune_mode != 'lr_overall':\n",
    "    print(f'Tuning for: {model_tune_name}')\n",
    "    tuner = run_tuning_encoder(\n",
    "                    INPUT_DIM, regression,\n",
    "                    # model_tune_num_iterations, TUNE_METRIC_NAME,\n",
    "                    430, TUNE_METRIC_NAME,\n",
    "                    tuner_seed=tuner_seed,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "x_O-RvSr0vQs",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Linear Regression Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if tune_mode == 'lr_overall':\n",
    "  INPUT_DATA_DIR = file_dir\n",
    "  TRAIN_DATA_COEFFICIENT = 0.60\n",
    "  VALIDATION_DATA_COEFFICIENT = 0.20\n",
    "  TEST_DATA_COEFFICIENT = 0.20\n",
    "  num_train = int(num_files * TRAIN_DATA_COEFFICIENT)\n",
    "  num_val = int(num_files * VALIDATION_DATA_COEFFICIENT)\n",
    "  num_test = int(num_files * TEST_DATA_COEFFICIENT)\n",
    "  TRAIN_FILES = [os.path.join(file_dir, f'{file}.tfrecord') for file in file_list[:num_train]]\n",
    "  VALIDATION_FILES = [os.path.join(file_dir, f'{file}.tfrecord') for file in file_list[num_train:num_train+num_val]]\n",
    "  TEST_FILES = [os.path.join(file_dir, f'{file}.tfrecord') for file in file_list[num_train+num_val:]]\n",
    "\n",
    "  df_train = get_overall_data(TRAIN_FILES)\n",
    "  df_val = get_overall_data(VALIDATION_FILES)\n",
    "  df_test = get_overall_data(TEST_FILES)\n",
    "  train_data, val_data, test_data = df_train.to_numpy(), df_val.to_numpy(), df_test.to_numpy()\n",
    "  lr_model_train_data = np.concatenate((train_data, val_data), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tune_mode == 'lr_overall':\n",
    "  metric='mean_squared_error'\n",
    "  grid_values = {\n",
    "              'alpha':\n",
    "                  [0.1, 0.3, 0.5, 0.7, 1.0, 1.5, 2, 2.5, 3, 3.5,\n",
    "                   5, 7, 10, 13, 15, 17, 17.5, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 35, 40, 45,\n",
    "                   50,75,100,150,200,250,300,325,350,365,370,375,380,385,400,450,500,550,600],\n",
    "              'solver': ['svd', 'cholesky', 'lsqr', 'lbfgs']}\n",
    "  lr_model, out, all_data = overall_model_grid_search(train_data, val_data, test_data, lr_model_train_data,\n",
    "                                  grid_values, cv=10, overfit_penalty=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = Ridge(alpha=100, solver='cholesky', random_state=42)\n",
    "X, y = lr_model_train_data[:, :-3], lr_model_train_data[:, -3:-1]\n",
    "lr_model.fit(X, y[:,0], sample_weight=y[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tune_mode == 'lr_overall':\n",
    "  # Save Model\n",
    "  MODEL_SAVE_DIR = os.path.join(DATA_DIR, 'Saved_Models')\n",
    "  saved_model = joblib.dump(lr_model, os.path.join(MODEL_SAVE_DIR ,'overall_best.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rtkO3uVfZRAx",
    "qZTsXSwbhLnQ",
    "_9Y1rWU_CEAr",
    "oLEFBKXKCGZz",
    "uo92xM5xS9Dl",
    "UOOQL_Vu5k_I",
    "SBQbE4UB7R-t",
    "TjHUisvbJpbe",
    "fucA0ySgwXZ1",
    "w3WT6m4j5fd9",
    "ySI-aJNy5hqx",
    "x_O-RvSr0vQs",
    "2gaU3sC4f_sK",
    "c35YXK3GdWPz"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
