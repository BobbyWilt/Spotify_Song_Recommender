{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeoJUD49rAp6"
   },
   "source": [
    "# Spotify Recommendation Model\n",
    " This notebook file includes code to:\n",
    "1. Import required toolboxes\n",
    "2. Connect to a spotify dev account\n",
    "3. Organize and pre-process data from provided playlists\n",
    "4. Build and train and neural network regression model\n",
    "5. Generate song reccommendations and filter them through model\n",
    "6. Upload model-filtered song recommendations to a provided spotify playlist\n",
    "\n",
    "To run this notebook you will need a spotify dev account.  To create one, go to this link and setup an account: https://developer.spotify.com/dashboard/\n",
    "\n",
    "> Once you have a dev account, go to the dashboard tab and create an app.  Next navigate to your app and locate your Client ID and Client ID Secret.  Copy these two client codes into the notebook code cell below labeld \"Authorization\".  Next go to your spotify dev project and click \"Edit Settings\" and under the \"Redirect URL\" section, place your desired redirect url (ie: https://www.google.com/) for authorization and select save.  Place your redirect url into notebook code cell below \"Authorization\".\n",
    "\n",
    "If you are running this code on google colab:\n",
    "1. Use Chrome as the web browser\n",
    "2. Change the Runtime type to use a GPU - select Runtime at the top, change runtime type,  select GPU under hardware accelerator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtkO3uVfZRAx"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import spacy\n",
    "    nlp_model = spacy.load(\"en_core_web_lg\")\n",
    "    print('spacy model already downloaded')\n",
    "except:\n",
    "    !python -m spacy download en_core_web_lg\n",
    "    import spacy\n",
    "    nlp_model = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23014,
     "status": "ok",
     "timestamp": 1705526194879,
     "user": {
      "displayName": "Bobby Wilt",
      "userId": "15841792391103222095"
     },
     "user_tz": 480
    },
    "id": "C3tMUu7IpbLN",
    "outputId": "e89c2af9-d761-439e-ecaf-12e6448fe127"
   },
   "outputs": [],
   "source": [
    "# General Toolboxes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import os\n",
    "import itertools\n",
    "import math\n",
    "import collections\n",
    "from random import shuffle\n",
    "import zipfile\n",
    "import ast\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import math\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "import shutil\n",
    "from numpy2tfrecord import Numpy2TFRecordConverter, build_dataset_from_tfrecord\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout, FloatSlider\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import keras\n",
    "from keras.layers import Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tensorflow imports\n",
    "import gc\n",
    "import gc\n",
    "from keras.activations import relu, sigmoid, softmax, tanh, selu, elu, gelu, leaky_relu\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation, AlphaDropout, LSTM, RNN, GRU, SimpleRNN, LayerNormalization, InputLayer, TimeDistributed, Bidirectional\n",
    "from keras.layers import ReLU, ELU, LeakyReLU, PReLU, MaxPooling1D, AveragePooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Conv1D, Concatenate, Flatten\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.regularizers import l2, l1, L1L2\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import keras_tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Imports for custom tuner\n",
    "# from keras_tuner.src.engine import tuner_utils\n",
    "import copy\n",
    "import random\n",
    "try:\n",
    "    import scipy\n",
    "    import scipy.optimize\n",
    "except ImportError:\n",
    "    scipy = None\n",
    "\n",
    "\n",
    "# Spotify\n",
    "import spotipy\n",
    "import spotipy.util as util\n",
    "\n",
    "# SkLearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# NLP\n",
    "import spacy\n",
    "\n",
    "import joblib\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environmental variables\n",
    "load_dotenv(override=True)\n",
    "DATA_DIR = os.environ.get('DATA_DIR')\n",
    "def man_win_lin_encode(str_path):\n",
    "    if ':' in str_path:\n",
    "        out = str_path.replace('\\\\', '/')\n",
    "        out = out.replace(f'{DATA_DIR[:2]}', f'/mnt/{DATA_DIR[0]}'.lower())\n",
    "    else:\n",
    "        out = str_path\n",
    "    return out\n",
    "DATA_DIR = man_win_lin_encode(DATA_DIR)\n",
    "API_KEYS = ast.literal_eval(os.environ.get('APIS'))\n",
    "SCOPE = os.environ.get('API_SCOPE')\n",
    "REDIRECT_URI = os.environ.get('REDIRECT_URI')\n",
    "API_NAMES = list(API_KEYS.keys())\n",
    "INPUT_PLAYLISTS = ast.literal_eval(os.environ.get('INPUT_PLAYLISTS'))\n",
    "BLACKLIST_PLAYLISTS = ast.literal_eval(os.environ.get('BLACKLIST_PLAYLISTS'))\n",
    "FEATURE=None\n",
    "\n",
    "# Check to make sure subfolders are created for environment\n",
    "dir_folders = ['Databases', 'Model_Tuning', 'Saved_Models']\n",
    "for folder in dir_folders:\n",
    "    dirname = os.path.join(DATA_DIR, folder)\n",
    "    if os.path.exists(dirname) == False:\n",
    "        os.mkdir(dirname)\n",
    "rec_dir = os.path.join(DATA_DIR, 'Databases', 'Recommend_Data')\n",
    "if os.path.exists(rec_dir)==False:\n",
    "    os.mkdir(rec_dir)\n",
    "# Check for config file and load it\n",
    "if os.path.isfile('config.json')==False:\n",
    "  json.dump(dict(), open('config.json', 'w'))\n",
    "\n",
    "# Set environemntal variables\n",
    "BASIC_DATASET_API_COUNTER = 0\n",
    "AUDIO_FEATURES_API_COUNTER = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Widgets\n",
    "style = {'description_width': '40%'}\n",
    "api_wdg = widgets.Dropdown(\n",
    "    options=API_NAMES,\n",
    "    value=API_NAMES[0],\n",
    "    description='API Key',\n",
    "    layout=Layout(width='25%'),\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "operation_wdg = widgets.Dropdown(\n",
    "    options=[\"create initial basic dataset\", \"generate raw td dataset\", \"create feature tables\", \"create final data table\"],\n",
    "    value='create initial basic dataset',\n",
    "    description='Dataset Creation Operation:',\n",
    "    layout=Layout(width='25%'),\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "weights_wdg = widgets.Dropdown(\n",
    "    options=[\"balanced\", \"custom\"],\n",
    "    value='balanced',\n",
    "    description='Select how you want to calculate class weights.  Balanced evens all ratings and custom will allow you to pick your own per playlist:',\n",
    "    layout=Layout(width='25%'),\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "use_neutral_songs_wdg = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Add songs to the database that were recommended from the model and from songs only in blacklist playlists',\n",
    "    disabled=False,\n",
    "    indent= False,\n",
    "    style={'description_width': '80%'},\n",
    "  layout=Layout(width='25%'))\n",
    "\n",
    "made_widgets = [api_wdg, operation_wdg, weights_wdg, use_neutral_songs_wdg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10_zF_QYqkFs"
   },
   "source": [
    "# Config\n",
    "\n",
    "Steps:\n",
    "1. Create Initial Basic Dataset\n",
    "> a. Optionally check box for calculate initial_min_max_values and add to saved set <br>\n",
    "> b. Save Dataset, rename, and upload to Drive <br>\n",
    "> c. Update File directory in Generate Raw TD Dataset\n",
    "2. Create Raw Time Domain Dataset\n",
    "> a. Download, rename, and upload datasets to drive <br>\n",
    "> b. Update file directory in Create Feature Tables\n",
    "3. Calculate Values for Feature Tables by checking the Calculate_data_dict_values box for the Following:\n",
    "> a. genre <br>\n",
    "> b. general <br>\n",
    "> c. sct_data <br>\n",
    "> d. sgm_loud <br>\n",
    "> e. sgm_timbre max and mean/sdv <br>\n",
    "\n",
    "4. Create Feature Tables\n",
    "> a. general <br>\n",
    "> b. genre <br>\n",
    "> c. sct_data <br>\n",
    "> d. sgm_loud <br>\n",
    "> e. sgm_timbre <br>\n",
    "> f. sgm_pitch\n",
    "\n",
    "5. Tune Feature Models and Save Best Model\n",
    "6. Put Feature model directories into notebook\n",
    "7. Create Final Data Table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for widget in made_widgets:\n",
    "  display(widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SELECTED_API = api_wdg.value\n",
    "operation = operation_wdg.value\n",
    "weight_mode = weights_wdg.value\n",
    "prediction_type = 'Regression'\n",
    "use_neutral_songs = use_neutral_songs_wdg.value\n",
    "normalization_mode='minmax'\n",
    "\n",
    "# Update API\n",
    "CLIENT_ID, CLIENT_SECRET = API_KEYS[SELECTED_API]\n",
    "\n",
    "# Update config file\n",
    "config = json.load(open('config.json'))\n",
    "config['prediction_type'] = prediction_type\n",
    "if 'norms' not in config.keys(): config['norms'] = {}\n",
    "for feature in [\"general\", \"genre\", \"sct_data\", \"sgm_loud\", \"sgm_timbre\"]:\n",
    "  if feature not in config['norms'].keys(): config['norms'][feature] = {}\n",
    "json.dump(config, open('config.json', 'w'))\n",
    "\n",
    "GEN_INPUT_SHAPES = {\n",
    "  'genre': [None, 300],\n",
    "  'general': [None, 49],\n",
    "  'sct_data': [None, 300, 10],\n",
    "  'sgm_loud': [None, 3000, 5],\n",
    "  'sgm_pitch': [None, 3000, 13],\n",
    "  'sgm_timbre': [None, 3000, 13],\n",
    "  'big_data': [None, 3000, 39],\n",
    "  'overall': [None, 6]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9Y1rWU_CEAr"
   },
   "source": [
    "# Authorization\n",
    "To authenticate your connection to Spotify, click on the link after running this code, and paste the redirected URL into the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1705526195995,
     "user": {
      "displayName": "Bobby Wilt",
      "userId": "15841792391103222095"
     },
     "user_tz": 480
    },
    "id": "oauzMcuENfd6"
   },
   "outputs": [],
   "source": [
    "if operation == 'generate raw td dataset' or operation == 'create initial basic dataset':\n",
    "  def create_connection(hide_output=False):\n",
    "    '''Creates a spotipy connection.  If fails then delete .cache file and re-try'''\n",
    "    global CLIENT_ID, CLIENT_SECRET, REDIRECT_URI, SCOPE, SELECTED_API\n",
    "    try:\n",
    "      sp = spotipy.Spotify(\n",
    "          auth_manager=spotipy.SpotifyOAuth(\n",
    "              client_id=CLIENT_ID,\n",
    "              client_secret=CLIENT_SECRET,\n",
    "              redirect_uri=REDIRECT_URI,\n",
    "              scope=SCOPE, open_browser=False),\n",
    "              requests_timeout=20, retries=3)\n",
    "      form_conn = sp.artist('spotify:artist:3jOstUTkEu2JkjvRdBA5Gu')\n",
    "      test_audio_analysis = sp.audio_analysis('3a1lNhkSLSkpJE4MSHpDu9')\n",
    "      if hide_output==False:\n",
    "        print(f'Authorization {SELECTED_API} Sucessfull!')\n",
    "    except:\n",
    "      try:\n",
    "        print('Removing spotipy cache file since api failed the 1st try.')\n",
    "        os.remove('.cache')\n",
    "        sp = spotipy.Spotify(\n",
    "          auth_manager=spotipy.SpotifyOAuth(\n",
    "              client_id=CLIENT_ID,\n",
    "              client_secret=CLIENT_SECRET,\n",
    "              redirect_uri=REDIRECT_URI,\n",
    "              scope=SCOPE, open_browser=False),\n",
    "              requests_timeout=20, retries=3)\n",
    "        form_conn = sp.artist('spotify:artist:3jOstUTkEu2JkjvRdBA5Gu')\n",
    "        test_audio_analysis = sp.audio_analysis('3a1lNhkSLSkpJE4MSHpDu9')\n",
    "        if hide_output==False:\n",
    "          print(f'Authorization {SELECTED_API} Sucessfull!')\n",
    "      except:\n",
    "        try:\n",
    "            os.remove('.cache')\n",
    "            key_list = list(API_KEYS.keys())\n",
    "            current_key_idx = key_list.index(SELECTED_API)\n",
    "            if current_key_idx==len(key_list)-1:\n",
    "              next_key_idx = 0\n",
    "            elif current_key_idx==0 and len(key_list)==1:\n",
    "              print('Need more spotify api keys to prepare data.  Please create more and add them to .env')\n",
    "            else:\n",
    "              next_key_idx = current_key_idx + 1\n",
    "            SELECTED_API = key_list[next_key_idx]\n",
    "            CLIENT_ID, CLIENT_SECRET = API_KEYS[SELECTED_API]\n",
    "        except:\n",
    "            sp=None\n",
    "            print(f'Authorization {SELECTED_API} Failed!')\n",
    "            print('API connection Failed!  Check your api client id, secret, scope and redirect ui.')\n",
    "    return sp\n",
    "  sp = create_connection()\n",
    "else:\n",
    "  sp=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1131,
     "status": "ok",
     "timestamp": 1705526195994,
     "user": {
      "displayName": "Bobby Wilt",
      "userId": "15841792391103222095"
     },
     "user_tz": 480
    },
    "id": "b3yaSZ2mqdVq"
   },
   "outputs": [],
   "source": [
    "# New Feature Code\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "def generate_big_data():\n",
    "  global FEATURE\n",
    "  DATA_FILE_DIR = os.path.join(DATA_DIR, 'Databases')\n",
    "  num_files = len(os.listdir(os.path.join(DATA_DIR, 'Databases', 'sgm_pitch')))\n",
    "  dirname = os.path.join(DATA_FILE_DIR, 'big_data')\n",
    "  if os.path.exists(dirname) == False:\n",
    "    os.mkdir(dirname)\n",
    "  for i in tqdm(range(1, num_files+1)):\n",
    "    for feature in ['sct_data', 'sgm_loud', 'sgm_pitch', 'sgm_timbre']:\n",
    "      FEATURE=feature\n",
    "      feat_path = os.path.join(DATA_FILE_DIR, feature, f'{feature}_dataset_p{i}.tfrecord')\n",
    "      dataset = tf.data.TFRecordDataset([feat_path])\n",
    "      dataset = dataset.map(parse_feature_function_song_id)\n",
    "      dataset = dataset.batch(500)\n",
    "      for raw_record in dataset:\n",
    "          x, y, weight, song_id = raw_record\n",
    "      x, y, weight, song_id = x.numpy(), y.numpy(), weight.numpy(), song_id.numpy()\n",
    "      if feature=='sct_data':\n",
    "        x = np.repeat(x, 10, axis=1)\n",
    "      df = pd.DataFrame(data=list(zip(song_id, y, weight, x)),\n",
    "               columns=['song_id', 'rating', 'weight', feature])\n",
    "      if feature=='sct_data':\n",
    "          df_pred = df\n",
    "      else:\n",
    "          df_pred = pd.merge(df_pred, df, how='inner',\n",
    "                            left_on=['song_id', 'rating', 'weight'],\n",
    "                            right_on=['song_id', 'rating', 'weight'])\n",
    "    x_sct_data = np.float32(np.array([np.array(row) for row in df_pred['sct_data']]))\n",
    "    x_sgm_loud = np.float32(np.array([np.array(row) for row in df_pred['sgm_loud']]))\n",
    "    x_sgm_pitch = np.float32(np.array([np.array(row) for row in df_pred['sgm_pitch']]))\n",
    "    x_sgm_timbre = np.float32(np.array([np.array(row) for row in df_pred['sgm_timbre']]))\n",
    "    x = np.array(np.dstack((np.dstack((np.dstack((x_sct_data, x_sgm_loud)), x_sgm_pitch[:,:,1:])), x_sgm_timbre[:,:,1:])))\n",
    "    y = df_pred['rating'].to_numpy()\n",
    "    weight_output = df_pred['weight'].to_numpy()\n",
    "    song_id = df_pred['song_id'].to_numpy()\n",
    "    y = np.float32(y.astype(np.float64).reshape(y.shape[0],1))\n",
    "    weight_output = np.float32(weight_output.astype(np.float64).reshape(weight_output.shape[0],1))\n",
    "    song_id = np.int64(song_id.reshape(song_id.shape[0],1))\n",
    "    save_name = os.path.join(dirname, f'big_data_dataset_p{i}.tfrecord')\n",
    "    with Numpy2TFRecordConverter(save_name) as converter:\n",
    "      sample={\"x\": x, \"y\": y, \"weight\": weight_output, 'song_id': song_id}\n",
    "      converter.convert_batch(sample)\n",
    "      # delete info files\n",
    "    for file in os.listdir(dirname):\n",
    "        if '.info' in file:\n",
    "          os.remove(os.path.join(dirname, file))\n",
    "\n",
    "\n",
    "def translate_ids(song_ids, return_id_only=True):\n",
    "    song_id_lookup = json.load(open('song_id_lookup.json'))\n",
    "    if type(song_ids)==str:\n",
    "        unq_ids, ratings = song_id_lookup[song_ids]\n",
    "    else:\n",
    "        unq_ids, ratings = [], []\n",
    "        for song_id in song_ids:\n",
    "            unq_id, rating = song_id_lookup[song_id]\n",
    "            unq_ids.append(unq_id)\n",
    "            ratings.append(rating)\n",
    "    if return_id_only:\n",
    "        return unq_ids\n",
    "    else:\n",
    "        return unq_ids, ratings\n",
    "\n",
    "def reverse_translate_ids(song_ids, return_song_id=True):\n",
    "    song_id_lookup = json.load(open('song_id_lookup.json'))\n",
    "    reverse_dict = {}\n",
    "    for key, value in song_id_lookup.items():\n",
    "        reverse_dict[value[0]] = (key, value[1])\n",
    "    if type(song_ids)==str:\n",
    "        unq_ids, ratings = reverse_dict[song_ids]\n",
    "    else:\n",
    "        unq_ids, ratings = [], []\n",
    "        for song_id in song_ids:\n",
    "            unq_id, rating = reverse_dict[song_id]\n",
    "            unq_ids.append(unq_id)\n",
    "            ratings.append(rating)\n",
    "    if return_song_id:\n",
    "        return unq_ids\n",
    "    else:\n",
    "        return unq_ids, ratings\n",
    "\n",
    "\n",
    "def get_overall_data():\n",
    "  global FEATURE\n",
    "  DATA_FILE_DIR = os.path.join(DATA_DIR, 'Databases')\n",
    "  num_files = len(os.listdir(os.path.join(DATA_DIR, 'Databases', 'sgm_pitch')))\n",
    "  dirname = os.path.join(DATA_FILE_DIR, 'overall')\n",
    "  # Loop through files\n",
    "  features = [\"genre\", \"general\", \"sct_data\", \"sgm_loud\", \"sgm_pitch\", \"sgm_timbre\"]\n",
    "  FEATURE='overall'\n",
    "  for i in range(1, num_files+1):\n",
    "      feat_path = os.path.join(dirname, f'overall_dataset_p{i}.tfrecord')\n",
    "      dataset = tf.data.TFRecordDataset([feat_path])\n",
    "      dataset = dataset.map(parse_feature_function_song_id)\n",
    "      dataset = dataset.batch(500)\n",
    "      for raw_record in dataset:\n",
    "          x, y, weight, song_id = raw_record\n",
    "      x, y, weight, song_id = x.numpy(), y.numpy().reshape(-1,1), weight.numpy().reshape(-1,1), song_id.numpy().reshape(-1,1)\n",
    "      song_id = np.array(reverse_translate_ids(song_id.reshape(-1))).reshape(-1,1)\n",
    "      data = np.hstack((x,y,weight))\n",
    "      df_pred = pd.DataFrame(data=data, columns=features + ['rating', 'weight'])\n",
    "      df_pred['song_id'] = song_id\n",
    "      if i==1:\n",
    "          out=df_pred\n",
    "      else:\n",
    "          out = pd.concat([out, df_pred], axis=0)\n",
    "  return out\n",
    "\n",
    "\n",
    "def parse_feature_function(example_proto, tune_shapes=GEN_INPUT_SHAPES):\n",
    "    data_shape=tune_shapes[FEATURE][1:]\n",
    "    tfrecord_format = {\n",
    "            \"x\": tf.io.FixedLenFeature(data_shape, tf.float32),\n",
    "            \"y\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"weight\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"song_id\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    features = tf.io.parse_single_example(example_proto, tfrecord_format)\n",
    "    x=features['x']\n",
    "    y = features['y']\n",
    "    weight = features['weight']\n",
    "    return x, y, weight\n",
    "\n",
    "\n",
    "def parse_feature_function_song_id(example_proto, tune_shapes=GEN_INPUT_SHAPES):\n",
    "    data_shape=tune_shapes[FEATURE][1:]\n",
    "    tfrecord_format = {\n",
    "            \"x\": tf.io.FixedLenFeature(data_shape, tf.float32),\n",
    "            \"y\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"weight\": tf.io.FixedLenFeature([], tf.float32),\n",
    "            \"song_id\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    features = tf.io.parse_single_example(example_proto, tfrecord_format)\n",
    "    x=features['x']\n",
    "    y = features['y']\n",
    "    weight = features['weight']\n",
    "    song_id = features['song_id']\n",
    "    return x, y, weight, song_id\n",
    "\n",
    "\n",
    "def generate_final_dataset(run_normalization=False):\n",
    "    global FEATURE\n",
    "    DATA_FILE_DIR = os.path.join(DATA_DIR, 'Databases')\n",
    "    num_files = len(os.listdir(os.path.join(DATA_DIR, 'Databases', 'sgm_pitch')))\n",
    "    dirname = os.path.join(DATA_FILE_DIR, 'overall')\n",
    "    if os.path.exists(dirname) == False:\n",
    "      os.mkdir(dirname)\n",
    "    # Loop through files\n",
    "    features = [[\"genre\"], [\"general\"], [\"sct_data\"], [\"sgm_loud\"], [\"sgm_pitch\"], ['sgm_timbre']]\n",
    "    for i in range(1, num_files+1):\n",
    "        config = json.load(open('config.json'))\n",
    "        for feature in features:\n",
    "          for sub_feature in feature:\n",
    "            FEATURE=sub_feature\n",
    "            feat_path = os.path.join(DATA_FILE_DIR, sub_feature, f'{sub_feature}_dataset_p{i}.tfrecord')\n",
    "            dataset = tf.data.TFRecordDataset([feat_path])\n",
    "            dataset = dataset.map(parse_feature_function_song_id)\n",
    "            dataset = dataset.batch(500)\n",
    "            for raw_record in dataset:\n",
    "                x, y, weight, song_id = raw_record\n",
    "            x, y, weight, song_id = x.numpy(), y.numpy(), weight.numpy(), song_id.numpy()\n",
    "            song_id = reverse_translate_ids(song_id)\n",
    "            # Combine time series data points\n",
    "            if sub_feature not in ('genre', 'general'):\n",
    "              # if sub_feature=='sct_data':\n",
    "              #   x = np.repeat(x, 10, axis=1)\n",
    "              df = pd.DataFrame(data=list(zip(song_id, y, weight, x)),\n",
    "                       columns=['song_id', 'rating', 'weight', sub_feature])\n",
    "              if sub_feature=='sct_data':\n",
    "                  df_pred = df\n",
    "              else:\n",
    "                  df_pred = pd.merge(df_pred, df, how='inner',\n",
    "                                    left_on=['song_id', 'rating', 'weight'],\n",
    "                                    right_on=['song_id', 'rating', 'weight'])\n",
    "          if feature not in (['genre'], ['general']):\n",
    "            ## non big feature setup\n",
    "            feature_name=feature[0]\n",
    "            x = np.float32(np.array([np.array(row) for row in df_pred[feature_name]]))\n",
    "            ## big feature setup\n",
    "            # feature_name='big_data'\n",
    "            # x_sct_data = np.float32(np.array([np.array(row) for row in df_pred['sct_data']]))\n",
    "            # x_sgm_loud = np.float32(np.array([np.array(row) for row in df_pred['sgm_loud']]))\n",
    "            # x_sgm_pitch = np.float32(np.array([np.array(row) for row in df_pred['sgm_pitch']]))\n",
    "            # x_sgm_timbre = np.float32(np.array([np.array(row) for row in df_pred['sgm_timbre']]))\n",
    "            # x = np.array(np.dstack((np.dstack((np.dstack((x_sct_data, x_sgm_loud)), x_sgm_pitch[:,:,1:])), x_sgm_timbre[:,:,1:])))\n",
    "            y = df_pred['rating'].to_numpy()\n",
    "            weight = df_pred['weight'].to_numpy()\n",
    "            song_id = df_pred['song_id'].to_numpy()\n",
    "          else:\n",
    "            feature_name=feature[0]\n",
    "          # make prediction\n",
    "          preds = model_dict[feature_name].predict(x, verbose=0)\n",
    "          df = pd.DataFrame(data=list(zip(song_id, preds.reshape(-1,), y, weight)),\n",
    "               columns=['song_id', feature_name, 'rating', 'weight'])\n",
    "          if feature_name=='genre':\n",
    "              df_final = df\n",
    "          else:\n",
    "              df_final = pd.merge(df_final, df, how='inner',\n",
    "                                left_on=['song_id', 'rating', 'weight'],\n",
    "                                right_on=['song_id', 'rating', 'weight'])\n",
    "\n",
    "        # Calculate mins and maxes\n",
    "        temp_norm_dict = {}\n",
    "        # for feature in ['genre', 'general', 'big_data']:\n",
    "        for feature in [\"genre\", \"general\", \"sct_data\", \"sgm_loud\", \"sgm_pitch\", \"sgm_timbre\"]:\n",
    "          temp_norm_dict[f'{feature}_min'] = float(df_final[feature].min())*0.7\n",
    "          temp_norm_dict[f'{feature}_max'] = float(df_final[feature].max())*1.3\n",
    "          for norm in ['min', 'max']:\n",
    "            calculated_value = temp_norm_dict[f'{feature}_{norm}']\n",
    "            # print(calculated_value)\n",
    "            if 'overall' not in config['norms'].keys():\n",
    "                config['norms']['overall'] = {}\n",
    "            if f'{feature}_{norm}' not in config['norms']['overall'].keys():\n",
    "              config['norms']['overall'][f'{feature}_{norm}'] = calculated_value\n",
    "            elif 'min' in norm:\n",
    "              if calculated_value < config['norms']['overall'][f'{feature}_{norm}']:\n",
    "                config['norms']['overall'][f'{feature}_{norm}'] = calculated_value\n",
    "            elif 'max' in norm:\n",
    "              if calculated_value > config['norms']['overall'][f'{feature}_{norm}']:\n",
    "                config['norms']['overall'][f'{feature}_{norm}'] = calculated_value\n",
    "          json.dump(config, open('config.json', 'w'))\n",
    "        if run_normalization:\n",
    "          #normalize\n",
    "          # for feature in ['genre', 'general', 'big_data']:\n",
    "          for feature in [\"genre\", \"general\", \"sct_data\", \"sgm_loud\", \"sgm_pitch\", \"sgm_timbre\"]:\n",
    "            df_final[feature] = minmax(df_final[feature], config['norms']['overall'][f'{feature}_min'],\n",
    "                                     config['norms']['overall'][f'{feature}_max'])\n",
    "          # x = df_final[['genre', 'general', 'big_data']].to_numpy()\n",
    "          x = df_final[[\"genre\", \"general\", \"sct_data\", \"sgm_loud\", \"sgm_pitch\", \"sgm_timbre\"]].to_numpy()\n",
    "          y = df_final['rating'].to_numpy()\n",
    "          weight_output = df_final['weight'].to_numpy()\n",
    "          song_id = df_final['song_id'].to_numpy()\n",
    "          song_id = np.array(translate_ids(song_id))\n",
    "          save_name = os.path.join(dirname, f'overall_dataset_p{i}.tfrecord')\n",
    "          with Numpy2TFRecordConverter(save_name) as converter:\n",
    "            sample={\"x\": x, \"y\": y, \"weight\": weight_output, 'song_id': song_id}\n",
    "            converter.convert_batch(sample)\n",
    "            # delete info files\n",
    "          for file in os.listdir(dirname):\n",
    "              if '.info' in file:\n",
    "                os.remove(os.path.join(dirname, file))\n",
    "\n",
    "\n",
    "def calc_td_norm_values(data, feature, normalization_mode='minmax'):\n",
    "  '''Calculate normalization values by feature'''\n",
    "  # Get normalization mode\n",
    "  if normalization_mode=='minmax':\n",
    "    scope=['min', 'max']\n",
    "  elif normalization_mode=='zscore':\n",
    "    scope=['mean', 'sdv']\n",
    "  out={}\n",
    "  if feature=='sct_data':\n",
    "    # sct loud\n",
    "    dict_out = transform_to_td(feature_list=list(data['sct_loud']),\n",
    "                               transform_feat_names=['sct_conf', 'sct_loud'],\n",
    "                               transform_idxs=[2,4],\n",
    "                               loud_idxs=[4], scope=scope)\n",
    "    out.update(dict_out)\n",
    "    # sct tempo\n",
    "    dict_out = transform_to_td(feature_list=list(data['sct_tempo']),\n",
    "                          transform_feat_names=['sct_tempo', 'sct_tempo_conf'],\n",
    "                          transform_idxs=[4,5],\n",
    "                          loud_idxs=[], loud_add=None, scope=scope)\n",
    "    out.update(dict_out)\n",
    "    # sct key\n",
    "    dict_out = transform_to_td(feature_list=list(data['sct_key']),\n",
    "                          transform_feat_names=['sct_key', 'sct_key_conf'],\n",
    "                          transform_idxs=[4,5],\n",
    "                          loud_idxs=[4], loud_add=1, max_val=12, scope=scope)\n",
    "    out.update(dict_out)\n",
    "    # sct mode\n",
    "    dict_out = transform_to_td(feature_list=list(data['sct_mode']),\n",
    "                          transform_feat_names=['sct_mode', 'sct_mode_conf'],\n",
    "                          transform_idxs=[4,5],\n",
    "                          loud_idxs=[4], loud_add=0, max_val=1, scope=scope)\n",
    "    out.update(dict_out)\n",
    "    # sct time sig\n",
    "    dict_out = transform_to_td(feature_list=list(data['sct_time_sig']),\n",
    "                          transform_feat_names=['sct_time_sig', 'sct_time_sig_conf'],\n",
    "                          transform_idxs=[4,5],\n",
    "                          loud_idxs=[4], loud_add=0, max_val=7, scope=scope)\n",
    "    out.update(dict_out)\n",
    "\n",
    "  elif feature=='sgm_loud':\n",
    "    out = transform_to_td(feature_list=list(data['sgm_loud']),\n",
    "                          transform_feat_names=['sgm_conf', 'sgm_loud_start',\n",
    "                                                'sgm_loud_max', 'sgm_loud_max_time',\n",
    "                                                'sgm_loud_end'],\n",
    "                          transform_idxs=[2, 4, 5, 6, 7],\n",
    "                          loud_idxs=[4, 5, 7], loud_add=100, max_val=1, scope=scope)\n",
    "\n",
    "\n",
    "  elif feature=='sgm_timbre':\n",
    "    out = transform_to_td(feature_list=list(data['sgm_timbre']),\n",
    "                          transform_feat_names=[f'sgm_timbre_{i}' for i in range(1,13)],\n",
    "                          transform_idxs=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "                          loud_idxs=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "                          loud_add=1000, max_val=None, scope=scope)\n",
    "\n",
    "  elif feature=='genre':\n",
    "    out = {}\n",
    "    for col in range(300):\n",
    "      if normalization_mode=='minmax':\n",
    "        out[f'{col}_min'] = float(data[col].min())*0.7\n",
    "        out[f'{col}_max'] = float(data[col].max())*1.3\n",
    "\n",
    "\n",
    "  elif feature=='general':\n",
    "    # Obtain dictionary values\n",
    "    cols = ['pop', 'artist_pop', 'acoust', 'dance', 'energy', 'instrument', 'live',\n",
    "            'loud', 'speech', 'valence', 'release_year', 'song_dur', 'tempo',\n",
    "            'key', 'fade_in_dur', 'fade_out_dur', 'num_bars',\n",
    "            'num_beats', 'num_sections', 'num_segments', 'num_tatums',\n",
    "            'bar_dur_avg', 'beat_dur_avg', 'tatum_dur_avg',\n",
    "            'bar_dur_sdv', 'beat_dur_sdv', 'tatum_dur_sdv',\n",
    "            'bar_conf_avg', 'beat_conf_avg', 'tatum_conf_avg',\n",
    "            'bar_conf_sdv', 'beat_conf_sdv', 'tatum_conf_sdv',\n",
    "            'section_dur_avg', 'section_dur_sdv', 'section_conf_avg', 'section_conf_sdv',\n",
    "            'segment_dur_avg', 'segment_dur_sdv', 'segment_conf_avg', 'segment_conf_sdv'\n",
    "            ]\n",
    "    out = {}\n",
    "    for col in cols:\n",
    "      if normalization_mode=='minmax':\n",
    "        out[f'{col}_min'] = float(data[col].min())*0.7\n",
    "        out[f'{col}_max'] = float(data[col].max())*1.3\n",
    "      elif normalization_mode=='zscore':\n",
    "        print(f\"'{col}_mean': {np.mean(df_data[col])},\")\n",
    "        print(f\"'{col}_sdv': {np.std(df_data[col])},\")\n",
    "\n",
    "  else:\n",
    "    out=None\n",
    "  return out\n",
    "\n",
    "def generate_feature_norms(raw_td_path, raw_td_file_list, basic_dataset_path,\n",
    "                           feature=feature, normalization_mode=normalization_mode,\n",
    "                          run_normalization=False, save_dir=None, null_tracker=None,\n",
    "                          output_format='npy'):\n",
    "  '''Create feature table for feature from raw time domain dataset and basic dataset'''\n",
    "  # loop through basic and raw td files and extract feature contents\n",
    "  if run_normalization:\n",
    "    print(f'Applying normalization and weights for {feature}')\n",
    "  else:\n",
    "    print(f'Calculating Normalization values for {feature}')\n",
    "  for i in tqdm(range(len(raw_td_file_list))):\n",
    "    df_basic = pd.read_csv(os.path.join(basic_dataset_path, f'basic_dataset_p{i+1}.csv'),\n",
    "                          index_col='song_id')\n",
    "    df_basic.genres = df_basic.genres.apply(process_str_list)\n",
    "    if feature!='genre':\n",
    "      if feature == 'sct_data':\n",
    "        load_cols = ['song_id', 'rating', 'weight', 'sct_loud', 'sct_tempo', 'sct_key', 'sct_mode', 'sct_time_sig']\n",
    "      elif feature == 'general':\n",
    "        load_cols = ['song_id', 'rating', 'weight', 'song_dur', 'tempo', 'tempo_confidence', 'time_sig',\n",
    "                     'time_sig_confidence', 'key', 'key_confidence',\n",
    "                     'mode', 'mode_confidence', 'fade_in_dur', 'fade_out_dur',\n",
    "                     'num_bars', 'num_beats', 'num_sections', 'num_segments', 'num_tatums',\n",
    "                     'bar_dur_avg', 'beat_dur_avg', 'tatum_dur_avg',\n",
    "                     'bar_dur_sdv', 'beat_dur_sdv', 'tatum_dur_sdv',\n",
    "                     'bar_conf_avg', 'beat_conf_avg', 'tatum_conf_avg',\n",
    "                     'bar_conf_sdv', 'beat_conf_sdv', 'tatum_conf_sdv',\n",
    "                     'section_dur_avg', 'section_dur_sdv', 'section_conf_avg', 'section_conf_sdv',\n",
    "                     'segment_dur_avg', 'segment_dur_sdv', 'segment_conf_avg', 'segment_conf_sdv']\n",
    "      else:\n",
    "        load_cols= ['song_id', 'rating', 'weight', feature]\n",
    "      df_td_data = pd.read_csv(os.path.join(raw_td_path, f'raw_td_feature_p{i+1}.csv'),\n",
    "                           index_col='song_id', usecols=load_cols)\n",
    "\n",
    "    # remove rows with row in any of the dataframes by checking null tracker\n",
    "    try:\n",
    "      null_idxs = null_tracker[f'{i+1}']\n",
    "    except:\n",
    "      null_idxs = null_tracker[i+1]\n",
    "    if (feature=='sgm_loud') or (feature=='sgm_timbre') or (feature=='sgm_pitch'):\n",
    "        try:\n",
    "            df_td_data[feature] = df_td_data[feature].apply(lambda x: json.loads(x))\n",
    "        except:\n",
    "            # missing data in row so check null tracker to remove\n",
    "            df_td_data.drop(df_td_data.index[null_idxs], inplace=True)\n",
    "            df_td_data[feature] = df_td_data[feature].apply(lambda x: json.loads(x))\n",
    "    if feature=='genre':\n",
    "      drop_ids=[]\n",
    "      data_list = list(df_basic['genres'])\n",
    "      df_output = pd.DataFrame(data_list)\n",
    "      # Convert to dataframe and add index back in\n",
    "      df_output = pd.DataFrame(df_output)\n",
    "      test = df_output.to_numpy()\n",
    "      for j, row in enumerate(test):\n",
    "        for k, col in enumerate(row):\n",
    "          if type(col)!=np.float64 or np.isnan(col):\n",
    "            drop_ids = list(set(drop_ids + [j]))\n",
    "            null_tracker[f'{i+1}'] = list(set(null_tracker[f'{i+1}'] + [j]))\n",
    "      df_output.index = df_basic.index\n",
    "      df_output['song_id'] = df_basic.index\n",
    "      df_output['rating'] = df_basic['rating']\n",
    "      df_output['weight'] = df_basic['weight']\n",
    "      if run_normalization == False:\n",
    "          df_output.drop(df_output.index[drop_ids], inplace=True)\n",
    "    elif (feature=='sgm_pitch') or (feature=='sgm_timbre'):\n",
    "      feature_list = df_td_data[feature].tolist()\n",
    "      out_new = []\n",
    "      for song in feature_list:\n",
    "        song_data = []\n",
    "        for j in range(0, len(song),5):\n",
    "          segment_data = [song[j], song[j+1], song[j+2], song[j+3]]\n",
    "          segment_data.extend([item for item in song[j+4]])\n",
    "          song_data.append(segment_data)\n",
    "        out_new.append(song_data)\n",
    "      df_td_data[feature] = out_new\n",
    "      df_output = df_td_data\n",
    "    elif feature=='general':\n",
    "      drop_ids=[]\n",
    "      # Combine dataframes\n",
    "      df_output = pd.merge(df_basic, df_td_data, how='right',\n",
    "                                  left_on=['song_id', 'rating', 'weight'],\n",
    "                                  right_on=['song_id', 'rating', 'weight'], suffixes=('_x', ''))\n",
    "      df_output.drop(columns=['genres', 'mode_x', 'time_sig_x', 'key_x', 'tempo_x'], inplace=True)\n",
    "      if df_output.shape[0]!=df_td_data.shape[0]:\n",
    "          print('Non Norm Step')\n",
    "          print(f'Different basic and td data shapes for feature {feature} and index {i+1}')\n",
    "          print(df_basic.shape, df_td_data.shape, df_output.shape)\n",
    "      # Check for nulls\n",
    "      test = df_output.to_numpy()\n",
    "      if run_normalization == False:\n",
    "          for j, row in enumerate(test):\n",
    "            for k, col in enumerate(row):\n",
    "              if type(col)!=np.float64 or np.isnan(col):\n",
    "                drop_ids = list(set(drop_ids + [j]))\n",
    "                null_tracker[f'{i+1}'] = list(set(null_tracker[f'{i+1}'] + [j]))\n",
    "          df_output.reset_index(inplace=True)\n",
    "          df_output.drop(df_output.index[drop_ids], inplace=True)\n",
    "    elif feature=='sct_data':\n",
    "      # convert list in string to list\n",
    "      try:\n",
    "        for col in df_td_data.columns:\n",
    "          if type(df_td_data[col][0])==str:\n",
    "            df_td_data[col] = df_td_data[col].apply(ast.literal_eval)\n",
    "      except:\n",
    "        # nulls detected, grab ids and drop rows\n",
    "        null_idxs = df_td_data.reset_index()[df_td_data.reset_index().isnull().any(axis=1)].index.tolist()\n",
    "        null_tracker[f'{i+1}'] = list(set(null_tracker[f'{i+1}'] + null_idxs))\n",
    "        if run_normalization == False:\n",
    "            df_td_data.drop(df_td_data.index[null_idxs], inplace=True)\n",
    "            df_basic.drop(df_basic.index[null_idxs], inplace=True)\n",
    "            for col in df_td_data.columns:\n",
    "              if type(df_td_data[col][0])==str:\n",
    "                df_td_data[col] = df_td_data[col].apply(ast.literal_eval)\n",
    "      df_output = df_td_data\n",
    "    else:\n",
    "      df_output = df_td_data\n",
    "\n",
    "    config = json.load(open('config.json'))\n",
    "\n",
    "    if run_normalization==False and feature!='sgm_pitch':\n",
    "      # Get norm values\n",
    "      out = calc_td_norm_values(df_output, feature)\n",
    "      # Update config norm values if they are less than mins or greater than maxes\n",
    "      for key in out.keys():\n",
    "        if key not in config['norms'][feature].keys():\n",
    "          config['norms'][feature][key] = out[key]\n",
    "        elif 'min' in key:\n",
    "          if out[key] < config['norms'][feature][key]:\n",
    "            config['norms'][feature][key] = out[key]\n",
    "        elif 'max' in key:\n",
    "          if out[key] > config['norms'][feature][key]:\n",
    "            config['norms'][feature][key] = out[key]\n",
    "      json.dump(config, open('config.json', 'w'))\n",
    "\n",
    "    ### RUN NORMALIZATION###\n",
    "    elif run_normalization:\n",
    "      # apply normalization\n",
    "      # Drop null rows\n",
    "      if len(null_idxs) > 0:\n",
    "          df_basic.drop(df_basic.index[null_idxs], inplace=True)\n",
    "          if feature!='genre':\n",
    "              df_td_data.drop(df_td_data.index[null_idxs], inplace=True)\n",
    "          if feature=='general':\n",
    "              df_output.drop(df_output.index[null_idxs], inplace=True)\n",
    "      if feature != 'sgm_pitch':\n",
    "        min_max_dict = config['norms'][feature]\n",
    "      if feature=='sct_data':\n",
    "        for col in df_output.columns:\n",
    "          if type(df_td_data[col][0])==str:\n",
    "            df_output[col] = df_output[col].apply(ast.literal_eval)\n",
    "        ## sct_loud\n",
    "        sub_feature='sct_loud'\n",
    "        mins = [min_max_dict['sct_conf_min'], min_max_dict[f'{sub_feature}_min']]\n",
    "        maxs = [min_max_dict['sct_conf_max'], min_max_dict[f'{sub_feature}_max']]\n",
    "        out_loud = np.array(create_td_data(feature_list=list(df_output[sub_feature]),\n",
    "                                           time_segments=list(range(1,301)), feature_idxs=[2,4],\n",
    "                                           mins=mins, maxs=maxs, modify_idxs=[4], add_val=100))\n",
    "        ## sct_tempo\n",
    "        sub_feature='sct_tempo'\n",
    "        mins = [min_max_dict['sct_conf_min'], min_max_dict[f'{sub_feature}_min'], min_max_dict[f'{sub_feature}_conf_min']]\n",
    "        maxs = [min_max_dict['sct_conf_max'], min_max_dict[f'{sub_feature}_max'], min_max_dict[f'{sub_feature}_conf_max']]\n",
    "        out_tempo = np.array(create_td_data(feature_list=list(df_output[sub_feature]), time_segments=list(range(1,301)),\n",
    "                                            feature_idxs=[2, 4, 5], mins=mins, maxs=maxs,\n",
    "                                            modify_idxs=[4]))\n",
    "        # sct_key\n",
    "        sub_feature='sct_key'\n",
    "        mins = [min_max_dict['sct_conf_min'], min_max_dict[f'{sub_feature}_min'], min_max_dict[f'{sub_feature}_conf_min']]\n",
    "        maxs = [min_max_dict['sct_conf_max'], min_max_dict[f'{sub_feature}_max'], min_max_dict[f'{sub_feature}_conf_max']]\n",
    "        out_key = np.array(create_td_data(feature_list=list(df_output[sub_feature]), time_segments=list(range(1,301)),\n",
    "                                          feature_idxs=[2, 4, 5], mins=mins, maxs=maxs,\n",
    "                                          modify_idxs=[4], add_val=1, divide_val=12))\n",
    "        # sct_mode\n",
    "        sub_feature='sct_mode'\n",
    "        mins = [min_max_dict['sct_conf_min'], min_max_dict[f'{sub_feature}_min'], min_max_dict[f'{sub_feature}_conf_min']]\n",
    "        maxs = [min_max_dict['sct_conf_max'], min_max_dict[f'{sub_feature}_max'], min_max_dict[f'{sub_feature}_conf_max']]\n",
    "        out_mode = np.array(create_td_data(feature_list=list(df_output[sub_feature]), time_segments=list(range(1,301)),\n",
    "                                          feature_idxs=[2, 4, 5], mins=mins, maxs=maxs,\n",
    "                                          modify_idxs=[4], add_val=0, divide_val=1))\n",
    "        # sct_time_sig\n",
    "        sub_feature='sct_time_sig'\n",
    "        mins = [min_max_dict['sct_conf_min'], min_max_dict[f'{sub_feature}_min'], min_max_dict[f'{sub_feature}_conf_min']]\n",
    "        maxs = [min_max_dict['sct_conf_max'], min_max_dict[f'{sub_feature}_max'], min_max_dict[f'{sub_feature}_conf_max']]\n",
    "        out_time_sig = np.array(create_td_data(feature_list=list(df_output[sub_feature]), time_segments=list(range(1,301)),\n",
    "                                          feature_idxs=[2, 4, 5], mins=mins, maxs=maxs,\n",
    "                                          modify_idxs=[4], add_val=0, divide_val=7))\n",
    "        # Concat section features\n",
    "        a,b,c = df_output.index.values, df_output.rating.values, df_output.weight.values\n",
    "        df_output = np.concatenate([out_loud, out_tempo[:, :, 1:], out_key[:, :, 1:], out_mode[:, :, 1:], out_time_sig[:, :, 1:]], -1)\n",
    "        df_output = pd.DataFrame(data=zip(df_output.tolist()), columns=['sct_data'])\n",
    "        df_output['song_id']= a\n",
    "        df_output['rating'] = b\n",
    "        df_output['weight'] = c\n",
    "\n",
    "      elif feature=='sgm_loud':\n",
    "        mins = [min_max_dict[f'sgm_conf_min'], min_max_dict['sgm_loud_start_min'], min_max_dict['sgm_loud_max_min'],\n",
    "                min_max_dict['sgm_loud_max_time_min'], min_max_dict['sgm_loud_end_min']]\n",
    "        maxs = [min_max_dict[f'sgm_conf_max'], min_max_dict['sgm_loud_start_max'], min_max_dict['sgm_loud_max_max'],\n",
    "                  min_max_dict['sgm_loud_max_time_max'], min_max_dict['sgm_loud_end_max']]\n",
    "        out_sgm_loud = np.array(create_td_data(feature_list=list(df_output[feature]),\n",
    "                                               time_segments=list(np.arange(0.1,300.1, 0.1)),\n",
    "                                               feature_idxs=[2, 4, 5, 6, 7], mins=mins, maxs=maxs,\n",
    "                                               modify_idxs=[4, 5, 7], add_val=100, divide_val=1))\n",
    "        a,b,c = df_output.index.values, df_output.rating.values, df_output.weight.values\n",
    "        df_output = pd.DataFrame(data=zip(out_sgm_loud.tolist()), columns=[feature])\n",
    "        df_output['song_id']= a\n",
    "        df_output['rating'] = b\n",
    "        df_output['weight'] = c\n",
    "\n",
    "\n",
    "      elif feature=='sgm_timbre':\n",
    "        mins = [config['norms']['sgm_loud']['sgm_conf_min']] + [min_max_dict[f'sgm_timbre_{k}_min'] for k in range(1,13)]\n",
    "        maxs = [config['norms']['sgm_loud']['sgm_conf_max']] + [min_max_dict[f'sgm_timbre_{k}_max'] for k in range(1,13)]\n",
    "        out_sgm_timbre = np.array(create_td_data(feature_list=list(df_output[feature]),\n",
    "                                                 time_segments=list(np.arange(0.1,300.1, 0.1)),\n",
    "                                                 feature_idxs=[2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "                                                 mins=mins, maxs=maxs,\n",
    "                                                 modify_idxs=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "                                                 add_val=1000))\n",
    "        a,b,c = df_output.index.values, df_output.rating.values, df_output.weight.values\n",
    "        df_output = pd.DataFrame(data=zip(out_sgm_timbre.tolist()), columns=[feature])\n",
    "        df_output['song_id']= a\n",
    "        df_output['rating'] = b\n",
    "        df_output['weight'] = c\n",
    "\n",
    "\n",
    "      elif feature=='sgm_pitch':\n",
    "        mins = [config['norms']['sgm_loud']['sgm_conf_min']] + [0 for k in range(1,13)]\n",
    "        maxs = [config['norms']['sgm_loud']['sgm_conf_max']] + [1 for k in range(1,13)]\n",
    "        out_sgm_pitch = np.array(create_td_data(feature_list=list(df_output[feature]),\n",
    "                                                time_segments=list(np.arange(0.1,300.1, 0.1)),\n",
    "                                                feature_idxs=[2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "                                                mins=mins, maxs=maxs,\n",
    "                                                modify_idxs=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15],\n",
    "                                                add_val=0.000001, divide_val=1))\n",
    "        a,b,c = df_output.index.values, df_output.rating.values, df_output.weight.values\n",
    "        df_output = pd.DataFrame(data=zip(out_sgm_pitch.tolist()), columns=[feature])\n",
    "        df_output['song_id']= a\n",
    "        df_output['rating'] = b\n",
    "        df_output['weight'] = c\n",
    "\n",
    "\n",
    "      elif feature=='genre':\n",
    "        for col in range(300):\n",
    "          df_output[col] = minmax(df_output[col], min_max_dict[f'{col}_min'], min_max_dict[f'{col}_max'])\n",
    "          df_output.loc[df_output[col] > 1, col] = 1\n",
    "          df_output.loc[df_output[col] < 0, col] = 0\n",
    "\n",
    "      elif feature=='general':\n",
    "        # repeat column\n",
    "        if df_output.shape[0]!=df_td_data.shape[0]:\n",
    "            print('norm data')\n",
    "            print(f'Different basic and td data shapes for feature {feature} and index {i+1}')\n",
    "            print(df_basic.shape, df_td_data.shape, df_output.shape)\n",
    "            return df_basic, df_td_data, df_output\n",
    "        try:\n",
    "          df_output = df_output.drop(columns=['dur'])\n",
    "        except:\n",
    "          pass\n",
    "\n",
    "        cols = list(df_output.columns)\n",
    "        for bad_id in ['explicit', 'album_type', 'rating', 'mode', 'weight', 'time_sig',\n",
    "                       'mode_confidence', 'key_confidence', 'time_sig_confidence',\n",
    "                       'tempo_confidence']:\n",
    "          try:\n",
    "              cols.remove(bad_id)\n",
    "          except:\n",
    "              print('fail removing non-norm, general columns')\n",
    "              print(bad_id, cols)\n",
    "              pass\n",
    "        for col in cols:\n",
    "            df_output[col] = minmax(df_output[col], min_max_dict[f'{col}_min'], min_max_dict[f'{col}_max'])\n",
    "            df_output.loc[df_output[col] > 1, col] = 1\n",
    "            df_output.loc[df_output[col] < 0, col] = 0\n",
    "        df_output.reset_index(inplace=True)\n",
    "        df_output = df_output.sample(frac=1)\n",
    "      if output_format=='npy':\n",
    "        np.save(os.path.join(save_dir, f'{feature}_dataset_p{i+1}'), np.array(df_output))\n",
    "      elif output_format=='trf':\n",
    "          song_id_lookup = json.load(open('song_id_lookup.json'))\n",
    "          df_output['song_id'] = df_output['song_id'].map(translate_ids)\n",
    "          frame=np.array(df_output)\n",
    "          if feature=='sgm_loud' or feature=='sgm_timbre' or feature=='sgm_pitch' or feature=='sct_data':\n",
    "              x = np.float32(np.array([np.array(row) for row in frame[:,0]]))\n",
    "              output = np.float32(frame[:, -2].astype(np.float64).reshape(frame[:,-2].shape[0],1))\n",
    "              weight_output = np.float32(frame[:, -1].astype(np.float64).reshape(frame[:,-1].shape[0],1))\n",
    "              song_id = np.int64(frame[:,-3].reshape(frame[:,-3].shape[0],1))\n",
    "          # New Feature Code\n",
    "          elif feature=='general':\n",
    "              x=np.float32(np.delete(frame, (0,4,5), 1))\n",
    "              output=np.float32(frame[:, 4].reshape(frame[:,4].shape[0],1))\n",
    "              weight_output=np.float32(frame[:, 5].reshape(frame[:,5].shape[0],1))\n",
    "              song_id = np.int64(frame[:,0].reshape(frame[:,0].shape[0],1))\n",
    "          elif feature=='genre':\n",
    "              x=np.float32(frame[:,:-3])\n",
    "              output=np.float32(frame[:, -2].reshape(frame[:,-2].shape[0],1))\n",
    "              weight_output=np.float32(frame[:, -1].reshape(frame[:,-1].shape[0],1))\n",
    "              song_id = np.int64(frame[:,-3].reshape(frame[:,-3].shape[0],1))\n",
    "          save_path = os.path.join(save_dir, f'{feature}_dataset_p{i+1}.tfrecord')\n",
    "          with Numpy2TFRecordConverter(save_path) as converter:\n",
    "            sample={\"x\": x, \"y\": output, \"weight\": weight_output, \"song_id\": song_id}\n",
    "            converter.convert_batch(sample)\n",
    "          # delete info files\n",
    "          for file in os.listdir(save_dir):\n",
    "            if '.info' in file:\n",
    "                os.remove(os.path.join(save_dir, file))\n",
    "      else:\n",
    "        df_output.to_csv(os.path.join(save_dir, f'{feature}_dataset_p{i+1}.csv'))\n",
    "\n",
    "  return null_tracker\n",
    "\n",
    "\n",
    "def process_str_list(sample):\n",
    "      '''Converst string list to list'''\n",
    "      sample = sample.replace(\"\\n\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "      sample_list = list(sample.split(\" \"))\n",
    "      sample_list=[x for x in sample_list if len(x)>0]\n",
    "      sample_list = [float(x.replace(',','')) for x in sample_list]\n",
    "      return sample_list\n",
    "\n",
    "\n",
    "def create_song_id_df(prediction_type=prediction_type, sp=sp):\n",
    "  '''Create song_id playlist with ratings.'''\n",
    "  config = json.load(open('config.json'))\n",
    "  df_song_ids = pd.DataFrame(columns=['song_id', 'rating']).set_index('song_id')\n",
    "\n",
    "  # Loop through provided playlists\n",
    "  for playlist_name, playlist_uri in INPUT_PLAYLISTS.items():\n",
    "    sp = check_api_swap(sp)\n",
    "    print(f'Gathering song ids for: {playlist_name}')\n",
    "    rating = config['ratings'][prediction_type][playlist_name]\n",
    "    df_temp, sp = get_song_ids_from_playlist(playlist_uri, rating)\n",
    "    df_song_ids = pd.concat([df_song_ids, df_temp])\n",
    "    df_song_ids = df_song_ids[~df_song_ids.index.duplicated(keep='first')]\n",
    "\n",
    "  if use_neutral_songs:\n",
    "    # Add in blacklist_playlist\n",
    "    for playlist_name, playlist_uri in BLACKLIST_PLAYLISTS.items():\n",
    "      print(f'Gathering song ids for: {playlist_name}')\n",
    "      rating = config['ratings'][prediction_type]['Neutral_Songs']\n",
    "      df_temp, sp = get_song_ids_from_playlist(playlist_uri, rating)\n",
    "      df_temp = df_temp[~df_temp.isin(df_song_ids)].dropna()\n",
    "      df_song_ids = pd.concat([df_song_ids, df_temp])\n",
    "      df_song_ids = df_song_ids[~df_song_ids.index.duplicated(keep='first')]\n",
    "    # Non-rated songs that were recommended\n",
    "    neutral_song_path = os.path.join(DATA_DIR, 'Databases', 'Recommend_Data', 'song_listened.csv')\n",
    "    if os.path.exists(neutral_song_path):\n",
    "      df_neutral_songs = pd.read_csv(neutral_song_path).set_index('song_id')\n",
    "      df_neutral_songs['rating'] = [config['ratings'][prediction_type]['Neutral_Songs']] * df_neutral_songs.shape[0]\n",
    "      df_neutral_songs = df_neutral_songs[~df_neutral_songs.isin(df_song_ids)].dropna()\n",
    "      df_song_ids = pd.concat([df_song_ids, df_neutral_songs])\n",
    "      df_song_ids = df_song_ids[~df_song_ids.index.duplicated(keep='first')]\n",
    "\n",
    "  return df_song_ids\n",
    "\n",
    "\n",
    "def get_n_chunks(data, num_per_dataset=200):\n",
    "  '''Get number of chunks based on df_song_id size'''\n",
    "  n_classes = data.rating.nunique()\n",
    "  data_len = data.shape[0]\n",
    "  n_chunks = int(np.ceil(data_len / num_per_dataset))\n",
    "  return n_chunks\n",
    "\n",
    "\n",
    "def get_rating_cnts(data, prediction_type=prediction_type):\n",
    "  df_cnts = pd.DataFrame(data.value_counts()).reset_index()\n",
    "  df_cnts['percent_of_data'] = 100 * (df_cnts['count'] / df_cnts['count'].sum())\n",
    "  config = json.load(open('config.json'))\n",
    "  rating_dict_map = dict((v,k) for k,v in config['ratings'][prediction_type].items())\n",
    "  df_cnts['playlist'] = df_cnts['rating'].map(rating_dict_map)\n",
    "\n",
    "  return df_cnts\n",
    "\n",
    "\n",
    "def calc_weights(df_song_ids, df_cnts, weight_mode=weight_mode):\n",
    "  '''Create automatic or custom class weights'''\n",
    "  if weight_mode == 'balanced':\n",
    "    custom_weights = compute_class_weight(class_weight = 'balanced', classes = df_cnts['rating'].values, y=df_song_ids['rating'])\n",
    "    custom_weights = dict(zip(df_cnts['playlist'].values, custom_weights))\n",
    "    print(custom_weights)\n",
    "  else:\n",
    "    fig = px.bar(df_cnts, x='playlist', y='percent_of_data', color='rating', title='Default data percentages without weights')\n",
    "    fig.show()\n",
    "    custom_weights = compute_class_weight(class_weight = 'balanced', classes = df_cnts['rating'].values, y=df_song_ids['rating'])\n",
    "    custom_weights = dict(zip(df_cnts['playlist'].values, custom_weights))\n",
    "    print(f'Example weights for balanced dataset: {custom_weights}')\n",
    "    custom_weights={}\n",
    "    for playlist in df_cnts['playlist']:\n",
    "      input_data = float(input(f'Enter a weight value for {playlist}:'))\n",
    "      custom_weights[playlist] = input_data\n",
    "    df_cnts['weights'] = df_cnts['playlist'].map(custom_weights)\n",
    "    df_cnts['weighted_count'] = df_cnts['count'] * df_cnts['weights']\n",
    "    df_cnts['weighted_percent_of_data'] = 100 * (df_cnts['weighted_count'] / df_cnts['weighted_count'].sum())\n",
    "    fig = px.bar(df_cnts, x='playlist', y='weighted_percent_of_data', color='rating', title='Weighted data percentages')\n",
    "    fig.show()\n",
    "    print('If you are happy with these weights then run the next cell!')\n",
    "\n",
    "  return custom_weights\n",
    "\n",
    "\n",
    "def save_weights_and_add_to_df(custom_weights, df_song_ids, prediction_type=prediction_type):\n",
    "  '''Save Custom weights to config file'''\n",
    "  config = json.load(open('config.json'))\n",
    "  if 'weights' not in config.keys():\n",
    "      config['weights'] = {}\n",
    "  config['weights'][prediction_type] = custom_weights\n",
    "  json.dump(config, open('config.json', 'w'))\n",
    "\n",
    "  # add weights to df_song_ids\n",
    "  mapping_dict = {}\n",
    "  for playlist in config['ratings'][prediction_type].keys():\n",
    "    rating = config['ratings'][prediction_type][playlist]\n",
    "    mapping_dict[rating] = config['weights'][prediction_type][playlist]\n",
    "  df_song_ids['weight'] = df_song_ids['rating'].map(mapping_dict)\n",
    "\n",
    "  return df_song_ids\n",
    "\n",
    "\n",
    "def make_dir(path):\n",
    "  ''' Makes Folder if directory doesn't exist'''\n",
    "  if os.path.exists(path) == False:\n",
    "    os.mkdir(path)\n",
    "\n",
    "\n",
    "def generate_basic_datasets():\n",
    "  '''Using song_ids to generate basic dataset chunks'''\n",
    "  basic_data_ids_save_dir = os.path.join(DATA_DIR, 'Databases', 'Basic_Dataset_Song_Ids')\n",
    "  song_id_files = os.listdir(basic_data_ids_save_dir)\n",
    "  save_dir = os.path.join(DATA_DIR, 'Databases', 'Basic_Dataset')\n",
    "  make_dir(save_dir)\n",
    "  for i in tqdm(range(len(song_id_files))):  # edit to ad all\n",
    "    path = os.path.join(basic_data_ids_save_dir, f'song_ids_p{i+1}.csv')\n",
    "    data = pd.read_csv(path)\n",
    "    df = create_song_df(data)\n",
    "    df = wrangle(df)\n",
    "    df.to_csv(os.path.join(save_dir, f'basic_dataset_p{i+1}.csv'))\n",
    "  print('Finished saving basic dataset chunks!')\n",
    "\n",
    "\n",
    "def check_api_swap(sp):\n",
    "  '''Checks api global environmental counter variables and sees if the api\n",
    "  should be switched to another one to prevent over-usage and from spotify\n",
    "  temporarily disabling the api'''\n",
    "  global BASIC_DATASET_API_COUNTER, AUDIO_FEATURES_API_COUNTER, CLIENT_ID, CLIENT_SECRET, SELECTED_API\n",
    "  if BASIC_DATASET_API_COUNTER >= 10000 or AUDIO_FEATURES_API_COUNTER >=1700:\n",
    "    os.remove('.cache')  # Spotify api will block further connections if this is reached regardless of switching api\n",
    "    key_list = list(API_KEYS.keys())\n",
    "    current_key_idx = key_list.index(SELECTED_API)\n",
    "    if current_key_idx==len(key_list)-1:\n",
    "      next_key_idx = 0\n",
    "    elif current_key_idx==0 and len(key_list)==1:\n",
    "      print('Need more spotify api keys to prepare data.  Please create more and add them to .env')\n",
    "    else:\n",
    "      next_key_idx = current_key_idx + 1\n",
    "    SELECTED_API = key_list[next_key_idx]\n",
    "    CLIENT_ID, CLIENT_SECRET = API_KEYS[SELECTED_API]\n",
    "    if AUDIO_FEATURES_API_COUNTER>=1600:\n",
    "      attempts=0\n",
    "      while attempts <= 5:\n",
    "        try:\n",
    "          sp = create_connection()\n",
    "          attempts=6\n",
    "        except:\n",
    "          os.remove('.cache')  # Spotify api will block further connections if this is reached regardless of switching api\n",
    "          attempts +=1\n",
    "    else:\n",
    "      sp = create_connection(hide_output=True)\n",
    "    BASIC_DATASET_API_COUNTER = 0\n",
    "    AUDIO_FEATURES_API_COUNTER = 0\n",
    "  return sp\n",
    "\n",
    "\n",
    "def split_dataset_into_chunks(data, n_splits,  class_col, save_dir):\n",
    "  #Get each of the classes into their own list of samples\n",
    "  n_classes = data.rating.nunique()\n",
    "  make_dir(save_dir)\n",
    "  class_split_list = {}\n",
    "  for i, class_name in enumerate(df_song_ids[class_col].unique()):\n",
    "      class_list = dict(data.groupby(class_col).groups)[class_name].values.tolist()\n",
    "      shuffle(class_list)\n",
    "      class_split_list[i] = np.array_split(class_list,n_splits)#create a dict of split chunks\n",
    "\n",
    "  stratified_sample_chunks = []\n",
    "  for i in range(n_splits):\n",
    "      class_chunks = []\n",
    "      for j in range(n_classes):\n",
    "          class_chunks.extend(class_split_list[j][i])#get split from current class\n",
    "      stratified_sample_chunks.append(class_chunks)\n",
    "  for i, chunk in enumerate(stratified_sample_chunks):\n",
    "    df = data.iloc[chunk].set_index('song_id')\n",
    "    save_path = os.path.join(save_dir, f'song_ids_p{i+1}.csv')\n",
    "    df.to_csv(save_path)\n",
    "  print('Successfully saved all datasets')\n",
    "\n",
    "\n",
    "\n",
    "def get_song_ids_from_playlist(playlist_uri, rating, sp=sp):\n",
    "  '''Create a dictionary from playlist_uri with a column for song_id and and column for rating'''\n",
    "  global BASIC_DATASET_API_COUNTER\n",
    "  raw_song_data = []\n",
    "  count=0\n",
    "  while True:\n",
    "    sp = check_api_swap(sp)\n",
    "    results = sp.playlist_tracks(playlist_uri, limit=50, offset=count)['items']\n",
    "    BASIC_DATASET_API_COUNTER += len(results)\n",
    "    # Stop adding new song once we reach the end of liked song list\n",
    "    if len(results)==0: break\n",
    "    count+=50\n",
    "    raw_song_data.extend([song['track']['id'] for song in results])\n",
    "  df = pd.DataFrame(list(zip(raw_song_data, [rating] * len(raw_song_data))),\n",
    "                   columns=['song_id', 'rating']).set_index('song_id')\n",
    "  return df, sp\n",
    "\n",
    "\n",
    "def rating_input(playlists, use_neutral_songs=use_neutral_songs, prediction_type=prediction_type):\n",
    "  '''Add rating values to config.json file based on prediction_type'''\n",
    "  config = json.load(open('config.json'))\n",
    "  rating_ids = {}\n",
    "  if 'ratings' not in config.keys():\n",
    "    config['ratings'] = {}\n",
    "\n",
    "  if prediction_type=='Regression':\n",
    "    prompt='Enter in a numerical value with higher values being more liked songs and lower for less liked songs for'\n",
    "  elif prediction_type=='Classification':\n",
    "    prompt='Enter in 0 for disliked playlist and 1 for liked playlist'\n",
    "\n",
    "  for playlist in playlists:\n",
    "    rating_ids[playlist] = float(input(f'{prompt}   {playlist}:'))\n",
    "\n",
    "  # Add in neutral songs\n",
    "  if use_neutral_songs:\n",
    "    neutral_id_value = float(input(f'{prompt}   Neutral Songs:'))\n",
    "    rating_ids['Neutral_Songs'] = neutral_id_value\n",
    "\n",
    "  config['ratings'][prediction_type] = rating_ids\n",
    "  json.dump(config, open('config.json', 'w'))\n",
    "  print(f'Successfully added {prediction_type} ratings to config!')\n",
    "\n",
    "\n",
    "def create_playlists(playlist_links, playlist_names):\n",
    "  '''Takes a list of playlist links and names.  Returns an ordered dictionary\n",
    "  of playlist name as key and link as value'''\n",
    "  count = 1\n",
    "  playlists = collections.OrderedDict()\n",
    "  for i in range(len(inp_play_links)):\n",
    "    if len(inp_play_links[i]) > 0:\n",
    "      if len(inp_play_names[i])==0:\n",
    "        name = f'playlist {count}'\n",
    "      else:\n",
    "        name = inp_play_names[i]\n",
    "      count +=1\n",
    "      assert inp_play_links[i][0]=='s', f'provided playlist: {inp_play_links[i]} \\\n",
    "        is not a spotify URI link.  Make sure to double check how to get playlist link in instructions'\n",
    "      playlists[name] = inp_play_links[i]\n",
    "\n",
    "  playlist_scores = range(len(playlists))\n",
    "  print('Playlists Successfully Added!')\n",
    "  return playlists, playlist_scores\n",
    "\n",
    "\n",
    "def normalize(column, negative=False):\n",
    "    if negative:\n",
    "      for i, song in enumerate(column):\n",
    "        for j, segment in enumerate(song):\n",
    "          for k, item in enumerate(segment):\n",
    "            if item < 0:\n",
    "              column[i][j][k] = -1 * item\n",
    "            else:\n",
    "              column[i][j][k] = (item+0.00001)  * 2\n",
    "      return column\n",
    "    upper = column.max()\n",
    "    lower = column.min()\n",
    "    y = (column - lower)/(upper-lower)\n",
    "    return y.tolist()\n",
    "\n",
    "\n",
    "def audio_analysis(song_list, sp=None):\n",
    "  '''Obtains audio analysis data'''\n",
    "  global AUDIO_FEATURES_API_COUNTER\n",
    "\n",
    "  total_songs = len(song_list)\n",
    "  out, song_ids = [], []\n",
    "  counter=0\n",
    "  sp = create_connection(hide_output=True)\n",
    "  for song in song_list:\n",
    "    if counter >= 1000:\n",
    "      try:\n",
    "        sp = create_connection()\n",
    "        counter = 0\n",
    "      except:\n",
    "        print('Failed to reset authorization to spotify')\n",
    "        break\n",
    "    else:\n",
    "      counter += 1\n",
    "    # Obtain Data\n",
    "    try:\n",
    "      out.append(sp.audio_analysis(song))\n",
    "      song_ids.append(song)\n",
    "      AUDIO_FEATURES_API_COUNTER += 1\n",
    "      sp = check_api_swap(sp)\n",
    "    except:\n",
    "        print(f'Failed to get data for {song}')\n",
    "        pass\n",
    "\n",
    "  # Create empty lists\n",
    "  categories = [\"song_dur\", \"tempo\", \"tempo_confidence\", \"time_sig\", \"time_sig_confidence\",\n",
    "    \"key\", \"key_confidence\", \"mode\", \"mode_confidence\", \"fade_in_dur\", \"fade_out_dur\",\n",
    "    \"num_bars\", \"num_beats\", \"num_sections\", \"num_segments\", \"num_tatums\",\n",
    "    \"bar_data\", \"beat_data\", \"tatum_data\", 'sct_loud', 'sct_tempo', 'sct_key', 'sct_mode', 'sct_time_sig',\n",
    "    'sgm_loud', 'sgm_pitch', 'sgm_timbre',]\n",
    "\n",
    "  song_dur, tempo, tempo_confidence, time_sig, time_sig_confidence = [], [], [], [], []\n",
    "  key, key_confidence, mode, mode_confidence, fade_in_dur, fade_out_dur = [], [], [], [], [], []\n",
    "  num_bars, num_beats, num_sections, num_segments, num_tatums = [], [], [], [], []\n",
    "  bar_data, beat_data, tatum_data = [], [], []\n",
    "  # Section Data\n",
    "  sct_loud, sct_tempo, sct_key, sct_mode, sct_time_sig = [], [], [], [], []\n",
    "  # Segment Data\n",
    "  sgm_loud, sgm_pitch, sgm_timbre = [], [], []\n",
    "  extra_song_ids = []\n",
    "\n",
    "  for song, song_id in zip(out, song_ids):\n",
    "    try:\n",
    "      extra_song_ids.append(song_id)\n",
    "      # Song Traits\n",
    "      duration = song['track']['duration']\n",
    "      song_dur.append(song['track']['duration'])\n",
    "      tempo.append(song['track']['tempo'])\n",
    "      tempo_confidence.append(song['track']['tempo_confidence'])\n",
    "      time_sig.append(song['track']['time_signature'] / 7)\n",
    "      time_sig_confidence.append(song['track']['time_signature_confidence'])\n",
    "      key.append(song['track']['key'])\n",
    "      key_confidence.append(song['track']['key_confidence'])\n",
    "      mode.append(song['track']['mode'])\n",
    "      mode_confidence.append(song['track']['mode_confidence'])\n",
    "      fade_in_dur.append(song['track']['end_of_fade_in'])\n",
    "      fade_out_dur.append(duration - song['track']['start_of_fade_out'])\n",
    "      num_bars.append(len(song['bars']))\n",
    "      num_beats.append(len(song['beats']))\n",
    "      num_sections.append(len(song['sections']))\n",
    "      num_segments.append(len(song['segments']))\n",
    "      num_tatums.append(len(song['tatums']))\n",
    "\n",
    "      # Obtain bar Data\n",
    "      song_bar_data = []\n",
    "      for bar in song['bars']:\n",
    "        temp_data = [bar['start'], bar['duration'], bar['confidence']]\n",
    "        song_bar_data.append(temp_data)\n",
    "      bar_data.append(song_bar_data)\n",
    "\n",
    "      # Obtain beat data\n",
    "      song_beat_data = []\n",
    "      for beat in song['beats']:\n",
    "        temp_data = [beat['start'], beat['duration'], beat['confidence']]\n",
    "        song_beat_data.append(temp_data)\n",
    "      beat_data.append(song_beat_data)\n",
    "\n",
    "      # Section Data\n",
    "      song_sct_loud, song_sct_tempo, song_sct_key, song_sct_mode, song_sct_time_sig = [], [], [], [], []\n",
    "      for section in song['sections']:\n",
    "        # Loudness\n",
    "        temp_data = [section['start'], section['duration'], section['confidence'], section['start'] + section['duration'],  section['loudness']]\n",
    "        song_sct_loud.append(temp_data)\n",
    "        # Tempo\n",
    "        temp_data = [section['start'], section['duration'], section['confidence'], section['start'] + section['duration'], section['tempo'], section['tempo_confidence']]\n",
    "        song_sct_tempo.append(temp_data)\n",
    "        # Key\n",
    "        temp_data = [section['start'], section['duration'], section['confidence'], section['start'] + section['duration'], section['key'], section['key_confidence']]\n",
    "        song_sct_key.append(temp_data)\n",
    "        # Mode\n",
    "        temp_data = [section['start'], section['duration'], section['confidence'], section['start'] + section['duration'], section['mode'], section['mode_confidence']]\n",
    "        song_sct_mode.append(temp_data)\n",
    "        # Time signature\n",
    "        temp_data = [section['start'], section['duration'], section['confidence'], section['start'] + section['duration'], section['time_signature'], section['time_signature_confidence']]\n",
    "        song_sct_time_sig.append(temp_data)\n",
    "      sct_loud.append(song_sct_loud)\n",
    "      sct_tempo.append(song_sct_tempo)\n",
    "      sct_key.append(song_sct_key)\n",
    "      sct_mode.append(song_sct_mode)\n",
    "      sct_time_sig.append(song_sct_time_sig)\n",
    "\n",
    "      # Segment data\n",
    "      song_sgm_loud, song_sgm_pitch, song_sgm_timbre = [], [], []\n",
    "      for segment in song['segments']:\n",
    "        # Loudness\n",
    "        temp_data = [segment['start'], segment['duration'], segment['confidence'], segment['start'] + segment['duration'], segment['loudness_start'], segment['loudness_max'], segment['loudness_max_time'], segment['loudness_end']]\n",
    "        song_sgm_loud.append(temp_data)\n",
    "        # Pitch\n",
    "        temp_data = [segment['start'], segment['duration'], segment['confidence'], segment['start'] + segment['duration'], segment['pitches']]\n",
    "        song_sgm_pitch.extend(temp_data)\n",
    "        # Timbre\n",
    "        temp_data = [segment['start'], segment['duration'], segment['confidence'], segment['start'] + segment['duration'], segment['timbre']]\n",
    "        song_sgm_timbre.extend(temp_data)\n",
    "\n",
    "      sgm_loud.append(song_sgm_loud)\n",
    "      sgm_pitch.append(song_sgm_pitch)\n",
    "      sgm_timbre.append(song_sgm_timbre)\n",
    "\n",
    "      # Tatum data\n",
    "      song_tatum_data = []\n",
    "      for tatum in song['tatums']:\n",
    "        temp_data = [tatum['start'], tatum['duration'], tatum['confidence']]\n",
    "        song_tatum_data.append(temp_data)\n",
    "      tatum_data.append(song_tatum_data)\n",
    "    except:\n",
    "      print(f'no data for song_id: {song_id}, filling with 0')\n",
    "      variables = [song_dur, tempo, tempo_confidence, time_sig, time_sig_confidence,\n",
    "                  key, key_confidence, mode, mode_confidence, fade_in_dur, fade_out_dur,\n",
    "                  num_bars, num_beats, num_sections, num_segments, num_tatums,\n",
    "                  bar_data, beat_data, tatum_data,\n",
    "                  sct_loud, sct_tempo, sct_key, sct_mode, sct_time_sig,\n",
    "                  sgm_loud, sgm_pitch, sgm_timbre]\n",
    "      for variable in variables:\n",
    "        variable.append(0)\n",
    "\n",
    "\n",
    "  # Create DataFrame\n",
    "  data = list(zip(song_dur, tempo, tempo_confidence, time_sig, time_sig_confidence,\n",
    "    key, key_confidence, mode, mode_confidence, fade_in_dur, fade_out_dur,\n",
    "    num_bars, num_beats, num_sections, num_segments, num_tatums,\n",
    "    bar_data, beat_data, tatum_data,\n",
    "    sct_loud, sct_tempo, sct_key, sct_mode, sct_time_sig,\n",
    "    sgm_loud, sgm_pitch, sgm_timbre))\n",
    "  df = pd.DataFrame(data=data, columns=categories, index=extra_song_ids)\n",
    "  # Drop rows with bad index\n",
    "  # Handle Bar, Beat and tatum Data, Beat\n",
    "  for name, item in zip(['section', 'segment', 'bar', 'beat', 'tatum' ], ['sct_loud', 'sgm_loud','bar_data', 'beat_data', 'tatum_data']):\n",
    "    data = list(df[item].values)\n",
    "    s_dur_avg, s_dur_sdv, s_conf_avg, s_conf_sdv = [], [], [], []\n",
    "    for song in data:\n",
    "      dur, conf = [], []\n",
    "      for component in song:\n",
    "        dur.append(component[1])\n",
    "        conf.append(component[2])\n",
    "      s_dur_avg.append(np.mean(dur))\n",
    "      s_dur_sdv.append(np.std(dur))\n",
    "      s_conf_sdv.append(np.std(conf))\n",
    "      s_conf_avg.append(np.mean(conf))\n",
    "    df[f'{name}_dur_avg'] = s_dur_avg\n",
    "    df[f'{name}_dur_sdv'] = s_dur_sdv\n",
    "    df[f'{name}_conf_avg'] = s_conf_avg\n",
    "    df[f'{name}_conf_sdv'] = s_conf_sdv\n",
    "\n",
    "  # Drop Columns\n",
    "  df.drop(columns=['bar_data', 'beat_data', 'tatum_data'], inplace=True)\n",
    "\n",
    "  return df, sp\n",
    "\n",
    "\n",
    "def process_component(data, max_length, col_id_skip=None, shift_param=0.00001):\n",
    "  '''Input is a list and returns normalized dataframe'''\n",
    "  out = []\n",
    "  cols_skip = []\n",
    "  counter = 0\n",
    "  for row in data:  # Loop through song\n",
    "    row_data = []\n",
    "    for item in row: # loop through bar\n",
    "      for i, subitem in enumerate(item): # loop through elements in bar\n",
    "        if col_id_skip:\n",
    "          if i in col_id_skip:\n",
    "            cols_skip.append([f'comp_{counter}'])\n",
    "        counter += 1\n",
    "        row_data.append(subitem)\n",
    "    # Check if row needs padding or trimming\n",
    "    if len(row_data) < max_length:\n",
    "      zero_pads = [0] * (max_length - len(row_data))\n",
    "      row_data.extend(zero_pads)\n",
    "    elif len(row_data) >= max_length:\n",
    "      row_data = row_data[:max_length]\n",
    "    out.append(row_data)\n",
    "\n",
    "  column_names = [f'comp_{i}' for i in range(max_length)]\n",
    "  df = pd.DataFrame(data=out, columns=column_names)\n",
    "  # Apply boxcox normalization to each column\n",
    "  for col in df.columns:\n",
    "    if cols_skip:\n",
    "      if col in cols_skip:\n",
    "        continue\n",
    "    try:\n",
    "      norm_comp = normalize(stats.boxcox(df[col]+shift_param)[0])\n",
    "      df[col] = norm_comp\n",
    "    except:\n",
    "      print('data values are negative, applying normalization first')\n",
    "      print(col)\n",
    "      continue\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "\n",
    "def z_score(data, avg, sdv):\n",
    "  return (data-avg) / sdv\n",
    "\n",
    "\n",
    "def minmax(data, min, max):\n",
    "  return ( (data - min) / (max - min) )\n",
    "\n",
    "\n",
    "def transform_to_td(feature_list, transform_feat_names, transform_idxs,\n",
    "                    loud_idxs=None, loud_add=100, max_val=None,\n",
    "                    scope=['mean', 'sdv']):\n",
    "  '''feature = 'sct_loud'\n",
    "  feature_list = list(df_new_feats[feature])\n",
    "  transform_idxs = [2,4]\n",
    "  loud_idx=4\n",
    "  transform_feat_names = ['sct_loud_conf', 'sct_loud']'''\n",
    "\n",
    "  out_dict = {}\n",
    "  for name, idx in zip(transform_feat_names, transform_idxs):\n",
    "    locals()[f'{name}_flat'] = []\n",
    "    for song in feature_list:\n",
    "      for feature in song:\n",
    "        if idx in loud_idxs:\n",
    "          if max_val:\n",
    "            if type(max_val)==list:\n",
    "              value = (loud_add + feature[idx]) / max_val[idx-4]\n",
    "            else:\n",
    "              value = (loud_add + feature[idx]) / max_val\n",
    "          else:\n",
    "            value = (loud_add + feature[idx])\n",
    "        else:\n",
    "          value = feature[idx]\n",
    "        locals()[f'{name}_flat'].append(value)\n",
    "    # Add z_score to output dictionary\n",
    "    if 'min' in scope:\n",
    "      out_dict[f'{name}_min'] = min(locals()[f'{name}_flat'])*0.7\n",
    "    if 'max' in scope:\n",
    "      out_dict[f'{name}_max'] = max(locals()[f'{name}_flat'])*1.3\n",
    "    if 'mean' in scope:\n",
    "      out_dict[f'{name}_mean'] = np.mean(locals()[f'{name}_flat'])\n",
    "    if 'sdv' in scope:\n",
    "      out_dict[f'{name}_sdv'] = np.std(locals()[f'{name}_flat'])\n",
    "\n",
    "  return out_dict\n",
    "\n",
    "\n",
    "def create_td_data(feature_list, time_segments, feature_idxs, mins=None, maxs=None,\n",
    "                   means=None, sdvs=None, modify_idxs=None, add_val=None,\n",
    "                   divide_val=None, divide_list=None,\n",
    "                   normalization_mode='minmax', run_normalization=True):\n",
    "  '''Creates a time series transformation of the data'''\n",
    "  songs = []\n",
    "  # Loop through song\n",
    "  for song in feature_list:\n",
    "    song_data = []\n",
    "    counter=0\n",
    "    # Loop through columns to input for feature data\n",
    "    for threshold in time_segments:\n",
    "      # Loop through feature values and stop once you find a match\n",
    "      features = []\n",
    "      for feature in song[counter:]:\n",
    "        start, end = feature[0], feature[3]\n",
    "        # Check if features apply to this time period\n",
    "        if (threshold > start) & (threshold <= end):\n",
    "          components=[]\n",
    "          # loop through components and get z-score or minmax\n",
    "          if normalization_mode=='minmax':\n",
    "            for i, idx, min, max in zip(range(len(feature_idxs)), feature_idxs, mins, maxs):\n",
    "              if idx in modify_idxs:\n",
    "                if divide_list:\n",
    "                  value = (feature[idx] + add_val) / divide_list[i-1]\n",
    "                  value = minmax(value, min, max)\n",
    "                else:\n",
    "                  if (add_val != None) & (divide_val != None):\n",
    "                    value = (feature[idx] + add_val) / divide_val\n",
    "                    if run_normalization:\n",
    "                      value = minmax(value, min, max)\n",
    "                  else:\n",
    "                    if add_val != None:\n",
    "                      value = minmax(add_val + feature[idx], min, max)\n",
    "                    else:\n",
    "                      value = minmax(feature[idx], min, max)\n",
    "              else:\n",
    "                value = minmax(feature[idx], min, max)\n",
    "              components.append(value)\n",
    "          # add values\n",
    "          features.extend(components)\n",
    "          break\n",
    "\n",
    "        # seconds is past feature\n",
    "        elif threshold > start:\n",
    "          counter+=1\n",
    "      # no matches add 0\n",
    "      if len(features)==0:\n",
    "        features.extend([0] * (len(feature_idxs)))\n",
    "      song_data.append(features)\n",
    "    # Add song features\n",
    "    songs.append(song_data)\n",
    "\n",
    "  return songs\n",
    "\n",
    "\n",
    "def process_feat_list(sample):\n",
    "  '''Converst string list to list'''\n",
    "  return json.loads(sample)\n",
    "\n",
    "def feat_data(data):\n",
    "  '''Ingest Audia Features data and create dataframe with raw audio analysis features'''\n",
    "  song_list = data.index.values.tolist()\n",
    "  # song_list = data.index.values.tolist()[:5]\n",
    "  df_new_feats = audio_analysis(song_list, sp=sp)\n",
    "  df_new_feats.index = df.index\n",
    "  return df_new_feats\n",
    "\n",
    "\n",
    "def transform_feature(df, feature_target):\n",
    "  '''Apply transformations to time series feature data to prepare for modeling'''\n",
    "  # Transformation code\n",
    "  # Section\n",
    "\n",
    "  if feature_target=='sct_data':\n",
    "    # sct_loud\n",
    "    feature='sct_loud'\n",
    "    feature_list = list(df[feature])\n",
    "    sec_cols = list(range(1,301))\n",
    "    feature_idxs = [2, 4]\n",
    "    means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean']]\n",
    "    sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv']]\n",
    "    modify_idxs=[4]\n",
    "    add_val=100\n",
    "    divide_val=None\n",
    "    out_loud = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                          modify_idxs, add_val, divide_val)\n",
    "    out_loud = np.array(out_loud)\n",
    "\n",
    "   # sct_tempo\n",
    "    feature ='sct_tempo'\n",
    "    feature_list = list(df[feature])\n",
    "    sec_cols = list(range(1,301))\n",
    "    feature_idxs = [2, 4, 5]\n",
    "    means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean'], z_score_dict[f'{feature}_conf_mean']]\n",
    "    sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv'], z_score_dict[f'{feature}_conf_sdv']]\n",
    "    modify_idxs=[]\n",
    "    add_val=None\n",
    "    divide_val=None\n",
    "    out_tempo = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                          modify_idxs, add_val, divide_val)\n",
    "    out_tempo = np.array(out_tempo)\n",
    "\n",
    "    # sct_key\n",
    "    feature='sct_key'\n",
    "    feature_list = list(df[feature])\n",
    "    sec_cols = list(range(1,301))\n",
    "    feature_idxs = [2, 4, 5]\n",
    "    means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean'], z_score_dict[f'{feature}_conf_mean']]\n",
    "    sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv'], z_score_dict[f'{feature}_conf_sdv']]\n",
    "    modify_idxs=[4]\n",
    "    add_val=1\n",
    "    divide_val=12\n",
    "    out_key = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                          modify_idxs, add_val, divide_val, run_z=True)\n",
    "    out_key = np.array(out_key)\n",
    "\n",
    "    # sct_mode\n",
    "    feature='sct_mode'\n",
    "    feature_list = list(df[feature])\n",
    "    sec_cols = list(range(1,301))\n",
    "    feature_idxs = [2, 4, 5]\n",
    "    means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean'], z_score_dict[f'{feature}_conf_mean']]\n",
    "    sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv'], z_score_dict[f'{feature}_conf_sdv']]\n",
    "    modify_idxs=[4]\n",
    "    add_val=0\n",
    "    divide_val=1\n",
    "    out_mode = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                          modify_idxs, add_val, divide_val)\n",
    "    out_mode = np.array(out_mode)\n",
    "\n",
    "    # sct_time_sig\n",
    "    feature='sct_time_sig'\n",
    "    feature_list = list(df[feature])\n",
    "    sec_cols = list(range(1,301))\n",
    "    feature_idxs = [2, 4, 5]\n",
    "    means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean'], z_score_dict[f'{feature}_conf_mean']]\n",
    "    sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv'], z_score_dict[f'{feature}_conf_sdv']]\n",
    "    modify_idxs=[4]\n",
    "    add_val=0\n",
    "    divide_val=7\n",
    "    out_time_sig = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                          modify_idxs, add_val, divide_val, run_z=True)\n",
    "    out_time_sig = np.array(out_time_sig)\n",
    "\n",
    "  # Concat section features\n",
    "  out_section = np.concatenate([out_loud, out_tempo[:, :, 1:], out_key[:, :, 1:], out_mode[:, :, 1:], out_time_sig[:, :, 1:]], -1)\n",
    "  # Make Predictions\n",
    "  if trim_model==True:\n",
    "    # Trim Model\n",
    "    model_pred = Model(inputs=models[0].input, outputs=models[0].layers[-3].output)\n",
    "    # Make Predictions\n",
    "    pred = model_pred.predict(out_section).tolist()\n",
    "    length = len(pred[0])\n",
    "    for i in range(length):\n",
    "      df[f'sct_feature_{i}'] = [item[i] for item in pred]\n",
    "  else:\n",
    "    pred = models[0].predict(out_section).tolist()\n",
    "    df['sct_pred'] = [item[0] for item in pred]\n",
    "\n",
    "\n",
    "\n",
    "  # sgm loud\n",
    "  feature = 'sgm_loud'\n",
    "  feature_list = list(df[feature])\n",
    "  mcr_sec_cols = list(np.arange(0.1,300.1, 0.1))\n",
    "  feature_idxs = [2, 4, 5, 6, 7]\n",
    "  means = [z_score_dict[f'sgm_conf_mean'], z_score_dict['sgm_loud_start_mean'], z_score_dict['sgm_loud_max_mean'],\n",
    "          z_score_dict['sgm_loud_max_time_mean'], z_score_dict['sgm_loud_end_mean']]\n",
    "  sddevs = [z_score_dict[f'sgm_conf_sdv'], z_score_dict['sgm_loud_start_sdv'], z_score_dict['sgm_loud_max_sdv'],\n",
    "            z_score_dict['sgm_loud_max_time_sdv'], z_score_dict['sgm_loud_end_sdv']]\n",
    "  modify_idxs=[4, 5, 7]\n",
    "  add_val=100\n",
    "  divide_val=1\n",
    "  out_sgm_loud = create_td_data(feature_list, mcr_sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val, run_z=True)\n",
    "  out_sgm_loud = np.array(out_sgm_loud)\n",
    "  # Make Predictions\n",
    "  if trim_model==True:\n",
    "    # Trim Model\n",
    "    model_pred = Model(inputs=models[1].input, outputs=models[1].layers[-3].output)\n",
    "    # Make Predictions\n",
    "    pred = model_pred.predict(out_sgm_loud).tolist()\n",
    "    length = len(pred[0])\n",
    "    for i in range(length):\n",
    "      df[f'sgm_loud_feature_{i}'] = [item[i] for item in pred]\n",
    "  else:\n",
    "    pred = models[1].predict(out_sgm_loud).tolist()\n",
    "    df['sgm_loud_pred'] = [item[0] for item in pred]\n",
    "\n",
    "\n",
    "  # sgm_pitch\n",
    "  feature = 'sgm_pitch'\n",
    "  feature_list = list(df[feature])\n",
    "  mcr_sec_cols = list(np.arange(0.1,300.1, 0.1))\n",
    "  feature_idxs = [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "  means = [z_score_dict['sgm_conf_mean']] + [z_score_dict[f'sgm_timbre_{i}_mean'] for i in range(1,13)]\n",
    "  sddevs = [z_score_dict['sgm_conf_sdv']] + [z_score_dict[f'sgm_timbre_{i}_sdv'] for i in range(1,13)]\n",
    "  modify_idxs=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "  add_val=0.000001\n",
    "  divide_val=1\n",
    "  out_sgm_pitch = create_td_data(feature_list, mcr_sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val)\n",
    "  out_sgm_pitch = np.array(out_sgm_pitch)\n",
    "  # Make Predictions\n",
    "  if trim_model==True:\n",
    "    # Trim Model\n",
    "    model_pred = Model(inputs=models[2].input, outputs=models[2].layers[-3].output)\n",
    "    # Make Predictions\n",
    "    pred = model_pred.predict(out_sgm_pitch).tolist()\n",
    "    length = len(pred[0])\n",
    "    for i in range(length):\n",
    "      df[f'sgm_pitch_{i}'] = [item[i] for item in pred]\n",
    "  else:\n",
    "    pred = models[2].predict(out_sgm_pitch).tolist()\n",
    "    df['sgm_pitch_pred'] = [item[0] for item in pred]\n",
    "\n",
    "\n",
    "  # sgm_timbre\n",
    "  feature = 'sgm_timbre'\n",
    "  feature_list = list(df[feature])\n",
    "  mcr_sec_cols = list(np.arange(0.1,300.1, 0.1))\n",
    "  feature_idxs = [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "  means = [z_score_dict['sgm_conf_mean']] + [z_score_dict[f'sgm_timbre_{i}_mean'] for i in range(1,13)]\n",
    "  sddevs = [z_score_dict['sgm_conf_sdv']] + [z_score_dict[f'sgm_timbre_{i}_sdv'] for i in range(1,13)]\n",
    "  modify_idxs=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "  add_val=1000\n",
    "  divide_val=None\n",
    "  divide_list = [timbre_max_dict[f'sgm_timbre_{i}_max'] for i in range(1,13)]\n",
    "  out_sgm_timbre = create_td_data(feature_list, mcr_sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val, divide_list)\n",
    "  out_sgm_timbre = np.array(out_sgm_timbre)\n",
    "  # Make Predictions\n",
    "  if trim_model==True:\n",
    "    # Trim Model\n",
    "    model_pred = Model(inputs=models[3].input, outputs=models[3].layers[-3].output)\n",
    "    # Make Predictions\n",
    "    pred = model_pred.predict(out_sgm_timbre).tolist()\n",
    "    length = len(pred[0])\n",
    "    for i in range(length):\n",
    "      df[f'sgm_timbre_{i}'] = [item[i] for item in pred]\n",
    "  else:\n",
    "    pred = models[3].predict(out_sgm_timbre).tolist()\n",
    "    df['sgm_timbre_pred'] = [item[0] for item in pred]\n",
    "\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def transform_td_features(df, z_score_dict, timbre_max_dict, models, trim_model=False):\n",
    "  '''Apply transformations to time series feature data to prepare for modeling'''\n",
    "  # Transformation code\n",
    "  # Section\n",
    "  ## sct_loud\n",
    "  feature='sct_loud'\n",
    "  feature_list = list(df[feature])\n",
    "  sec_cols = list(range(1,301))\n",
    "  feature_idxs = [2, 4]\n",
    "  means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean']]\n",
    "  sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv']]\n",
    "  modify_idxs=[4]\n",
    "  add_val=100\n",
    "  divide_val=None\n",
    "  out_loud = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val)\n",
    "  out_loud = np.array(out_loud)\n",
    "\n",
    "\n",
    "  ## sct_tempo\n",
    "  feature='sct_tempo'\n",
    "  feature_list = list(df[feature])\n",
    "  sec_cols = list(range(1,301))\n",
    "  feature_idxs = [2, 4, 5]\n",
    "  means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean'], z_score_dict[f'{feature}_conf_mean']]\n",
    "  sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv'], z_score_dict[f'{feature}_conf_sdv']]\n",
    "  modify_idxs=[]\n",
    "  add_val=None\n",
    "  divide_val=None\n",
    "  out_tempo = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val)\n",
    "  out_tempo = np.array(out_tempo)\n",
    "\n",
    "  # sct_key\n",
    "  feature='sct_key'\n",
    "  feature_list = list(df[feature])\n",
    "  sec_cols = list(range(1,301))\n",
    "  feature_idxs = [2, 4, 5]\n",
    "  means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean'], z_score_dict[f'{feature}_conf_mean']]\n",
    "  sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv'], z_score_dict[f'{feature}_conf_sdv']]\n",
    "  modify_idxs=[4]\n",
    "  add_val=1\n",
    "  divide_val=12\n",
    "  out_key = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val, run_z=True)\n",
    "  out_key = np.array(out_key)\n",
    "\n",
    "  # sct_mode\n",
    "  feature='sct_mode'\n",
    "  feature_list = list(df[feature])\n",
    "  sec_cols = list(range(1,301))\n",
    "  feature_idxs = [2, 4, 5]\n",
    "  means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean'], z_score_dict[f'{feature}_conf_mean']]\n",
    "  sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv'], z_score_dict[f'{feature}_conf_sdv']]\n",
    "  modify_idxs=[4]\n",
    "  add_val=0\n",
    "  divide_val=1\n",
    "  out_mode = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val)\n",
    "  out_mode = np.array(out_mode)\n",
    "\n",
    "  # sct_time_sig\n",
    "  feature='sct_time_sig'\n",
    "  feature_list = list(df[feature])\n",
    "  sec_cols = list(range(1,301))\n",
    "  feature_idxs = [2, 4, 5]\n",
    "  means = [z_score_dict['sct_conf_mean'], z_score_dict[f'{feature}_mean'], z_score_dict[f'{feature}_conf_mean']]\n",
    "  sddevs = [z_score_dict['sct_conf_sdv'], z_score_dict[f'{feature}_sdv'], z_score_dict[f'{feature}_conf_sdv']]\n",
    "  modify_idxs=[4]\n",
    "  add_val=0\n",
    "  divide_val=7\n",
    "  out_time_sig = create_td_data(feature_list, sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val, run_z=True)\n",
    "  out_time_sig = np.array(out_time_sig)\n",
    "\n",
    "  # Concat section features\n",
    "  out_section = np.concatenate([out_loud, out_tempo[:, :, 1:], out_key[:, :, 1:], out_mode[:, :, 1:], out_time_sig[:, :, 1:]], -1)\n",
    "  # Make Predictions\n",
    "  if trim_model==True:\n",
    "    # Trim Model\n",
    "    model_pred = Model(inputs=models[0].input, outputs=models[0].layers[-3].output)\n",
    "    # Make Predictions\n",
    "    pred = model_pred.predict(out_section).tolist()\n",
    "    length = len(pred[0])\n",
    "    for i in range(length):\n",
    "      df[f'sct_feature_{i}'] = [item[i] for item in pred]\n",
    "  else:\n",
    "    pred = models[0].predict(out_section).tolist()\n",
    "    df['sct_pred'] = [item[0] for item in pred]\n",
    "\n",
    "\n",
    "\n",
    "  # sgm loud\n",
    "  feature = 'sgm_loud'\n",
    "  feature_list = list(df[feature])\n",
    "  mcr_sec_cols = list(np.arange(0.1,300.1, 0.1))\n",
    "  feature_idxs = [2, 4, 5, 6, 7]\n",
    "  means = [z_score_dict[f'sgm_conf_mean'], z_score_dict['sgm_loud_start_mean'], z_score_dict['sgm_loud_max_mean'],\n",
    "          z_score_dict['sgm_loud_max_time_mean'], z_score_dict['sgm_loud_end_mean']]\n",
    "  sddevs = [z_score_dict[f'sgm_conf_sdv'], z_score_dict['sgm_loud_start_sdv'], z_score_dict['sgm_loud_max_sdv'],\n",
    "            z_score_dict['sgm_loud_max_time_sdv'], z_score_dict['sgm_loud_end_sdv']]\n",
    "  modify_idxs=[4, 5, 7]\n",
    "  add_val=100\n",
    "  divide_val=1\n",
    "  out_sgm_loud = create_td_data(feature_list, mcr_sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val, run_z=True)\n",
    "  out_sgm_loud = np.array(out_sgm_loud)\n",
    "  # Make Predictions\n",
    "  if trim_model==True:\n",
    "    # Trim Model\n",
    "    model_pred = Model(inputs=models[1].input, outputs=models[1].layers[-3].output)\n",
    "    # Make Predictions\n",
    "    pred = model_pred.predict(out_sgm_loud).tolist()\n",
    "    length = len(pred[0])\n",
    "    for i in range(length):\n",
    "      df[f'sgm_loud_feature_{i}'] = [item[i] for item in pred]\n",
    "  else:\n",
    "    pred = models[1].predict(out_sgm_loud).tolist()\n",
    "    df['sgm_loud_pred'] = [item[0] for item in pred]\n",
    "\n",
    "\n",
    "  # sgm_pitch\n",
    "  feature = 'sgm_pitch'\n",
    "  feature_list = list(df[feature])\n",
    "  mcr_sec_cols = list(np.arange(0.1,300.1, 0.1))\n",
    "  feature_idxs = [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "  means = [z_score_dict['sgm_conf_mean']] + [z_score_dict[f'sgm_timbre_{i}_mean'] for i in range(1,13)]\n",
    "  sddevs = [z_score_dict['sgm_conf_sdv']] + [z_score_dict[f'sgm_timbre_{i}_sdv'] for i in range(1,13)]\n",
    "  modify_idxs=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "  add_val=0.000001\n",
    "  divide_val=1\n",
    "  out_sgm_pitch = create_td_data(feature_list, mcr_sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val)\n",
    "  out_sgm_pitch = np.array(out_sgm_pitch)\n",
    "  # Make Predictions\n",
    "  if trim_model==True:\n",
    "    # Trim Model\n",
    "    model_pred = Model(inputs=models[2].input, outputs=models[2].layers[-3].output)\n",
    "    # Make Predictions\n",
    "    pred = model_pred.predict(out_sgm_pitch).tolist()\n",
    "    length = len(pred[0])\n",
    "    for i in range(length):\n",
    "      df[f'sgm_pitch_{i}'] = [item[i] for item in pred]\n",
    "  else:\n",
    "    pred = models[2].predict(out_sgm_pitch).tolist()\n",
    "    df['sgm_pitch_pred'] = [item[0] for item in pred]\n",
    "\n",
    "\n",
    "  # sgm_timbre\n",
    "  feature = 'sgm_timbre'\n",
    "  feature_list = list(df[feature])\n",
    "  mcr_sec_cols = list(np.arange(0.1,300.1, 0.1))\n",
    "  feature_idxs = [2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "  means = [z_score_dict['sgm_conf_mean']] + [z_score_dict[f'sgm_timbre_{i}_mean'] for i in range(1,13)]\n",
    "  sddevs = [z_score_dict['sgm_conf_sdv']] + [z_score_dict[f'sgm_timbre_{i}_sdv'] for i in range(1,13)]\n",
    "  modify_idxs=[4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "  add_val=1000\n",
    "  divide_val=None\n",
    "  divide_list = [timbre_max_dict[f'sgm_timbre_{i}_max'] for i in range(1,13)]\n",
    "  out_sgm_timbre = create_td_data(feature_list, mcr_sec_cols, feature_idxs, means, sddevs,\n",
    "                        modify_idxs, add_val, divide_val, divide_list)\n",
    "  out_sgm_timbre = np.array(out_sgm_timbre)\n",
    "  # Make Predictions\n",
    "  if trim_model==True:\n",
    "    # Trim Model\n",
    "    model_pred = Model(inputs=models[3].input, outputs=models[3].layers[-3].output)\n",
    "    # Make Predictions\n",
    "    pred = model_pred.predict(out_sgm_timbre).tolist()\n",
    "    length = len(pred[0])\n",
    "    for i in range(length):\n",
    "      df[f'sgm_timbre_{i}'] = [item[i] for item in pred]\n",
    "  else:\n",
    "    pred = models[3].predict(out_sgm_timbre).tolist()\n",
    "    df['sgm_timbre_pred'] = [item[0] for item in pred]\n",
    "\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "def chunks(lst, n):\n",
    "      \"\"\"Yield successive n-sized chunks from lst\n",
    "      Inputs:\n",
    "        lst (list): list of items to be split\n",
    "        n (int): number of splits to make\n",
    "      Output:\n",
    "        lst (list): returns list of lists that are broken up into chunks of size n\n",
    "        \"\"\"\n",
    "      for i in range(0, len(lst), n):\n",
    "          yield lst[i:i + n]\n",
    "\n",
    "\n",
    "def normalize_data(data, use_max=None, use_min=None, sav_max=False):\n",
    "    ''' Normalizes a list of data to be values from 0-1.  Uses a max value if\n",
    "        provided\n",
    "    Input:\n",
    "      data (list): list of data points\n",
    "      use_max (float): if specified, uses provided max value\n",
    "      sav_max (bool): if True, save outputs data max.\n",
    "    Output:-\n",
    "      norm_data (list): normalized from 0 - 1\n",
    "      norm_data (float): calculated maximum data point\n",
    "      '''\n",
    "    if use_max:\n",
    "      data_max = use_max\n",
    "      data_min = use_min\n",
    "    else:\n",
    "      data_max = np.max(data)*1.3\n",
    "      data_min = np.min(data)*0.7\n",
    "    norm_data = (data - data_min) / (data_max - data_min)\n",
    "    if sav_max:\n",
    "      return norm_data, data_max, data_min\n",
    "    else:\n",
    "      return norm_data\n",
    "\n",
    "\n",
    "def extract_audio_features(df):\n",
    "  '''Takes in a dataframe generated from extract_artist_info() and adds additional\n",
    "     columns for audio features from spotify's api for each song in the dataframe\n",
    "  Input:\n",
    "    df (DataFrame): Dataframe generated from extract_artist_info\n",
    "  Output:\n",
    "    df (DataFrame): merges audio feature columns into input dataframe'''\n",
    "\n",
    "  song_ids = list(df.song_id.values)\n",
    "  # Chunk up song ids to lengths of 50\n",
    "  song_chunks = list(chunks(song_ids, 50))\n",
    "  song_list=[]\n",
    "  for sample in song_chunks:\n",
    "    result = sp.audio_features(sample)\n",
    "    song_list.extend(result)\n",
    "  # loop through songs and store audio features in a list of lists\n",
    "  array_list = []\n",
    "  for song in song_list:\n",
    "    row_list = []\n",
    "    try:\n",
    "      acoust = song['acousticness']\n",
    "      dance = song['danceability']\n",
    "      energy = song['energy']\n",
    "      instrument = song['instrumentalness']\n",
    "      key = song['key']\n",
    "      live = song['liveness']\n",
    "      loud = song['loudness']\n",
    "      mode = song['mode']\n",
    "      speech = song['speechiness']\n",
    "      tempo = song['tempo']\n",
    "      time_sig = song['time_signature']\n",
    "      valence = song['valence']\n",
    "      row_list.extend([acoust, dance, energy, instrument, key, live, loud, mode,\n",
    "                       speech, tempo, time_sig, valence])\n",
    "      array_list.append(row_list)\n",
    "    except:\n",
    "      # If audio features can't be extracted from song, add NaN's\n",
    "      row_list.extend([np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, np.nan,\n",
    "                       np.nan, np.nan, np.nan, np.nan, np.nan])\n",
    "      array_list.append(row_list)\n",
    "      pass\n",
    "  # Create audio feature dataframe and merge with input dataframe\n",
    "  col_nams = ['acoust', 'dance', 'energy', 'instrument', 'key', 'live', 'loud',\n",
    "              'mode', 'speech', 'tempo', 'time_sig', 'valence']\n",
    "  df_songs = pd.DataFrame(data=array_list, columns=col_nams)\n",
    "  df = pd.merge(df, df_songs, left_index=True, right_index=True)\n",
    "  return df\n",
    "\n",
    "\n",
    "def extract_artist_info(df):\n",
    "  '''Takes a dataframe generated from extract_song_contents() and looks up all\n",
    "     of the artist ids in the dataframe.  A dictionary is created to keep track of\n",
    "     artist id row location\n",
    "  Input:\n",
    "    df (DataFrame): Dataframe generated from extract_song_contents\n",
    "  Output:\n",
    "    df (DataFrame): merges artist info and genre columns into input dataframe'''\n",
    "\n",
    "  # First flatten list of artist id's in df\n",
    "  nested_artists = list(df.artist_id.values)\n",
    "  artists = list(itertools.chain(*nested_artists))\n",
    "  # Partition artists list for artist lookup\n",
    "  artists_chunks = list(chunks(artists, 50))\n",
    "  artist_list = []\n",
    "  for sample in artists_chunks:\n",
    "    result = sp.artists(sample)\n",
    "    artist_list.extend(result['artists'])\n",
    "\n",
    "  # create a dataframe of all artists with their id, name, popularity, and genres\n",
    "  df_artists=[]\n",
    "  for artist in artist_list:\n",
    "    info = []\n",
    "    artist_id = artist['id']\n",
    "    artist_name = artist['name']\n",
    "    artist_genres = artist['genres']\n",
    "    artist_pop = artist['popularity']\n",
    "    info.extend([artist_id, artist_name, artist_genres, artist_pop])\n",
    "    df_artists.append(info)\n",
    "  df_artists = pd.DataFrame(data=df_artists,\n",
    "                            columns=['artist_id', 'artist_name', 'artist_genres',\n",
    "                                     'artist_pop']).drop_duplicates(subset='artist_id')\n",
    "  df_artists.set_index('artist_id', inplace=True)\n",
    "\n",
    "  # Loop through df and find artist info in df_artists\n",
    "  artist_genre, artist_pop = [], []\n",
    "  for row in df.artist_id:\n",
    "    multiple_art_genres =[]\n",
    "    sample = df_artists[df_artists.index==row[0]]\n",
    "    artist_pop.extend([sample.artist_pop[0]])\n",
    "    if len(row)<=1:\n",
    "      multiple_art_genres.extend(sample.artist_genres[0])\n",
    "    # Loop through other artists on the track\n",
    "    else:\n",
    "      for id in row:\n",
    "          sample = df_artists[df_artists.index==id]\n",
    "          multiple_art_genres.extend(sample.artist_genres[0])\n",
    "    artist_genre.append(multiple_art_genres)\n",
    "  # add artist genres and popularity to df_artists\n",
    "  df['artist_pop'] = artist_pop\n",
    "  df['genres'] = artist_genre\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def extract_song_contents(raw_song_data, ratings=None, weights=None):\n",
    "  data_lst = []\n",
    "  # Iterate through each item in the list and add song contents to an array\n",
    "  if ratings != None:\n",
    "    col_nams = ['song_id', 'song_nam', 'dur', 'explicit',\n",
    "                'pop', 'album_nam', 'album_type', 'release', 'artist_id',\n",
    "                'artist_nam', 'rating', 'weight']\n",
    "  else:\n",
    "    col_nams = ['song_id', 'song_nam', 'dur', 'explicit', 'pop', 'album_nam',\n",
    "                'album_type', 'release', 'artist_id', 'artist_nam']\n",
    "  for i, track in enumerate(raw_song_data):\n",
    "    try:\n",
    "      song_info=[]\n",
    "      # Get song general info\n",
    "      song_id = track['id']\n",
    "      song_nam = track['name']\n",
    "      dur = track[\"duration_ms\"]\n",
    "      explicit = int(track['explicit'])\n",
    "      pop = track['popularity']\n",
    "      album_nam = track['album']['name']\n",
    "      album_type = track['album']['album_type']\n",
    "      release = track['album']['release_date']\n",
    "      artist_id = [artist['id'] for artist in track['artists']]\n",
    "      artist_nam = [artist['name'] for artist in track['artists']]\n",
    "      if ratings != None:\n",
    "        rating, weight = ratings[i], weights[i]\n",
    "        song_info.extend([song_id, song_nam, dur, explicit, pop, album_nam,\n",
    "                          album_type, release, artist_id, artist_nam, rating, weight])\n",
    "      else:\n",
    "        song_info.extend([song_id, song_nam, dur, explicit, pop, album_nam,\n",
    "                          album_type, release, artist_id, artist_nam])\n",
    "      data_lst.append(song_info)\n",
    "    except:\n",
    "      pass\n",
    "  # Convert array to dataframe\n",
    "  data = pd.DataFrame(data=data_lst, columns=col_nams)\n",
    "  # Get artist info\n",
    "  data = extract_artist_info(data)\n",
    "  # Get Song Audio features\n",
    "  data = extract_audio_features(data)\n",
    "  # Drop rows with null values\n",
    "  data.dropna(inplace=True)\n",
    "\n",
    "  return data\n",
    "\n",
    "\n",
    "def create_song_df(data):\n",
    "  global BASIC_DATASET_API_COUNTER\n",
    "  raw_song_data = []\n",
    "  song_chunks = list(chunks(data['song_id'].tolist(), 50))\n",
    "  for sample in song_chunks:\n",
    "    results = sp.tracks(sample)['tracks']\n",
    "    data_len = len(results)\n",
    "    BASIC_DATASET_API_COUNTER += data_len\n",
    "    raw_song_data.extend(results)\n",
    "\n",
    "  # extract key song details from raw_song_data\n",
    "  song_info = extract_song_contents(raw_song_data,\n",
    "                                    ratings=data['rating'].tolist(),\n",
    "                                    weights=data['weight'].tolist())\n",
    "  return song_info\n",
    "\n",
    "\n",
    "def rand_sel(source_list, length=None):\n",
    "  '''Creates a random list of values with a random length from a sample list.\n",
    "     List length can be specified or randomly chosen.  For spotify's recommend\n",
    "     function, the max seed length is 5.  Putting length above 5 will break the\n",
    "     code.\n",
    "  Inputs:\n",
    "    source_list (list): list of items to be randomly queried from\n",
    "    length (int): number of items to randomly select from source_list\n",
    "  Outputs:\n",
    "    output (list): randomly selected n number of item/s from source_list based\n",
    "    on specified length'''\n",
    "  if length==None:\n",
    "    length = np.random.randint(len(source_list))\n",
    "  output = list(np.random.choice(source_list, (1,length))[0])\n",
    "  return output\n",
    "\n",
    "\n",
    "def get_date_float(input_str):\n",
    "  '''Convert release date to float with year_number.month percent of year.\n",
    "     If string format not easily interpretable, then return some value for year and month.\n",
    "  Input:\n",
    "    input_str (sting): datetime string\n",
    "  Output:\n",
    "    year (int): year specified in input_str\n",
    "    month (Int): month specified in input_str\n",
    "    '''\n",
    "  if len(input_str) > 5:\n",
    "    try:\n",
    "      time = datetime.strptime(input_str, '%Y-%m-%d')\n",
    "      year = time.timetuple()[0]\n",
    "      month = time.timetuple()[1]\n",
    "    except:\n",
    "      year=2021\n",
    "      month=1\n",
    "  else:\n",
    "    try:\n",
    "      time = datetime.strptime(input_str, '%Y')\n",
    "      year = time.timetuple()[0]\n",
    "      month = 1\n",
    "    except:\n",
    "      year = 2021\n",
    "      month = 1\n",
    "  return year, month\n",
    "\n",
    "\n",
    "def man_ord_encode(item):\n",
    "  '''Manual encode album type column with values between 0 and 1\n",
    "  Input:\n",
    "    item (string): album string name\n",
    "  Output:\n",
    "    out (float): value based on album string name\n",
    "    '''\n",
    "  if item.lower() == 'single':\n",
    "    out = 0.33\n",
    "  elif item.lower() == 'album':\n",
    "    out = 0.66\n",
    "  elif item.lower() == 'compilation':\n",
    "    out=0.99\n",
    "  else:\n",
    "    out=0\n",
    "  return out\n",
    "\n",
    "\n",
    "def wrangle(df, nlp=nlp_model):\n",
    "  '''Performs dataset preproccessing by encoding artists and genres, translating\n",
    "     album type to float, dropping irrelevant columns, and manually scaling\n",
    "    columns.\n",
    "      '''\n",
    "  max_col_vals = [100, 4753466*1.3, 91*1.3, 3.346*1.3, 5*1.3, 11*1.3, 247.936*1.3, 2040*1.3]\n",
    "  min_col_vals = [0, 0, 0, -60.0*1.3, 0, 0, 0.0, 1924*0.7]\n",
    "\n",
    "  df = df.copy()\n",
    "  # Format date column\n",
    "  output = df.release.apply(get_date_float)\n",
    "  year, month = list(map(list, zip(*output)))\n",
    "  df['release_year'] = [yr + mth for yr, mth in zip(year, month)]\n",
    "\n",
    "  # Apply spacy nlp model to get vectorizations\n",
    "  genre_vectorizations = [nlp(' '.join(genres)).vector for genres in df['genres']]\n",
    "\n",
    "  df['genres'] = genre_vectorizations\n",
    "\n",
    "  # Columns to normalize\n",
    "  cols_to_normalize = ['artist_pop', 'dur', 'pop', 'loud', 'time_sig',\n",
    "                       'key', 'tempo', 'release_year']\n",
    "\n",
    "    # normalize columns using provided max norm values\n",
    "  for col, max_norm, min_norm in zip(cols_to_normalize, max_col_vals, min_col_vals):\n",
    "    df[col] = normalize_data(df[col].values, use_max=max_norm, use_min=min_norm)\n",
    "\n",
    "  # Drop unneeded string columns\n",
    "  cols_to_drop = ['release', 'song_nam', 'album_nam', 'artist_id', 'artist_nam']\n",
    "  df.set_index('song_id', inplace=True)\n",
    "  df.drop(columns=cols_to_drop, inplace=True)\n",
    "  df.album_type = df.album_type.apply(man_ord_encode)\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "def get_genre_options(df):\n",
    "  '''Generates genre seed options based on genres present in good playlist.\n",
    "     Spotify has select amount of available genres used as seeds to generate\n",
    "     recommendations.\n",
    "  Inputs:\n",
    "    df (DataFrame): dataframe generated from create_song_df()\n",
    "  Outputs:\n",
    "    avail_genre_seeds (list): list of genre seeds\n",
    "    '''\n",
    "  good_genres=[]\n",
    "  for item in song_traits[song_traits['rating']==playlist_scores[-1]]['genres']:\n",
    "    good_genres.extend(item)\n",
    "  genre_opts = sp.recommendation_genre_seeds()['genres']\n",
    "  avail_genre_seeds = list(set(genre_opts) & set(good_genres))\n",
    "  return avail_genre_seeds\n",
    "\n",
    "\n",
    "def rec_batch_size(n_rec):\n",
    "  '''Generates a list of API call legnths based on n_rec which is the number\n",
    "     of desired recommendations.  Spotify API has a recommendation of 100 per\n",
    "     recommendation API call.\n",
    "  Input:\n",
    "    n_rec (int): number of desired recommendations\n",
    "  Output:\n",
    "    out (list): list of recommendation limit numbers whereby the max is 100 for\n",
    "    each list\n",
    "  Example:\n",
    "    n_rec = 230, output = [[100], [100], [23]]\n",
    "    '''\n",
    "  if n_rec <= 100:\n",
    "    out = [n_rec]\n",
    "  else:\n",
    "    num = math.ceil(n_rec / 100)\n",
    "    out = ([100] * (num - 1))\n",
    "    last = [n_rec - (num - 1) * 100]\n",
    "    out.extend(last)\n",
    "  return out\n",
    "\n",
    "\n",
    "def get_rec_filter_values(song_traits):\n",
    "  df_flt = song_traits[song_traits.rating==len(playlist_scores)-1]\n",
    "  rec_cats = ['acoust', 'dance', 'dur', 'energy', 'instrument', 'live', 'loud',\n",
    "              'pop', 'speech', 'tempo', 'valence']\n",
    "  rec_cats_nams = ['acousticness', 'danceability', 'duration_ms', 'energy',\n",
    "                  'instrumentalness', 'liveness', 'loudness', 'popularity',\n",
    "                  'speechiness', 'tempo', 'valence']\n",
    "  dict_rec_v = {}\n",
    "  for rec_cat, rec_nam in zip(rec_cats, rec_cats_nams):\n",
    "    v_min = min(df_flt[rec_cat])\n",
    "    v_max = max(df_flt[rec_cat])\n",
    "    v_avg = np.mean(df_flt[rec_cat])\n",
    "    # Spotify api recquires some values to be integer\n",
    "    if rec_cat in ['dur', 'pop']:\n",
    "      v_min, v_max, v_avg = int(v_min), int(v_max), int(v_avg)\n",
    "    dict_rec_v[f'min_{rec_nam}'] = v_min\n",
    "    dict_rec_v[f'max_{rec_nam}'] = v_max\n",
    "    dict_rec_v[f'target_{rec_nam}'] = v_avg\n",
    "\n",
    "  return dict_rec_v\n",
    "\n",
    "\n",
    "def vis_preds(data, data_vec, y_data, models, fig_title, pred_songs=False):\n",
    "  if pred_songs:\n",
    "    if 'rating' in df_test.columns: df_test.drop(columns='rating', inplace=True)\n",
    "    if len(models)>1:\n",
    "      out_pred = get_avg_predict(models, data, data_vec)\n",
    "    else:\n",
    "      model=models[0]\n",
    "      out_pred = model.predict(x=(data, data_vec))\n",
    "    fig = px.histogram(out_pred, nbins=200, title=fig_title)\n",
    "  else:\n",
    "    if len(models)>1:\n",
    "      train_pred = get_avg_predict(models, data, data_vec)\n",
    "    else:\n",
    "      model=models[0]\n",
    "      train_pred = model.predict(x=(data, data_vec))\n",
    "    cols = list(df.columns)\n",
    "    cols.remove('rating')\n",
    "    cols.remove('genres')\n",
    "    ac_test = pd.DataFrame(data=data, columns=cols)\n",
    "    ac_test['rating'] = y_data\n",
    "    ac_test['pred_rating'] = train_pred\n",
    "    ac_test.sort_values('rating', ascending=True, inplace=True)\n",
    "    fig = px.histogram(ac_test, x='pred_rating', color='rating', opacity=0.75,\n",
    "                       barmode='overlay', nbins=200, title=fig_title)\n",
    "  return fig.show()\n",
    "\n",
    "\n",
    "def vis_preds_newf(data, fig_title):\n",
    "  fig = px.histogram(data, x='rating', opacity=0.75,\n",
    "                      barmode='overlay', nbins=200, title=fig_title)\n",
    "  return fig.show()\n",
    "\n",
    "\n",
    "def get_avg_predict(models, data, data_vec):\n",
    "  '''Return average predicted rating from models'''\n",
    "  for i, model in enumerate(models):\n",
    "      if i==0:\n",
    "        out_pred = model.predict(x=(data, data_vec))\n",
    "      else:\n",
    "        out_pred += model.predict(x=(data, data_vec))\n",
    "  out_pred = out_pred / len(models)\n",
    "  return out_pred\n",
    "\n",
    "\n",
    "def get_avg_predict_newf(models, data):\n",
    "  '''Return average predicted rating from models'''\n",
    "  for i, model in enumerate(models):\n",
    "      if i==0:\n",
    "        out_pred = model.predict(x=data)\n",
    "      else:\n",
    "        out_pred += model.predict(x=data)\n",
    "  out_pred = out_pred / len(models)\n",
    "  return out_pred\n",
    "\n",
    "\n",
    "def load_model(file_path):\n",
    "  '''Load tensorflow model from Gdrive'''\n",
    "  zip_ref = zipfile.ZipFile(file_path, 'r')\n",
    "  zip_ref.extractall()\n",
    "  zip_ref.close()\n",
    "  # Load Model\n",
    "  model = tf.keras.models.load_model('content/saved_model/saved_model')\n",
    "  return model\n",
    "\n",
    "\n",
    "def data_splitter_for_model(data, col_idx=5):\n",
    "  '''Splits dataframe into two numpy arrays with song genre vectorization split into the second array'''\n",
    "  # Check if dataframe input\n",
    "  if type(data)!=np.ndarray:\n",
    "    data = data.to_numpy()\n",
    "  # Grab vectorization column\n",
    "  data_vec = np.array([row[col_idx] for row in data]).astype('float32')\n",
    "  # Remove vectorization column from data\n",
    "  data = np.delete(data, col_idx, 1)\n",
    "  # Convert arrays to float32 type\n",
    "  data = np.array(data).astype('float32')\n",
    "  data_vec = np.array(data_vec).astype('float32')\n",
    "  return data, data_vec\n",
    "\n",
    "\n",
    "def encode_data(data):\n",
    "  '''Applies two models to encode input data'''\n",
    "  if 'rating' in list(data.columns):\n",
    "    data = data.drop(columns='rating')\n",
    "  _, data_vec = data_splitter_for_model(data)\n",
    "  data_vec_comp = vec_model.predict(data_vec)\n",
    "  data.drop(columns=['genres'], inplace=True)\n",
    "  data[vec_col_nams] = data_vec_comp\n",
    "  data_processed = enc_model.predict(data)\n",
    "  return data_processed\n",
    "\n",
    "\n",
    "def encode_data_vec(data):\n",
    "  '''Applies vec model to encode input data'''\n",
    "  if 'rating' in list(data.columns):\n",
    "    data = data.drop(columns='rating')\n",
    "  _, data_vec = data_splitter_for_model(data)\n",
    "  data_vec_comp = vec_model.predict(data_vec)\n",
    "  data.drop(columns=['genres'], inplace=True)\n",
    "  data[vec_col_nams] = data_vec_comp\n",
    "  return data\n",
    "\n",
    "\n",
    "def generate_raw_data_segment(basic_dataset_file_path, sp=sp):\n",
    "  '''Generate time domain data from basic dataset'''\n",
    "  basic_dataset_file_list = os.listdir(basic_dataset_file_path)\n",
    "\n",
    "  for i in tqdm(range(len(basic_dataset_file_list))):\n",
    "    data = pd.read_csv(os.path.join(basic_dataset_file_path,\n",
    "                                    f'basic_dataset_p{i+1}.csv'))\n",
    "    song_list = data['song_id'].tolist()\n",
    "    df_raw_feats, sp = audio_analysis(song_list, sp=sp)\n",
    "    df_raw_feats = data[['song_id', 'rating', 'weight']].join(df_raw_feats, on='song_id')\n",
    "\n",
    "    save_dir = os.path.join(DATA_DIR, 'Databases', 'Raw_TD_Features')\n",
    "    make_dir(save_dir)\n",
    "    df_raw_feats.to_csv(os.path.join(save_dir, f'raw_td_feature_p{i+1}.csv'))\n",
    "    # clear varaible\n",
    "    df_raw_feats=None\n",
    "\n",
    "  print(f'Saved all TD parts saved successfully!')\n",
    "\n",
    "\n",
    "def transform_to_td(feature_list, transform_feat_names, transform_idxs,\n",
    "                    loud_idxs=None, loud_add=100, max_val=None,\n",
    "                    scope=['mean', 'sdv']):\n",
    "  '''feature = 'sct_loud'\n",
    "  feature_list = list(df_new_feats[feature])\n",
    "  transform_idxs = [2,4]\n",
    "  loud_idx=4\n",
    "  transform_feat_names = ['sct_loud_conf', 'sct_loud']'''\n",
    "\n",
    "  out_dict = {}\n",
    "  for name, idx in zip(transform_feat_names, transform_idxs):\n",
    "    locals()[f'{name}_flat'] = []\n",
    "    for song in feature_list:\n",
    "      for feature in song:\n",
    "        if idx in loud_idxs:\n",
    "          if max_val:\n",
    "            if type(max_val)==list:\n",
    "              value = (loud_add + feature[idx]) / max_val[idx-4]\n",
    "            else:\n",
    "              value = (loud_add + feature[idx]) / max_val\n",
    "          else:\n",
    "            value = (loud_add + feature[idx])\n",
    "        else:\n",
    "          value = feature[idx]\n",
    "        locals()[f'{name}_flat'].append(value)\n",
    "    # Add z_score to output dictionary\n",
    "    if 'min' in scope:\n",
    "      out_dict[f'{name}_min'] = min(locals()[f'{name}_flat'])*0.7\n",
    "    if 'max' in scope:\n",
    "      out_dict[f'{name}_max'] = max(locals()[f'{name}_flat'])*1.3\n",
    "    if 'mean' in scope:\n",
    "      out_dict[f'{name}_n'] = len(locals()[f'{name}_flat'])\n",
    "      out_dict[f'{name}_total'] = sum(locals()[f'{name}_flat'])\n",
    "      out_dict[f'{name}_sqr_total'] = sum(i*i for i in locals()[f'{name}_flat'])\n",
    "\n",
    "  return out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXTBs_c-xg3J",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Playlist links:\n",
    "To get consistent access to your playlists, you need the playlist URI:\n",
    "1. Open spotify and navigate to your playlist\n",
    "2. Select the three dots under the playlist and navigate to the share button.\n",
    "3. Next to the share button mouse over to \"Copy Link to Playlist\" and press control or command on your keyboard.  \"Copy Link to Playlist\" will change to \"Copy Spotify URI\" and click on \"Copy Spotify URI\".  This link to your playlist will be persitent while the default \"Copy Link to Playlist\" will change periodically.\n",
    "\n",
    "Playlists 1-5 will be used as the negative song label when building the model and will be used to remove songs from recommendations.  It is recommended to include a playlist of disliked songs to help the model filter out songs you don't like.  At least one playlist must be specified in playllsts 1-5.\n",
    "\n",
    "The target playlist will be the positive song label when building the building and will be used to select song recommendations.\n",
    "\n",
    "An output playlist is required in order for song recommendations to be sent to a playlist.  <strong>It is highly recommended to create a new playlist for the output playlist since songs can be removed in the playlist if specified in the config!</strong>  You will also have to be the creator of the output playlist to add and remove songs from it.<br><br>\n",
    "Note: the output playlist can not be a Collaborative one since spotify's api doesn't allow automated modification of collaborative playlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1705526194880,
     "user": {
      "displayName": "Bobby Wilt",
      "userId": "15841792391103222095"
     },
     "user_tz": 480
    },
    "id": "Qgej1r9IOAxb",
    "outputId": "d175222c-b739-4282-9cb4-857989e36d49"
   },
   "outputs": [],
   "source": [
    "if operation==\"create initial basic dataset\":\n",
    "  # Open config file and set ratings if not set\n",
    "  rating_input(playlists=list(INPUT_PLAYLISTS.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnPAg9TOX02p"
   },
   "source": [
    "# Create Initial Dataset\n",
    "Creates the basic dataset (song traits) that includes the data for genre and basic song traits.  Will be used for general feature and song ids.  Also creates ratings column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if operation==\"create initial basic dataset\":\n",
    "  # Obtain all song data and save with song_id, rating and weight\n",
    "  df_song_ids = create_song_id_df()\n",
    "  df_cnts = get_rating_cnts(df_song_ids[['rating']])\n",
    "  display(df_song_ids)\n",
    "  display(df_cnts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if operation==\"create initial basic dataset\":\n",
    "  # Weight Function\n",
    "  custom_weights = calc_weights(df_song_ids, df_cnts, weight_mode=weight_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation==\"create initial basic dataset\":\n",
    "  # Add weights to df_song_id\n",
    "  df_song_ids = save_weights_and_add_to_df(custom_weights, df_song_ids)\n",
    "  config = json.load(open('config.json'))\n",
    "  config['num_songs'] = df_song_ids.shape[0]\n",
    "  json.dump(config, open('config.json', 'w'))\n",
    "  display(df_song_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation==\"create initial basic dataset\":\n",
    "  basic_data_ids_save_dir = os.path.join(DATA_DIR, 'Databases', 'Basic_Dataset_Song_Ids')\n",
    "  n_chunks = get_n_chunks(df_song_ids)\n",
    "  split_dataset_into_chunks(df_song_ids.reset_index(), n_chunks, 'rating', basic_data_ids_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation==\"create initial basic dataset\":\n",
    "    # create song_id mapping dictionary\n",
    "    song_id_path = os.path.join(DATA_DIR, 'Databases', 'Basic_Dataset_Song_Ids')\n",
    "    song_id_files = os.listdir(song_id_path)\n",
    "    song_ids, ratings = [], []\n",
    "    for file in song_id_files:\n",
    "        df=pd.read_csv(os.path.join(song_id_path, file))\n",
    "        song_ids.extend(df['song_id'].tolist())\n",
    "        ratings.extend(df['rating'].tolist())\n",
    "    out_dict = {}\n",
    "    for song_id, rating, unq_id in zip(song_ids, ratings, list(range(1,len(song_ids)+1))):\n",
    "            out_dict[song_id] = (unq_id, rating)\n",
    "    if os.path.isfile('song_id_lookup.json')==False:\n",
    "      json.dump(dict(), open('song_id_lookup.json', 'w'))\n",
    "    json.dump(out_dict, open('song_id_lookup.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if operation==\"create initial basic dataset\":\n",
    "  generate_basic_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tayrKuDG2xhl",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Generate Raw TD Dataset\n",
    "Generates the raw time domain data as well as general features from spotify's audio analysis endpoint.  Will generate 2 output tables for access for generating the rest of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1705526197273,
     "user": {
      "displayName": "Bobby Wilt",
      "userId": "15841792391103222095"
     },
     "user_tz": 480
    },
    "id": "guR1vCPz21U0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if operation==\"generate raw td dataset\":\n",
    "  if BASIC_DATASET_API_COUNTER > 0:\n",
    "    try:\n",
    "      os.remove('.cache')\n",
    "    except: pass\n",
    "    sp = create_connection()\n",
    "  basic_dataset_path = os.path.join(DATA_DIR, 'Databases', 'Basic_Dataset')\n",
    "  basic_dataset_file_list = os.listdir(basic_dataset_path)\n",
    "  # This function may take several hours depending on the size of your dataset (~10 minutes per 1000 songs and 250mb per 1000 songs)\n",
    "  generate_raw_data_segment(basic_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJrI3YSAnTkr"
   },
   "source": [
    "# Create Feature Tables\n",
    "Creates our time series feature tables from the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1705526197273,
     "user": {
      "displayName": "Bobby Wilt",
      "userId": "15841792391103222095"
     },
     "user_tz": 480
    },
    "id": "bWeemdXJnWPf"
   },
   "outputs": [],
   "source": [
    "if operation==\"create feature tables\":\n",
    "  basic_dataset_path = os.path.join(DATA_DIR, 'Databases', 'Basic_Dataset')\n",
    "  basic_dataset_file_list = os.listdir(basic_dataset_path)\n",
    "  raw_td_dataset_path = os.path.join(DATA_DIR, 'Databases', 'Raw_TD_Features')\n",
    "  raw_td_file_list = os.listdir(raw_td_dataset_path)\n",
    "  config = json.load(open('config.json'))\n",
    "  if 'null_idxs' in config.keys():\n",
    "    null_tracker = config['null_idxs']\n",
    "  else:\n",
    "    null_tracker = {}\n",
    "  # null_tracker = {}\n",
    "  for i in range(len(raw_td_file_list)):\n",
    "    null_tracker[f'{i+1}'] = []\n",
    "  for feature in [\"genre\", \"general\", \"sct_data\", \"sgm_loud\", \"sgm_pitch\", \"sgm_timbre\"]:\n",
    "    feature_path = os.path.join(DATA_DIR, 'Databases', feature)\n",
    "    make_dir(feature_path)\n",
    "    # Caclculate normalization values and add them to config\n",
    "    null_tracker = generate_feature_norms(raw_td_dataset_path, raw_td_file_list, basic_dataset_path,\n",
    "                           feature=feature, normalization_mode=normalization_mode, run_normalization=False,\n",
    "                                         null_tracker=null_tracker)\n",
    "    config = json.load(open('config.json'))\n",
    "    config['null_idxs'] = null_tracker\n",
    "    json.dump(config, open('config.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if operation==\"create feature tables\":\n",
    "  basic_dataset_path = os.path.join(DATA_DIR, 'Databases', 'Basic_Dataset')\n",
    "  basic_dataset_file_list = os.listdir(basic_dataset_path)\n",
    "  raw_td_dataset_path = os.path.join(DATA_DIR, 'Databases', 'Raw_TD_Features')\n",
    "  raw_td_file_list = os.listdir(raw_td_dataset_path)\n",
    "  config = json.load(open('config.json'))\n",
    "  null_tracker = config['null_idxs']\n",
    "  # apply normalization and save datasets\n",
    "  for feature in [\"genre\", \"general\", \"sct_data\", \"sgm_loud\", \"sgm_pitch\", \"sgm_timbre\"]:\n",
    "  # for feature in [\"sct_data\"]:\n",
    "    feature_path = os.path.join(DATA_DIR, 'Databases', feature)\n",
    "    _ = generate_feature_norms(raw_td_dataset_path, raw_td_file_list, basic_dataset_path,\n",
    "                           feature=feature, normalization_mode=normalization_mode,\n",
    "                           run_normalization=True, save_dir=feature_path, null_tracker=null_tracker, output_format='trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation==\"create feature tables\":\n",
    "  generate_big_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XLbxNnGnWkT"
   },
   "source": [
    "# Create Final Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation == 'create final data table':\n",
    "  features = ['genre', 'general', 'sct_data', 'sgm_loud', 'sgm_timbre', 'sgm_pitch']\n",
    "  # Move models to Saved_Models\n",
    "  MODEL_TUNE_DIR_FILE_DIR = os.path.join(DATA_DIR, 'Model_tuning')\n",
    "  for feature in features:\n",
    "    for file in os.listdir(os.path.join(MODEL_TUNE_DIR_FILE_DIR, feature)):\n",
    "          if file.endswith('.keras'):\n",
    "            source_path = os.path.join(MODEL_TUNE_DIR_FILE_DIR, feature, file)\n",
    "            destination_path = os.path.join(DATA_DIR, 'Saved_Models', file)\n",
    "            shutil.copyfile(source_path, destination_path)\n",
    "          continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation == 'create final data table':\n",
    "  # Load Models\n",
    "  model_path = os.path.join(DATA_DIR, 'Saved_Models')\n",
    "  model_dict = {}\n",
    "  for model in os.listdir(model_path):\n",
    "    if 'genre' in model:\n",
    "      model_dict['genre'] =  tf.keras.models.load_model(os.path.join(model_path, model))\n",
    "    elif 'general' in model:\n",
    "      model_dict['general'] = tf.keras.models.load_model(os.path.join(model_path, model))\n",
    "    elif 'sct_data' in model:\n",
    "      model_dict['sct_data'] = tf.keras.models.load_model(os.path.join(model_path, model))\n",
    "    elif 'sgm_loud' in model:\n",
    "      model_dict['sgm_loud'] = tf.keras.models.load_model(os.path.join(model_path, model))\n",
    "    elif 'sgm_timbre' in model:\n",
    "      model_dict['sgm_timbre'] = tf.keras.models.load_model(os.path.join(model_path, model))\n",
    "    elif 'sgm_pitch' in model:\n",
    "      model_dict['sgm_pitch'] = tf.keras.models.load_model(os.path.join(model_path, model))\n",
    "    elif 'overall_best' in model:\n",
    "      model_dict['overall'] = joblib.load(os.path.join(model_path, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if operation == 'create final data table':\n",
    "  # Find normalization values\n",
    "  generate_final_dataset(run_normalization=False)\n",
    "  # Apply normalization values\n",
    "  generate_final_dataset(run_normalization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if operation == 'create final data table':\n",
    "  model_path = os.path.join(DATA_DIR, 'Saved_Models')\n",
    "  for model in os.listdir(model_path):\n",
    "    if 'overall_best' in model:\n",
    "      model_dict['overall'] = joblib.load(os.path.join(model_path, model))\n",
    "    out = get_overall_data()\n",
    "    if 'overall' in os.listdir(model_path):\n",
    "        lr_model = model_dict['overall']\n",
    "        preds = lr_model.predict(out[[\"genre\", \"general\", \"sct_data\", \"sgm_loud\", \"sgm_pitch\", \"sgm_timbre\"]])\n",
    "        out['overall_model'] = preds\n",
    "        df_out = out.sort_values('rating').reset_index().drop(columns=['index'])\n",
    "        df_out['song_id'] = df_out.index\n",
    "        df_out['rating'] = (df_out['rating'] + abs(df_out['rating'].min())) / (df_out['rating'].max() + abs(df_out['rating'].min()))\n",
    "        df_out['overall_model'] = (df_out['overall_model'] + abs(df_out['overall_model'].min())) / (df_out['overall_model'].max() + abs(df_out['overall_model'].min()))\n",
    "        df_out.drop(columns=['weight'], inplace=True)\n",
    "        df_melt = df_out.melt(id_vars=['song_id'], value_name='model_score', var_name='model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation == 'create final data table':\n",
    "  if 'overall' in model_dict.keys():\n",
    "    fig = px.scatter(df_melt, x='song_id', y='model_score', color='model'\n",
    "       , title='Normalized Model Predictions and Model Rating', opacity=1,\n",
    "       trendline='ols', trendline_color_override=\"black\", height=800, width=2100)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_contour(df_melt, x=\"song_id\", y=\"model_score\", color=\"model\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if operation == 'create final data table':\n",
    "  if 'overall' in model_dict.keys():\n",
    "    fig = px.scatter(df_melt, x='song_id', y='model_score', color='model'\n",
    "       , title='Normalized Model Predictions and Model Rating', opacity=1,\n",
    "       trendline='ols', trendline_color_override=\"black\", height=800, width=2100)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOKBetKPASqPsIAs99aq0/s",
   "collapsed_sections": [
    "SeoJUD49rAp6",
    "rtkO3uVfZRAx",
    "AJCTWGo6qbPd",
    "89-iLxTrXwii",
    "rnPAg9TOX02p",
    "AG5t2dNYn6cH",
    "0XLbxNnGnWkT"
   ],
   "gpuClass": "premium",
   "gpuType": "T4",
   "machine_shape": "hm",
   "mount_file_id": "1czsNIfwzdRxt4hXmCgMZqidfvZTQPGX-",
   "provenance": [
    {
     "file_id": "1Vw2g2Sr1PVZI_KTAq9T71GGiEbSfEICl",
     "timestamp": 1647279395040
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
